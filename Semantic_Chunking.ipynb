{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIOL07yevblt8mpvTM+NkR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/generalkeno-b/Semantic_Chunking/blob/main/Semantic_Chunking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hptVrKKAKrh",
        "outputId": "ce31e68b-77fa-4596-9a65-271d06b8d9b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Hyper fast Audio and Video encoder\n",
            "usage: ffmpeg [options] [[infile options] -i infile]... {[outfile options] outfile}...\n",
            "\n",
            "\u001b[0;33mUse -h to get full help or, even better, run 'man ffmpeg'\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube pydub --quiet\n",
        "! pip install git+https://github.com/m-bain/whisperx.git --quiet\n",
        "! pip install git+https://github.com/openai/whisper.git --quiet\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZnE2UkpUUnm",
        "outputId": "d136a10a-53bb-408f-b95b-417dfbf7d784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.7/208.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.3/192.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.1/119.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.1/760.1 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.2/802.2 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for whisperx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "def download_youtube_audio(youtube_url, output_path='/content'):\n",
        "    try:\n",
        "        # Download YouTube video\n",
        "        yt = YouTube(youtube_url)\n",
        "        video = yt.streams.filter(only_audio=True).first()\n",
        "        video.download(output_path=output_path, filename='video.mp4')\n",
        "\n",
        "        # Convert video to audio\n",
        "        video_path = os.path.join(output_path, 'video.mp4')\n",
        "        audio_path = os.path.join(output_path, 'audio.mp3')\n",
        "\n",
        "        audio = AudioSegment.from_file(video_path)\n",
        "        audio.export(audio_path, format='mp3')\n",
        "\n",
        "        print(f'Audio extracted and saved to {audio_path}')\n",
        "        return audio_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage\n",
        "youtube_url = 'https://www.youtube.com/watch?v=Sby1uJ_NFIY'\n",
        "audio_file_path = download_youtube_audio(youtube_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKLG05F1Uhmb",
        "outputId": "7cf0e060-b4d4-4684-f782-38a9852a8a56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio extracted and saved to /content/audio.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisperx /content/audio.mp3 --model medium.en --output_dir /content --align_model WAV2VEC2_ASR_LARGE_LV60K_960H --align_extend 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyZCHHUbKvrL",
        "outputId": "3c61baef-969b-47c4-99f8-6fcd812a08f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-20 09:13:53.360868: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-20 09:13:53.360922: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-20 09:13:53.362284: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-20 09:13:54.511612: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "usage: whisperx [-h] [--model MODEL] [--model_dir MODEL_DIR] [--device DEVICE]\n",
            "                [--device_index DEVICE_INDEX] [--batch_size BATCH_SIZE]\n",
            "                [--compute_type {float16,float32,int8}] [--output_dir OUTPUT_DIR]\n",
            "                [--output_format {all,srt,vtt,txt,tsv,json,aud}] [--verbose VERBOSE]\n",
            "                [--task {transcribe,translate}]\n",
            "                [--language {af,am,ar,as,az,ba,be,bg,bn,bo,br,bs,ca,cs,cy,da,de,el,en,es,et,eu,fa,fi,fo,fr,gl,gu,ha,haw,he,hi,hr,ht,hu,hy,id,is,it,ja,jw,ka,kk,km,kn,ko,la,lb,ln,lo,lt,lv,mg,mi,mk,ml,mn,mr,ms,mt,my,ne,nl,nn,no,oc,pa,pl,ps,pt,ro,ru,sa,sd,si,sk,sl,sn,so,sq,sr,su,sv,sw,ta,te,tg,th,tk,tl,tr,tt,uk,ur,uz,vi,yi,yo,yue,zh,Afrikaans,Albanian,Amharic,Arabic,Armenian,Assamese,Azerbaijani,Bashkir,Basque,Belarusian,Bengali,Bosnian,Breton,Bulgarian,Burmese,Cantonese,Castilian,Catalan,Chinese,Croatian,Czech,Danish,Dutch,English,Estonian,Faroese,Finnish,Flemish,French,Galician,Georgian,German,Greek,Gujarati,Haitian,Haitian Creole,Hausa,Hawaiian,Hebrew,Hindi,Hungarian,Icelandic,Indonesian,Italian,Japanese,Javanese,Kannada,Kazakh,Khmer,Korean,Lao,Latin,Latvian,Letzeburgesch,Lingala,Lithuanian,Luxembourgish,Macedonian,Malagasy,Malay,Malayalam,Maltese,Maori,Marathi,Moldavian,Moldovan,Mongolian,Myanmar,Nepali,Norwegian,Nynorsk,Occitan,Panjabi,Pashto,Persian,Polish,Portuguese,Punjabi,Pushto,Romanian,Russian,Sanskrit,Serbian,Shona,Sindhi,Sinhala,Sinhalese,Slovak,Slovenian,Somali,Spanish,Sundanese,Swahili,Swedish,Tagalog,Tajik,Tamil,Tatar,Telugu,Thai,Tibetan,Turkish,Turkmen,Ukrainian,Urdu,Uzbek,Valencian,Vietnamese,Welsh,Yiddish,Yoruba}]\n",
            "                [--align_model ALIGN_MODEL] [--interpolate_method {nearest,linear,ignore}]\n",
            "                [--no_align] [--return_char_alignments] [--vad_onset VAD_ONSET]\n",
            "                [--vad_offset VAD_OFFSET] [--chunk_size CHUNK_SIZE] [--diarize]\n",
            "                [--min_speakers MIN_SPEAKERS] [--max_speakers MAX_SPEAKERS]\n",
            "                [--temperature TEMPERATURE] [--best_of BEST_OF] [--beam_size BEAM_SIZE]\n",
            "                [--patience PATIENCE] [--length_penalty LENGTH_PENALTY]\n",
            "                [--suppress_tokens SUPPRESS_TOKENS] [--suppress_numerals]\n",
            "                [--initial_prompt INITIAL_PROMPT]\n",
            "                [--condition_on_previous_text CONDITION_ON_PREVIOUS_TEXT] [--fp16 FP16]\n",
            "                [--temperature_increment_on_fallback TEMPERATURE_INCREMENT_ON_FALLBACK]\n",
            "                [--compression_ratio_threshold COMPRESSION_RATIO_THRESHOLD]\n",
            "                [--logprob_threshold LOGPROB_THRESHOLD]\n",
            "                [--no_speech_threshold NO_SPEECH_THRESHOLD] [--max_line_width MAX_LINE_WIDTH]\n",
            "                [--max_line_count MAX_LINE_COUNT] [--highlight_words HIGHLIGHT_WORDS]\n",
            "                [--segment_resolution {sentence,chunk}] [--threads THREADS] [--hf_token HF_TOKEN]\n",
            "                [--print_progress PRINT_PROGRESS]\n",
            "                audio [audio ...]\n",
            "whisperx: error: unrecognized arguments: --align_extend 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# Define your variables  # Make sure to set this variable correctly\n",
        "model = \"medium.en\"\n",
        "output_dir = \".\"\n",
        "align_model = \"WAV2VEC2_ASR_LARGE_LV60K_960H\"\n",
        "\n",
        "# Construct the command string\n",
        "command = f\"whisperx {audio_file_path} --model {model} --output_dir {output_dir} --align_model {align_model}\"\n",
        "\n",
        "# Execute the command\n",
        "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "\n",
        "# Check the result\n",
        "if result.returncode == 0:\n",
        "    print(\"Command executed successfully\")\n",
        "    print(\"Output:\", result.stdout)\n",
        "else:\n",
        "    print(\"Error occurred\")\n",
        "    print(\"Error output:\", result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zquhLc3-JuZL",
        "outputId": "371c8b40-72c9-4f09-f6f7-499cd7379f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Command executed successfully\n",
            "Output: Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.2.1+cu121. Bad things might happen unless you revert torch to 1.x.\n",
            ">>Performing transcription...\n",
            ">>Performing alignment...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "# audio_file_path = \"/content/Audio_YT.mp3\"\n",
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(audio_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zOqkm3Mq9ZP",
        "outputId": "11f7fbab-576e-46bf-e5b9-b8b9e0277802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['segments'][0]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0d_NWDqKy8H8",
        "outputId": "1d988b8a-7f11-4663-8f9b-881cd07a099d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Congratulations to you Mr. Raghavan for that. Thank you so much for joining us.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqiWn22Qy-es",
        "outputId": "ad6b631a-0b9c-45c9-a2ea-0d4fd15d9698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': \" Congratulations to you Mr. Raghavan for that. Thank you so much for joining us. Over to you. Hi everybody. How are you? Okay I am not hearing this at all. It's like a post lunch energy downer or something. Let's hear it. Are you guys awake? All right you better be because we have a superstar guest here. You heard the 41 million dollars and I didn't hear honestly anything she said after that. So we're going to ask for about 40 million dollars from him by the end of this conversation. But let's get started. I want to introduce Vivek and Pratius, she's co-founder who's not here. We wanted to start with a playing a video of what OpenHathe does. I encourage all of you to go to the website, www.severalm.ai and check it out. But let me start by introducing Vivek. Vivek is a dear friend and he is very, very modest. One of the most modest guys that I know. But his personal journey Vivek, you've got a PhD from Carnegie Mellon. You've sat in and sold the company to Magma. Vivek and I moved back to India from both in the valley on the same day actually. And you've been in India for the last 16 years. And what most people don't know is your journey at Adhar. He spent 13 years selflessly at Adhar. Nobody would have heard of him. But he was a pioneering technology visionary behind Adhar which we all take for granted today. So please give it out. Honestly when people, when I think of selfless service, truly selfless service, I always think of Vivek. And since then he also was at AI for Bharat which we're going to touch on where he met Pratiyusha's other co-founder. Pratiyusha had a PhD from ETH at Zurich. He was in the IBM research. He was at Microsoft Research playing a key role and a faculty at IIT Madras and at AI for Bharat. So that's a little brief introduction about them. These guys are modest, modest engineers. So they don't do their own hon. So forgive me for tooting their hon in this case. But let's jump right in about the money. Funding. 41 million bucks man. That's a lot of money. Every entrepreneur here is saying what the hell did these guys do? What did the investors see to write such a big check? No, I think it's a trend of what's going on in India. I think that for the very first time, I think the investors have looked at, let's try and build something deep-tech out of the country. And let's try to figure out how to build something as a foundational technology out of the country. And that's really what's really exciting. And I think that about, as Bala was mentioning for the past 15 years, I've been working in both digital public infrastructure and kind of non-profit kind of things. But when this whole thing of generative AI came about, we said, okay, how can I actually make a difference in this space? And I said, maybe this is the opportunity to actually come out and really build something. And the only way that we realize that you can do it is actually in the private sector. And I think that's, and then when we went out there and we said we want to build something, which is a continuation. And fundamentally, the question is the reason of what we want to do at Server May I is we want to basically make generative AI available and accessible to the people in the country. And that's the intent. And when we said that we want to do this, there was a resonance in the investment community. And I think it's a responsibility to really to show that something like this can be built out of India. So we see that as as as as confidence and a responsibility. And I also hope it's a trend that you know that there are many more people like us who are backed. Because if you look at it, maybe it's a large number in a you know in the Indian context, but in the global context, I think there is just there should be many, many more entrepreneurs who are backed to do things in India. So yeah, I'm going to come back to the many more entrepreneurs. I'm obviously going to ask you about Bhavesh's Krutri. So we're going to come back to that question. But again, 41 million dollars. I mean, all of what you said, you know, two million dollars, you know, that's a good amount of money for a startup which you know, which has not yet built anything. What are you going to do with all this money? I can have a perfect solution for the problem. I think in the last week I've got lots of calls, lots of people telling me how I can do it. But I know you first, okay, I'll be landed in the country in the same day. I'm in the front of the queue. But honestly, I think the key thing in this is to putting together an amazing team. And we actually have an amazing team, but we believe that it is talent that will drive this kind of thing. And so it is to get key talent. And of course, the other thing is compute. This is extremely expensive compute wise to actually do these kinds of things. And I think that those are the two primary things that you know, we'd use this for. I'm computing in my own head as an entrepreneur. Talent, okay, you have like 2015 people. How much are you paying these guys? But okay, well, you won't touch on that. But let's talk about what you guys actually built. What is open Hathi? How would you explain open Hathi to? Many people here who might not have known about it. So I think open Hathi is, so first of all, right, we come from, I personally come from the open source ecosystem and we, and also from the DPI ecosystem. So we believe that for this to work, we need the ecosystem to be successful. And as a result of that, one of the first things we did was, hey, there are these open source, large language models that exist, right? I mean, everybody knows about the Lama family from Meta. There are others like Mistral. There are a bunch of open source, you know, large language models. And then we said, is there any way that take an existing open source model and teach it language skills, right? I mean, you know, language. And that is really the, you know, what we decide, what we said that can we do something like that? And is this a, you know, relatively frugal way of actually, you know, making models, you know, work in diverse languages? Because the truth is still today, I mean, if you look at the amount of data and knowledge, it is still English dominates these things. And I think that how do you actually take and make it understand Indian language, understand Indian context, and all of those things in actually a, in an efficient way? And therefore, this was an attempt through that. And it's an open hearty is, you know, is currently based on the Lama 7 billion model, but we'll be releasing many more models in different languages, different sizes, and things like that as part of this, as part of this series. And of course, you know, we will be building further models on those and doing other things to actually, and we'll also have endpoints that people can use. So therefore, it's not, it's definitely, you know, something that people can can can use two things. And that's the essence of what this open hearty is. So what does it mean to people in the audience here who are either doing their own startups or a business or developers? How should they look at OpenAI? Sorry, not open. No, no, I think the way you look at it is that we are one of the important things that we are doing is we're not just building models. We are also going to be building a platform, a platform for developers where you can actually use a combination of various different kinds of models, some which are from us, some which are open source, some which may not be open source, and actually to actually pull together and figure out how to deploy, you know, generative AI applications at scale and understand and evaluate their performance in an efficient manner. And that's something that we're planning to do at this and this platform is, you know, in the next couple of months, we'll be coming out there, it will be available to developers. But of course, those who want to start with the open source things and hack with that, of course, please go ahead and do that as well. That's phenomenal. But how does it compare to OpenAI itself or Google? See, at least the things that we are doing now, right? I mean, one of the things that when we thought about building server, we said we want to build a full stack generative AI company and different people have, and our understanding of full stack is that we need to know how to train models from scratch. We need to know how to kind of figure out how to deploy models to solve real world use cases. And we need to play in the ecosystem to make sure that we can actually deploy population scale applications. So we were thinking about all of these things. But still the models we were talking about are fairly small models. They are fairly small models, right? The 7 to maybe up to 70 billion kind of range we're talking about. While these models like OpenAI and Google are obviously much bigger models, right? But we want to understand the techniques and be able to build that muscle to do all of these things to make it available to people. Now those models are I mean, as I said, you know, I think that there is space for all of those things. And I think as even Sridhar was talking about earlier in the day, we believe that these smaller models can do very, I mean, many, many kind of domain specific tasks extremely well, probably even better than the larger models. And that is really one of the key areas. And so the further value of these kinds of things, right? We are not aiming in these models to build any AGI, right? That's not our goal here. Our goal is to make things that work extremely well for domain specific use cases or or increase accessibility through language and all of those kinds of things. And obviously all of this unique to India. But what is unique about India? I mean, like, what is, is anything special in our ecosystem that makes a small model focused with Indian languages better for more suited for our problems? So I think that, I mean, there are quite a few things that are unique about India, right? The first thing is I think that we are a voice first nation. So therefore, I think voice has to be the core to doing things. The other thing, of course, India is extremely, it's a cost-conscious country from a cost perspective. Now, I would say that there are lots of interesting use cases where you can use OpenAI and the cost structure works that when we're depending on your application. But when you want to scale things to a massive level and make it work, then you have to figure out how small models work. So that's something that is also specific to India. The third thing which is specific to India is really the success that India has had in building all this digital public infrastructure. When you add the AI layer on top of it, then you can actually get dramatic, you know, dramatic, I think, multiplicative combinatorial effects based on doing things like that. That's a phenomenal point. Like, you know, it's like DPI to the power of AI almost in some ways. And as a part of Adar building Adar, no better person than you. So in summary, what I'm hearing is small models specialized with trained with Indic specific language data suited for Indian problems at a compelling cost point. We'll be suited for us. We're not solving some world autonomous vehicles or some complex problem. We're solving some basic problems specifically focused on voice with multiple languages. That is what you see as a future. Am I paraphrasing this correctly? No, yeah. So I think that certainly, I mean, voice and Indian languages are an important part of our strategy, but we will be building, you know, custom models to solve various other kinds of problems as well, right? That's not just limited to, I think, in different domains, working in different domains, making building things based on unique data that enterprises have and things like that. So that's something that we'll also look at. Fair enough. So coming back to the elephant in the room, no fun intended with OpenHathe, what about Bavesh Akarwal and Kruthrim? What does your take on that? No, I think it's great. I think it's wonderful, right? I mean, the fact that the technology AI is so important that we need multiple people working on it. The fact that there are other people thinking is actually validates that this is an important problem to be solved. And I think that we need everybody to come together and do that. So I really welcome that. I think it's great. And I think that there'll be different people who will have different takes as to how to solve this kind of problem. And hopefully as a result of that, the entire ecosystem benefits. One more question, and then I want to talk about some of the predictions that you've boldly made. So Vivek, I usually ask people about what do you think the future will be, and everybody usually hedges. I ask Vivek, what do you think is going to happen by December 2024? What do you think sitting in this room one year later, we can expect? And you made three bold predictions. So I want to talk about that before that I have one last question. What are the top three applications that you think are relevant for India? You would see the talk about medical, when any quick summary, what do you think the top three apps are for India for AI? So I mean, I think that as you said, things like education and medical are clearly areas where I think that things can be leveraged. The whole idea of all these kind of the DPI aspect of it is another major application where things can happen. And here I'm talking about country specific. And I think the whole idea which we've also talked about was the concept of software. And I think that clearly we have a very large software industry and how to reimagine those things in this context is also something that's quite fair enough. Are you guys ready for Vivek Ragavan's bold predictions? Yes? No, I'm not hearing any, yes, this is like a big deal. He's like one of the smartest guys that I know. He wants to make three predictions. You don't want to hear it. All right. So I asked him, what do you think, you know, year later, what do you think we can expect? And he came up with three things and usually people give very blind answers when you ask questions like this because they don't want to be caught wrong. Not Vivek. Vivek is bold. So he basically said three things and I'm going to list out the three things and then he's asking about it. So number one, he says, I will prefer to talk to an automated customer service than a real person because they'll give me a better answer. So that is Vivek Ragavan's prediction number one. So number two is that when everybody is talking about a GPU shortage, Vivek predicts that there'll be a GPU glutton India. He thinks there'll be too much GPU. Okay. So if you want a short and media stock, this is a good time. And number three, which was extremely unexpected, he said some companies will suddenly die. Okay. So Vivek, these are not what I expected. So you want to quickly talk about each of them, why you just came up with these and then we'll throw the audience questions. So I don't think I quite said it the way that that Vala Rianne is. But it's interesting. But I think the first thing that we said is I think that and I don't think that this is I think there will come a time when in areas of customer service, et cetera, when you want to do something very specific, today when you call when you call some kind of a bot, you actually end up, you mostly try to disconnect the call or you're extremely upset that you're talking to a bot. But I think that there will come a time and I'm predicting it as sooner than later that you will actually get better responses from the bot than what the human representative that at least the average human representative that you could talk to could give. And I think that that's just a, I just said that that there will come a time where you know, it's not a human you're talking to, but it's probably more likely to solve your intent than the human person. That's just something that I think that could happen. Definitely controversial, but we'll let it go. What about the GPU glut? No, I don't think that. So I think that the fact that there is a tremendous shortage right now, I think that shortage will ease because that is how the cycles of things go, right? When I think the fact that there was such a severe shortage last year, you know, basically caused a number of different players to ramp up in various kinds of forms. And I think that that will always go in a cycle. But you may find out that there are many, many more interesting problems that people will be able to solve. I still remember, you know, we were at a Genie I event in Bangalore and we were talking to people and we said, you know, how many people have access to, you know, four, eight hundred, so this was the question that I asked. And nobody in the room, and these are all extremely enthusiastic Genie I people and nobody had access. And I think that thing is going to change. You will be able to get these kinds of things and people who want to hack and do things will have access to these things in without, you know, having to write a, you know, a way to check. Vivek is also a semi-conductive guy before he went into other. So I would take his predictions very seriously. So I don't know what I, I'm going to sell my immediate stock. I would not do that, but that's not what I said. I want to blame you for this. I could go that. But the third one is pretty strange. You know, companies are born, companies die, but you said some companies will suddenly die. What does that mean? No, I think, see, I think the interesting thing is, and I think that it comes back to the fundamental nature of AI. AI is a tool, right? And you have to use that. And you have to use that within your business process, right? And how AI is being used, and so, and what's going to happen is that, I mean, I think this is true with, you know, when someone said in terms of, you know, people, they said, that the people who leverage AI will be, will, will be more effective than those who don't leverage AI. And that will, too, for organizations also. Organizations that leverage AI in, fundamentally in their core business processes, will be more effective than those who don't, right? And I think that's the thing. And you won't know the difference until one day it becomes too obvious, and it will be too late. And I think that's the reason why everybody needs to think about what it means for your business. Because you will, everything will be fine. Everything will be fine. And one day, somebody in your, either, either, either your competitor, your space, or somebody brand new coming into your space will be reimagining your business process completely. And at that stage, you will find that it's, you know, it's a very big, very tall, you know, mountain to climb. And that's why I think it's important for both people and entities to think about how they will, you know, they will upgrade themselves or they will modify their business processes to an, you know, to a certain extent. That's a very nuanced answer. And everybody here who's running a business should really think about it because life will be the same. And then suddenly, suddenly something will, you know, there will be a step change. We make a few more questions, but I'm sure the audience has a lot of questions for you. So how are we doing on time? Okay. So does, okay, a lot of questions, so love to, is there a mic that we can pass on? Thank you. My name is Kartik. I work for IT Service Industry. So you're saying that you're working on LLM, sorry, it's a fine tuned LLM on top of LAMA. My basic question, fundamental question is we don't have a foundation model for India. Most of the models are basically using English or those kind of things. So for example, Andrew was talking about the tokenizers and things like that. So are you working on anything like that? Or you want to use mostly the existing models and run on top of it? You asked a good question. You asked the cherry question for himself. Well, I think the interesting thing is that if you look at, and then we have actually a blog on this on our website, I think one of the things that we've actually built a customized tokenizer, which actually fundamentally changes the cost of some of these generations in Indian languages. And I think that we're not just fine tuning. We're actually, we are leveraging the existing retaining, but we are doing what's known as continual free training, but having said that, I think that when we have to figure out where is the data to train an extremely large model from scratch and some of those things are things which will happen over time. But I think that, I think that yes, I think that we will be doing various kinds of things, but the interesting thing is that if I want to change the accessibility problem with an existing open source model, how do I do that? And that's the problem that we have that we think we have solved and is going to be the heart of this open RTC series. It's extremely well explained in the blog even I could understand it so. Hi, I'm Prishant. I work for a FITEC company. My question is like, unlike China, we never had a consumer facing application coming out from India and in Web 1, Web 2, crypto and all. Why do you think it will be different this time in like AI? Because will the BPI and other things will solve the same purpose, but the great five world did in China or do you think like in because AI is a strategic sector, no outside country can work in NASA projects maybe or government content will go to them. What is at least the mode here for an Indian company? So I don't I think I think the the question is I don't know the answer to these questions, right? I mean, I think that it's difficult to predict, but I do believe, and as I'm repeating, that the combinatorial effect of being using Genai at a large scale in addition, along with the DPI work that we've done in India, will have people. And I think that in the end, it is the intent is that people need to be able to use it and they will vote by things that are useful for them. And if that doesn't happen, you're right. I think that we have to figure out what is the mechanism of delivery of apps, right? In Bhojhaut, where do Indians consume content? That's a question. I'm so sorry, but we are out of time. Vivek will be outside. So he would be able to answer the question. We have time for one last question. Can I just take one last? Thank you. I'm Manish Kudhari. I'm from ISBR Business School. Good that I got a chance to ask you this question. During lunchtime, there were a few of our educationists whom we were talking about, and there was one from school, and we are from the MBA institutions. We were thinking of these present generations, how do we get them into what you are doing? There is one thing that they have been regularly, that the concentrations that they're working on, but artificial intelligence, and getting into this, getting them into their academics, and making them a part of it is very important, including the trainers who train them, making them future-ready into what you are doing is amazing. And the speed that which is growing, it is calling for a lot of training that needs to be done. Can you from your angle through some light on how we could make them future-ready, how these people who are management graduates and from schools who are coming out, how do we get into this part of technology that you spoke about? So this is really a challenge, because I think everyone will need to understand at some level what this technology does, and I think that we have to rethink how we get everyone into this, and that this kind of education has to be at many different levels. There are from a core set of having people who are extremely good at some, and there you don't need as many, but then there are basically vast numbers of people who can actually leverage these tools. By the way, the most important thing about, and maybe that's part of what makes an LLM interesting, is that how you use it, your mileage varies by that, and to understand how to actually leverage this in an interesting way is something that we have to widely teach many, many people, and because asking the things in the right way and having the right kind of applications will make a huge difference to how people can leverage these tools. Thank you. Thank you very much. We'll make a very good luck to Sarvam and good luck to India. I think it's going to be a lot right on the show, guys. Thanks, Bala. Thank you, Mr. Raghavan, and\",\n",
              " 'segments': [{'id': 0,\n",
              "   'seek': 0,\n",
              "   'start': 0.0,\n",
              "   'end': 3.6,\n",
              "   'text': ' Congratulations to you Mr. Raghavan for that. Thank you so much for joining us.',\n",
              "   'tokens': [50364,\n",
              "    9694,\n",
              "    281,\n",
              "    291,\n",
              "    2221,\n",
              "    13,\n",
              "    497,\n",
              "    14842,\n",
              "    21071,\n",
              "    337,\n",
              "    300,\n",
              "    13,\n",
              "    1044,\n",
              "    291,\n",
              "    370,\n",
              "    709,\n",
              "    337,\n",
              "    5549,\n",
              "    505,\n",
              "    13,\n",
              "    50544],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.30939904126254,\n",
              "   'compression_ratio': 1.4407582938388626,\n",
              "   'no_speech_prob': 0.10069405287504196},\n",
              "  {'id': 1,\n",
              "   'seek': 0,\n",
              "   'start': 3.6,\n",
              "   'end': 5.6000000000000005,\n",
              "   'text': ' Over to you.',\n",
              "   'tokens': [50544, 4886, 281, 291, 13, 50644],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.30939904126254,\n",
              "   'compression_ratio': 1.4407582938388626,\n",
              "   'no_speech_prob': 0.10069405287504196},\n",
              "  {'id': 2,\n",
              "   'seek': 0,\n",
              "   'start': 8.56,\n",
              "   'end': 10.0,\n",
              "   'text': ' Hi everybody. How are you?',\n",
              "   'tokens': [50792, 2421, 2201, 13, 1012, 366, 291, 30, 50864],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.30939904126254,\n",
              "   'compression_ratio': 1.4407582938388626,\n",
              "   'no_speech_prob': 0.10069405287504196},\n",
              "  {'id': 3,\n",
              "   'seek': 0,\n",
              "   'start': 11.76,\n",
              "   'end': 16.88,\n",
              "   'text': \" Okay I am not hearing this at all. It's like a post lunch energy downer or something.\",\n",
              "   'tokens': [50952,\n",
              "    1033,\n",
              "    286,\n",
              "    669,\n",
              "    406,\n",
              "    4763,\n",
              "    341,\n",
              "    412,\n",
              "    439,\n",
              "    13,\n",
              "    467,\n",
              "    311,\n",
              "    411,\n",
              "    257,\n",
              "    2183,\n",
              "    6349,\n",
              "    2281,\n",
              "    760,\n",
              "    260,\n",
              "    420,\n",
              "    746,\n",
              "    13,\n",
              "    51208],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.30939904126254,\n",
              "   'compression_ratio': 1.4407582938388626,\n",
              "   'no_speech_prob': 0.10069405287504196},\n",
              "  {'id': 4,\n",
              "   'seek': 0,\n",
              "   'start': 18.080000000000002,\n",
              "   'end': 20.48,\n",
              "   'text': \" Let's hear it. Are you guys awake?\",\n",
              "   'tokens': [51268,\n",
              "    961,\n",
              "    311,\n",
              "    1568,\n",
              "    309,\n",
              "    13,\n",
              "    2014,\n",
              "    291,\n",
              "    1074,\n",
              "    15994,\n",
              "    30,\n",
              "    51388],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.30939904126254,\n",
              "   'compression_ratio': 1.4407582938388626,\n",
              "   'no_speech_prob': 0.10069405287504196},\n",
              "  {'id': 5,\n",
              "   'seek': 0,\n",
              "   'start': 21.84,\n",
              "   'end': 26.8,\n",
              "   'text': ' All right you better be because we have a superstar guest here.',\n",
              "   'tokens': [51456,\n",
              "    1057,\n",
              "    558,\n",
              "    291,\n",
              "    1101,\n",
              "    312,\n",
              "    570,\n",
              "    321,\n",
              "    362,\n",
              "    257,\n",
              "    38953,\n",
              "    8341,\n",
              "    510,\n",
              "    13,\n",
              "    51704],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.30939904126254,\n",
              "   'compression_ratio': 1.4407582938388626,\n",
              "   'no_speech_prob': 0.10069405287504196},\n",
              "  {'id': 6,\n",
              "   'seek': 2680,\n",
              "   'start': 27.6,\n",
              "   'end': 31.52,\n",
              "   'text': \" You heard the 41 million dollars and I didn't hear honestly anything she said after that.\",\n",
              "   'tokens': [50404,\n",
              "    509,\n",
              "    2198,\n",
              "    264,\n",
              "    18173,\n",
              "    2459,\n",
              "    3808,\n",
              "    293,\n",
              "    286,\n",
              "    994,\n",
              "    380,\n",
              "    1568,\n",
              "    6095,\n",
              "    1340,\n",
              "    750,\n",
              "    848,\n",
              "    934,\n",
              "    300,\n",
              "    13,\n",
              "    50600],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.22247144911024305,\n",
              "   'compression_ratio': 1.5121951219512195,\n",
              "   'no_speech_prob': 0.07092534005641937},\n",
              "  {'id': 7,\n",
              "   'seek': 2680,\n",
              "   'start': 33.120000000000005,\n",
              "   'end': 38.0,\n",
              "   'text': \" So we're going to ask for about 40 million dollars from him by the end of this conversation.\",\n",
              "   'tokens': [50680,\n",
              "    407,\n",
              "    321,\n",
              "    434,\n",
              "    516,\n",
              "    281,\n",
              "    1029,\n",
              "    337,\n",
              "    466,\n",
              "    3356,\n",
              "    2459,\n",
              "    3808,\n",
              "    490,\n",
              "    796,\n",
              "    538,\n",
              "    264,\n",
              "    917,\n",
              "    295,\n",
              "    341,\n",
              "    3761,\n",
              "    13,\n",
              "    50924],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.22247144911024305,\n",
              "   'compression_ratio': 1.5121951219512195,\n",
              "   'no_speech_prob': 0.07092534005641937},\n",
              "  {'id': 8,\n",
              "   'seek': 2680,\n",
              "   'start': 39.68,\n",
              "   'end': 44.88,\n",
              "   'text': \" But let's get started. I want to introduce Vivek and Pratius, she's co-founder who's not here.\",\n",
              "   'tokens': [51008,\n",
              "    583,\n",
              "    718,\n",
              "    311,\n",
              "    483,\n",
              "    1409,\n",
              "    13,\n",
              "    286,\n",
              "    528,\n",
              "    281,\n",
              "    5366,\n",
              "    44288,\n",
              "    74,\n",
              "    293,\n",
              "    2114,\n",
              "    267,\n",
              "    4872,\n",
              "    11,\n",
              "    750,\n",
              "    311,\n",
              "    598,\n",
              "    12,\n",
              "    33348,\n",
              "    567,\n",
              "    311,\n",
              "    406,\n",
              "    510,\n",
              "    13,\n",
              "    51268],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.22247144911024305,\n",
              "   'compression_ratio': 1.5121951219512195,\n",
              "   'no_speech_prob': 0.07092534005641937},\n",
              "  {'id': 9,\n",
              "   'seek': 2680,\n",
              "   'start': 45.68,\n",
              "   'end': 52.400000000000006,\n",
              "   'text': ' We wanted to start with a playing a video of what OpenHathe does. I encourage all of you to go',\n",
              "   'tokens': [51308,\n",
              "    492,\n",
              "    1415,\n",
              "    281,\n",
              "    722,\n",
              "    365,\n",
              "    257,\n",
              "    2433,\n",
              "    257,\n",
              "    960,\n",
              "    295,\n",
              "    437,\n",
              "    7238,\n",
              "    39,\n",
              "    267,\n",
              "    675,\n",
              "    775,\n",
              "    13,\n",
              "    286,\n",
              "    5373,\n",
              "    439,\n",
              "    295,\n",
              "    291,\n",
              "    281,\n",
              "    352,\n",
              "    51644],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.22247144911024305,\n",
              "   'compression_ratio': 1.5121951219512195,\n",
              "   'no_speech_prob': 0.07092534005641937},\n",
              "  {'id': 10,\n",
              "   'seek': 5240,\n",
              "   'start': 53.12,\n",
              "   'end': 59.199999999999996,\n",
              "   'text': ' to the website, www.severalm.ai and check it out. But let me start by introducing Vivek. Vivek is a',\n",
              "   'tokens': [50400,\n",
              "    281,\n",
              "    264,\n",
              "    3144,\n",
              "    11,\n",
              "    12520,\n",
              "    13,\n",
              "    405,\n",
              "    331,\n",
              "    304,\n",
              "    76,\n",
              "    13,\n",
              "    1301,\n",
              "    293,\n",
              "    1520,\n",
              "    309,\n",
              "    484,\n",
              "    13,\n",
              "    583,\n",
              "    718,\n",
              "    385,\n",
              "    722,\n",
              "    538,\n",
              "    15424,\n",
              "    44288,\n",
              "    74,\n",
              "    13,\n",
              "    44288,\n",
              "    74,\n",
              "    307,\n",
              "    257,\n",
              "    50704],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.29414040219467297,\n",
              "   'compression_ratio': 1.5612648221343874,\n",
              "   'no_speech_prob': 0.07092847675085068},\n",
              "  {'id': 11,\n",
              "   'seek': 5240,\n",
              "   'start': 59.199999999999996,\n",
              "   'end': 65.36,\n",
              "   'text': ' dear friend and he is very, very modest. One of the most modest guys that I know. But his personal',\n",
              "   'tokens': [50704,\n",
              "    6875,\n",
              "    1277,\n",
              "    293,\n",
              "    415,\n",
              "    307,\n",
              "    588,\n",
              "    11,\n",
              "    588,\n",
              "    25403,\n",
              "    13,\n",
              "    1485,\n",
              "    295,\n",
              "    264,\n",
              "    881,\n",
              "    25403,\n",
              "    1074,\n",
              "    300,\n",
              "    286,\n",
              "    458,\n",
              "    13,\n",
              "    583,\n",
              "    702,\n",
              "    2973,\n",
              "    51012],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.29414040219467297,\n",
              "   'compression_ratio': 1.5612648221343874,\n",
              "   'no_speech_prob': 0.07092847675085068},\n",
              "  {'id': 12,\n",
              "   'seek': 5240,\n",
              "   'start': 65.36,\n",
              "   'end': 71.68,\n",
              "   'text': \" journey Vivek, you've got a PhD from Carnegie Mellon. You've sat in and sold the company to Magma.\",\n",
              "   'tokens': [51012,\n",
              "    4671,\n",
              "    44288,\n",
              "    74,\n",
              "    11,\n",
              "    291,\n",
              "    600,\n",
              "    658,\n",
              "    257,\n",
              "    14476,\n",
              "    490,\n",
              "    47301,\n",
              "    376,\n",
              "    898,\n",
              "    266,\n",
              "    13,\n",
              "    509,\n",
              "    600,\n",
              "    3227,\n",
              "    294,\n",
              "    293,\n",
              "    3718,\n",
              "    264,\n",
              "    2237,\n",
              "    281,\n",
              "    6395,\n",
              "    1696,\n",
              "    13,\n",
              "    51328],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.29414040219467297,\n",
              "   'compression_ratio': 1.5612648221343874,\n",
              "   'no_speech_prob': 0.07092847675085068},\n",
              "  {'id': 13,\n",
              "   'seek': 5240,\n",
              "   'start': 72.24,\n",
              "   'end': 77.68,\n",
              "   'text': \" Vivek and I moved back to India from both in the valley on the same day actually. And you've been\",\n",
              "   'tokens': [51356,\n",
              "    44288,\n",
              "    74,\n",
              "    293,\n",
              "    286,\n",
              "    4259,\n",
              "    646,\n",
              "    281,\n",
              "    5282,\n",
              "    490,\n",
              "    1293,\n",
              "    294,\n",
              "    264,\n",
              "    17636,\n",
              "    322,\n",
              "    264,\n",
              "    912,\n",
              "    786,\n",
              "    767,\n",
              "    13,\n",
              "    400,\n",
              "    291,\n",
              "    600,\n",
              "    668,\n",
              "    51628],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.29414040219467297,\n",
              "   'compression_ratio': 1.5612648221343874,\n",
              "   'no_speech_prob': 0.07092847675085068},\n",
              "  {'id': 14,\n",
              "   'seek': 7768,\n",
              "   'start': 77.68,\n",
              "   'end': 84.4,\n",
              "   'text': \" in India for the last 16 years. And what most people don't know is your journey at Adhar.\",\n",
              "   'tokens': [50364,\n",
              "    294,\n",
              "    5282,\n",
              "    337,\n",
              "    264,\n",
              "    1036,\n",
              "    3165,\n",
              "    924,\n",
              "    13,\n",
              "    400,\n",
              "    437,\n",
              "    881,\n",
              "    561,\n",
              "    500,\n",
              "    380,\n",
              "    458,\n",
              "    307,\n",
              "    428,\n",
              "    4671,\n",
              "    412,\n",
              "    1999,\n",
              "    5854,\n",
              "    13,\n",
              "    50700],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1539433429115697,\n",
              "   'compression_ratio': 1.5732217573221758,\n",
              "   'no_speech_prob': 0.02816513180732727},\n",
              "  {'id': 15,\n",
              "   'seek': 7768,\n",
              "   'start': 85.12,\n",
              "   'end': 93.68,\n",
              "   'text': ' He spent 13 years selflessly at Adhar. Nobody would have heard of him. But he was a pioneering',\n",
              "   'tokens': [50736,\n",
              "    634,\n",
              "    4418,\n",
              "    3705,\n",
              "    924,\n",
              "    2698,\n",
              "    12048,\n",
              "    412,\n",
              "    1999,\n",
              "    5854,\n",
              "    13,\n",
              "    9297,\n",
              "    576,\n",
              "    362,\n",
              "    2198,\n",
              "    295,\n",
              "    796,\n",
              "    13,\n",
              "    583,\n",
              "    415,\n",
              "    390,\n",
              "    257,\n",
              "    19761,\n",
              "    1794,\n",
              "    51164],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1539433429115697,\n",
              "   'compression_ratio': 1.5732217573221758,\n",
              "   'no_speech_prob': 0.02816513180732727},\n",
              "  {'id': 16,\n",
              "   'seek': 7768,\n",
              "   'start': 93.68,\n",
              "   'end': 100.08000000000001,\n",
              "   'text': ' technology visionary behind Adhar which we all take for granted today. So please give it out.',\n",
              "   'tokens': [51164,\n",
              "    2899,\n",
              "    49442,\n",
              "    2261,\n",
              "    1999,\n",
              "    5854,\n",
              "    597,\n",
              "    321,\n",
              "    439,\n",
              "    747,\n",
              "    337,\n",
              "    12344,\n",
              "    965,\n",
              "    13,\n",
              "    407,\n",
              "    1767,\n",
              "    976,\n",
              "    309,\n",
              "    484,\n",
              "    13,\n",
              "    51484],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1539433429115697,\n",
              "   'compression_ratio': 1.5732217573221758,\n",
              "   'no_speech_prob': 0.02816513180732727},\n",
              "  {'id': 17,\n",
              "   'seek': 7768,\n",
              "   'start': 102.32000000000001,\n",
              "   'end': 106.64000000000001,\n",
              "   'text': ' Honestly when people, when I think of selfless service, truly selfless service, I always think of',\n",
              "   'tokens': [51596,\n",
              "    12348,\n",
              "    562,\n",
              "    561,\n",
              "    11,\n",
              "    562,\n",
              "    286,\n",
              "    519,\n",
              "    295,\n",
              "    2698,\n",
              "    1832,\n",
              "    2643,\n",
              "    11,\n",
              "    4908,\n",
              "    2698,\n",
              "    1832,\n",
              "    2643,\n",
              "    11,\n",
              "    286,\n",
              "    1009,\n",
              "    519,\n",
              "    295,\n",
              "    51812],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1539433429115697,\n",
              "   'compression_ratio': 1.5732217573221758,\n",
              "   'no_speech_prob': 0.02816513180732727},\n",
              "  {'id': 18,\n",
              "   'seek': 10664,\n",
              "   'start': 106.64,\n",
              "   'end': 113.28,\n",
              "   'text': \" Vivek. And since then he also was at AI for Bharat which we're going to touch on where he met\",\n",
              "   'tokens': [50364,\n",
              "    44288,\n",
              "    74,\n",
              "    13,\n",
              "    400,\n",
              "    1670,\n",
              "    550,\n",
              "    415,\n",
              "    611,\n",
              "    390,\n",
              "    412,\n",
              "    7318,\n",
              "    337,\n",
              "    49104,\n",
              "    267,\n",
              "    597,\n",
              "    321,\n",
              "    434,\n",
              "    516,\n",
              "    281,\n",
              "    2557,\n",
              "    322,\n",
              "    689,\n",
              "    415,\n",
              "    1131,\n",
              "    50696],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18974298949635357,\n",
              "   'compression_ratio': 1.528,\n",
              "   'no_speech_prob': 0.017507852986454964},\n",
              "  {'id': 19,\n",
              "   'seek': 10664,\n",
              "   'start': 113.28,\n",
              "   'end': 120.24,\n",
              "   'text': \" Pratiyusha's other co-founder. Pratiyusha had a PhD from ETH at Zurich. He was in the IBM research.\",\n",
              "   'tokens': [50696,\n",
              "    2114,\n",
              "    267,\n",
              "    4727,\n",
              "    1498,\n",
              "    64,\n",
              "    311,\n",
              "    661,\n",
              "    598,\n",
              "    12,\n",
              "    33348,\n",
              "    13,\n",
              "    2114,\n",
              "    267,\n",
              "    4727,\n",
              "    1498,\n",
              "    64,\n",
              "    632,\n",
              "    257,\n",
              "    14476,\n",
              "    490,\n",
              "    462,\n",
              "    9620,\n",
              "    412,\n",
              "    30518,\n",
              "    480,\n",
              "    13,\n",
              "    634,\n",
              "    390,\n",
              "    294,\n",
              "    264,\n",
              "    23487,\n",
              "    2132,\n",
              "    13,\n",
              "    51044],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18974298949635357,\n",
              "   'compression_ratio': 1.528,\n",
              "   'no_speech_prob': 0.017507852986454964},\n",
              "  {'id': 20,\n",
              "   'seek': 10664,\n",
              "   'start': 120.24,\n",
              "   'end': 126.48,\n",
              "   'text': ' He was at Microsoft Research playing a key role and a faculty at IIT Madras and at AI for Bharat.',\n",
              "   'tokens': [51044,\n",
              "    634,\n",
              "    390,\n",
              "    412,\n",
              "    8116,\n",
              "    10303,\n",
              "    2433,\n",
              "    257,\n",
              "    2141,\n",
              "    3090,\n",
              "    293,\n",
              "    257,\n",
              "    6389,\n",
              "    412,\n",
              "    286,\n",
              "    3927,\n",
              "    5326,\n",
              "    3906,\n",
              "    293,\n",
              "    412,\n",
              "    7318,\n",
              "    337,\n",
              "    49104,\n",
              "    267,\n",
              "    13,\n",
              "    51356],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18974298949635357,\n",
              "   'compression_ratio': 1.528,\n",
              "   'no_speech_prob': 0.017507852986454964},\n",
              "  {'id': 21,\n",
              "   'seek': 10664,\n",
              "   'start': 126.48,\n",
              "   'end': 130.88,\n",
              "   'text': \" So that's a little brief introduction about them. These guys are modest, modest engineers.\",\n",
              "   'tokens': [51356,\n",
              "    407,\n",
              "    300,\n",
              "    311,\n",
              "    257,\n",
              "    707,\n",
              "    5353,\n",
              "    9339,\n",
              "    466,\n",
              "    552,\n",
              "    13,\n",
              "    1981,\n",
              "    1074,\n",
              "    366,\n",
              "    25403,\n",
              "    11,\n",
              "    25403,\n",
              "    11955,\n",
              "    13,\n",
              "    51576],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18974298949635357,\n",
              "   'compression_ratio': 1.528,\n",
              "   'no_speech_prob': 0.017507852986454964},\n",
              "  {'id': 22,\n",
              "   'seek': 13088,\n",
              "   'start': 131.84,\n",
              "   'end': 138.32,\n",
              "   'text': \" So they don't do their own hon. So forgive me for tooting their hon in this case. But let's jump\",\n",
              "   'tokens': [50412,\n",
              "    407,\n",
              "    436,\n",
              "    500,\n",
              "    380,\n",
              "    360,\n",
              "    641,\n",
              "    1065,\n",
              "    2157,\n",
              "    13,\n",
              "    407,\n",
              "    10718,\n",
              "    385,\n",
              "    337,\n",
              "    281,\n",
              "    17001,\n",
              "    641,\n",
              "    2157,\n",
              "    294,\n",
              "    341,\n",
              "    1389,\n",
              "    13,\n",
              "    583,\n",
              "    718,\n",
              "    311,\n",
              "    3012,\n",
              "    50736],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18317222595214844,\n",
              "   'compression_ratio': 1.5375494071146245,\n",
              "   'no_speech_prob': 0.11854211986064911},\n",
              "  {'id': 23,\n",
              "   'seek': 13088,\n",
              "   'start': 138.32,\n",
              "   'end': 146.48,\n",
              "   'text': \" right in about the money. Funding. 41 million bucks man. That's a lot of money. Every entrepreneur\",\n",
              "   'tokens': [50736,\n",
              "    558,\n",
              "    294,\n",
              "    466,\n",
              "    264,\n",
              "    1460,\n",
              "    13,\n",
              "    13493,\n",
              "    278,\n",
              "    13,\n",
              "    18173,\n",
              "    2459,\n",
              "    11829,\n",
              "    587,\n",
              "    13,\n",
              "    663,\n",
              "    311,\n",
              "    257,\n",
              "    688,\n",
              "    295,\n",
              "    1460,\n",
              "    13,\n",
              "    2048,\n",
              "    14307,\n",
              "    51144],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18317222595214844,\n",
              "   'compression_ratio': 1.5375494071146245,\n",
              "   'no_speech_prob': 0.11854211986064911},\n",
              "  {'id': 24,\n",
              "   'seek': 13088,\n",
              "   'start': 146.48,\n",
              "   'end': 151.51999999999998,\n",
              "   'text': ' here is saying what the hell did these guys do? What did the investors see to write such a big check?',\n",
              "   'tokens': [51144,\n",
              "    510,\n",
              "    307,\n",
              "    1566,\n",
              "    437,\n",
              "    264,\n",
              "    4921,\n",
              "    630,\n",
              "    613,\n",
              "    1074,\n",
              "    360,\n",
              "    30,\n",
              "    708,\n",
              "    630,\n",
              "    264,\n",
              "    11519,\n",
              "    536,\n",
              "    281,\n",
              "    2464,\n",
              "    1270,\n",
              "    257,\n",
              "    955,\n",
              "    1520,\n",
              "    30,\n",
              "    51396],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18317222595214844,\n",
              "   'compression_ratio': 1.5375494071146245,\n",
              "   'no_speech_prob': 0.11854211986064911},\n",
              "  {'id': 25,\n",
              "   'seek': 13088,\n",
              "   'start': 152.56,\n",
              "   'end': 159.84,\n",
              "   'text': \" No, I think it's a trend of what's going on in India. I think that for the very first time,\",\n",
              "   'tokens': [51448,\n",
              "    883,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    309,\n",
              "    311,\n",
              "    257,\n",
              "    6028,\n",
              "    295,\n",
              "    437,\n",
              "    311,\n",
              "    516,\n",
              "    322,\n",
              "    294,\n",
              "    5282,\n",
              "    13,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    337,\n",
              "    264,\n",
              "    588,\n",
              "    700,\n",
              "    565,\n",
              "    11,\n",
              "    51812],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18317222595214844,\n",
              "   'compression_ratio': 1.5375494071146245,\n",
              "   'no_speech_prob': 0.11854211986064911},\n",
              "  {'id': 26,\n",
              "   'seek': 15984,\n",
              "   'start': 160.4,\n",
              "   'end': 166.24,\n",
              "   'text': \" I think the investors have looked at, let's try and build something deep-tech out of the country.\",\n",
              "   'tokens': [50392,\n",
              "    286,\n",
              "    519,\n",
              "    264,\n",
              "    11519,\n",
              "    362,\n",
              "    2956,\n",
              "    412,\n",
              "    11,\n",
              "    718,\n",
              "    311,\n",
              "    853,\n",
              "    293,\n",
              "    1322,\n",
              "    746,\n",
              "    2452,\n",
              "    12,\n",
              "    25970,\n",
              "    484,\n",
              "    295,\n",
              "    264,\n",
              "    1941,\n",
              "    13,\n",
              "    50684],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18326795366075305,\n",
              "   'compression_ratio': 1.6905829596412556,\n",
              "   'no_speech_prob': 0.021543404087424278},\n",
              "  {'id': 27,\n",
              "   'seek': 15984,\n",
              "   'start': 166.24,\n",
              "   'end': 170.88,\n",
              "   'text': \" And let's try to figure out how to build something as a foundational technology out of the country.\",\n",
              "   'tokens': [50684,\n",
              "    400,\n",
              "    718,\n",
              "    311,\n",
              "    853,\n",
              "    281,\n",
              "    2573,\n",
              "    484,\n",
              "    577,\n",
              "    281,\n",
              "    1322,\n",
              "    746,\n",
              "    382,\n",
              "    257,\n",
              "    32195,\n",
              "    2899,\n",
              "    484,\n",
              "    295,\n",
              "    264,\n",
              "    1941,\n",
              "    13,\n",
              "    50916],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18326795366075305,\n",
              "   'compression_ratio': 1.6905829596412556,\n",
              "   'no_speech_prob': 0.021543404087424278},\n",
              "  {'id': 28,\n",
              "   'seek': 15984,\n",
              "   'start': 170.88,\n",
              "   'end': 180.32,\n",
              "   'text': \" And that's really what's really exciting. And I think that about, as Bala was mentioning for\",\n",
              "   'tokens': [50916,\n",
              "    400,\n",
              "    300,\n",
              "    311,\n",
              "    534,\n",
              "    437,\n",
              "    311,\n",
              "    534,\n",
              "    4670,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    466,\n",
              "    11,\n",
              "    382,\n",
              "    363,\n",
              "    5159,\n",
              "    390,\n",
              "    18315,\n",
              "    337,\n",
              "    51388],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18326795366075305,\n",
              "   'compression_ratio': 1.6905829596412556,\n",
              "   'no_speech_prob': 0.021543404087424278},\n",
              "  {'id': 29,\n",
              "   'seek': 15984,\n",
              "   'start': 180.32,\n",
              "   'end': 188.48000000000002,\n",
              "   'text': \" the past 15 years, I've been working in both digital public infrastructure and kind of\",\n",
              "   'tokens': [51388,\n",
              "    264,\n",
              "    1791,\n",
              "    2119,\n",
              "    924,\n",
              "    11,\n",
              "    286,\n",
              "    600,\n",
              "    668,\n",
              "    1364,\n",
              "    294,\n",
              "    1293,\n",
              "    4562,\n",
              "    1908,\n",
              "    6896,\n",
              "    293,\n",
              "    733,\n",
              "    295,\n",
              "    51796],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18326795366075305,\n",
              "   'compression_ratio': 1.6905829596412556,\n",
              "   'no_speech_prob': 0.021543404087424278},\n",
              "  {'id': 30,\n",
              "   'seek': 18848,\n",
              "   'start': 189.28,\n",
              "   'end': 193.6,\n",
              "   'text': ' non-profit kind of things. But when this whole thing of generative AI came about,',\n",
              "   'tokens': [50404,\n",
              "    2107,\n",
              "    12,\n",
              "    14583,\n",
              "    733,\n",
              "    295,\n",
              "    721,\n",
              "    13,\n",
              "    583,\n",
              "    562,\n",
              "    341,\n",
              "    1379,\n",
              "    551,\n",
              "    295,\n",
              "    1337,\n",
              "    1166,\n",
              "    7318,\n",
              "    1361,\n",
              "    466,\n",
              "    11,\n",
              "    50620],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1571468754818565,\n",
              "   'compression_ratio': 1.6506550218340612,\n",
              "   'no_speech_prob': 0.03598250448703766},\n",
              "  {'id': 31,\n",
              "   'seek': 18848,\n",
              "   'start': 195.2,\n",
              "   'end': 200.88,\n",
              "   'text': ' we said, okay, how can I actually make a difference in this space? And I said, maybe this is the',\n",
              "   'tokens': [50700,\n",
              "    321,\n",
              "    848,\n",
              "    11,\n",
              "    1392,\n",
              "    11,\n",
              "    577,\n",
              "    393,\n",
              "    286,\n",
              "    767,\n",
              "    652,\n",
              "    257,\n",
              "    2649,\n",
              "    294,\n",
              "    341,\n",
              "    1901,\n",
              "    30,\n",
              "    400,\n",
              "    286,\n",
              "    848,\n",
              "    11,\n",
              "    1310,\n",
              "    341,\n",
              "    307,\n",
              "    264,\n",
              "    50984],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1571468754818565,\n",
              "   'compression_ratio': 1.6506550218340612,\n",
              "   'no_speech_prob': 0.03598250448703766},\n",
              "  {'id': 32,\n",
              "   'seek': 18848,\n",
              "   'start': 200.88,\n",
              "   'end': 207.92,\n",
              "   'text': ' opportunity to actually come out and really build something. And the only way that we realize that',\n",
              "   'tokens': [50984,\n",
              "    2650,\n",
              "    281,\n",
              "    767,\n",
              "    808,\n",
              "    484,\n",
              "    293,\n",
              "    534,\n",
              "    1322,\n",
              "    746,\n",
              "    13,\n",
              "    400,\n",
              "    264,\n",
              "    787,\n",
              "    636,\n",
              "    300,\n",
              "    321,\n",
              "    4325,\n",
              "    300,\n",
              "    51336],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1571468754818565,\n",
              "   'compression_ratio': 1.6506550218340612,\n",
              "   'no_speech_prob': 0.03598250448703766},\n",
              "  {'id': 33,\n",
              "   'seek': 18848,\n",
              "   'start': 207.92,\n",
              "   'end': 215.12,\n",
              "   'text': \" you can do it is actually in the private sector. And I think that's, and then when we went out there\",\n",
              "   'tokens': [51336,\n",
              "    291,\n",
              "    393,\n",
              "    360,\n",
              "    309,\n",
              "    307,\n",
              "    767,\n",
              "    294,\n",
              "    264,\n",
              "    4551,\n",
              "    6977,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    311,\n",
              "    11,\n",
              "    293,\n",
              "    550,\n",
              "    562,\n",
              "    321,\n",
              "    1437,\n",
              "    484,\n",
              "    456,\n",
              "    51696],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1571468754818565,\n",
              "   'compression_ratio': 1.6506550218340612,\n",
              "   'no_speech_prob': 0.03598250448703766},\n",
              "  {'id': 34,\n",
              "   'seek': 21512,\n",
              "   'start': 215.12,\n",
              "   'end': 219.92000000000002,\n",
              "   'text': ' and we said we want to build something, which is a continuation. And fundamentally, the question is',\n",
              "   'tokens': [50364,\n",
              "    293,\n",
              "    321,\n",
              "    848,\n",
              "    321,\n",
              "    528,\n",
              "    281,\n",
              "    1322,\n",
              "    746,\n",
              "    11,\n",
              "    597,\n",
              "    307,\n",
              "    257,\n",
              "    29357,\n",
              "    13,\n",
              "    400,\n",
              "    17879,\n",
              "    11,\n",
              "    264,\n",
              "    1168,\n",
              "    307,\n",
              "    50604],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14113009603399979,\n",
              "   'compression_ratio': 1.7827715355805243,\n",
              "   'no_speech_prob': 0.014031416736543179},\n",
              "  {'id': 35,\n",
              "   'seek': 21512,\n",
              "   'start': 220.64000000000001,\n",
              "   'end': 225.44,\n",
              "   'text': ' the reason of what we want to do at Server May I is we want to basically make generative AI',\n",
              "   'tokens': [50640,\n",
              "    264,\n",
              "    1778,\n",
              "    295,\n",
              "    437,\n",
              "    321,\n",
              "    528,\n",
              "    281,\n",
              "    360,\n",
              "    412,\n",
              "    25684,\n",
              "    1891,\n",
              "    286,\n",
              "    307,\n",
              "    321,\n",
              "    528,\n",
              "    281,\n",
              "    1936,\n",
              "    652,\n",
              "    1337,\n",
              "    1166,\n",
              "    7318,\n",
              "    50880],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14113009603399979,\n",
              "   'compression_ratio': 1.7827715355805243,\n",
              "   'no_speech_prob': 0.014031416736543179},\n",
              "  {'id': 36,\n",
              "   'seek': 21512,\n",
              "   'start': 226.48000000000002,\n",
              "   'end': 232.0,\n",
              "   'text': \" available and accessible to the people in the country. And that's the intent. And when we said\",\n",
              "   'tokens': [50932,\n",
              "    2435,\n",
              "    293,\n",
              "    9515,\n",
              "    281,\n",
              "    264,\n",
              "    561,\n",
              "    294,\n",
              "    264,\n",
              "    1941,\n",
              "    13,\n",
              "    400,\n",
              "    300,\n",
              "    311,\n",
              "    264,\n",
              "    8446,\n",
              "    13,\n",
              "    400,\n",
              "    562,\n",
              "    321,\n",
              "    848,\n",
              "    51208],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14113009603399979,\n",
              "   'compression_ratio': 1.7827715355805243,\n",
              "   'no_speech_prob': 0.014031416736543179},\n",
              "  {'id': 37,\n",
              "   'seek': 21512,\n",
              "   'start': 232.0,\n",
              "   'end': 237.68,\n",
              "   'text': \" that we want to do this, there was a resonance in the investment community. And I think it's\",\n",
              "   'tokens': [51208,\n",
              "    300,\n",
              "    321,\n",
              "    528,\n",
              "    281,\n",
              "    360,\n",
              "    341,\n",
              "    11,\n",
              "    456,\n",
              "    390,\n",
              "    257,\n",
              "    30944,\n",
              "    294,\n",
              "    264,\n",
              "    6078,\n",
              "    1768,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    309,\n",
              "    311,\n",
              "    51492],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14113009603399979,\n",
              "   'compression_ratio': 1.7827715355805243,\n",
              "   'no_speech_prob': 0.014031416736543179},\n",
              "  {'id': 38,\n",
              "   'seek': 21512,\n",
              "   'start': 237.68,\n",
              "   'end': 243.68,\n",
              "   'text': ' a responsibility to really to show that something like this can be built out of India. So we see',\n",
              "   'tokens': [51492,\n",
              "    257,\n",
              "    6357,\n",
              "    281,\n",
              "    534,\n",
              "    281,\n",
              "    855,\n",
              "    300,\n",
              "    746,\n",
              "    411,\n",
              "    341,\n",
              "    393,\n",
              "    312,\n",
              "    3094,\n",
              "    484,\n",
              "    295,\n",
              "    5282,\n",
              "    13,\n",
              "    407,\n",
              "    321,\n",
              "    536,\n",
              "    51792],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14113009603399979,\n",
              "   'compression_ratio': 1.7827715355805243,\n",
              "   'no_speech_prob': 0.014031416736543179},\n",
              "  {'id': 39,\n",
              "   'seek': 24368,\n",
              "   'start': 243.76000000000002,\n",
              "   'end': 249.92000000000002,\n",
              "   'text': \" that as as as as confidence and a responsibility. And I also hope it's a trend that you know that there\",\n",
              "   'tokens': [50368,\n",
              "    300,\n",
              "    382,\n",
              "    382,\n",
              "    382,\n",
              "    382,\n",
              "    6687,\n",
              "    293,\n",
              "    257,\n",
              "    6357,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    611,\n",
              "    1454,\n",
              "    309,\n",
              "    311,\n",
              "    257,\n",
              "    6028,\n",
              "    300,\n",
              "    291,\n",
              "    458,\n",
              "    300,\n",
              "    456,\n",
              "    50676],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.21530433829504114,\n",
              "   'compression_ratio': 1.8339350180505416,\n",
              "   'no_speech_prob': 0.014186068437993526},\n",
              "  {'id': 40,\n",
              "   'seek': 24368,\n",
              "   'start': 249.92000000000002,\n",
              "   'end': 255.84,\n",
              "   'text': \" are many more people like us who are backed. Because if you look at it, maybe it's a large number in a\",\n",
              "   'tokens': [50676,\n",
              "    366,\n",
              "    867,\n",
              "    544,\n",
              "    561,\n",
              "    411,\n",
              "    505,\n",
              "    567,\n",
              "    366,\n",
              "    20391,\n",
              "    13,\n",
              "    1436,\n",
              "    498,\n",
              "    291,\n",
              "    574,\n",
              "    412,\n",
              "    309,\n",
              "    11,\n",
              "    1310,\n",
              "    309,\n",
              "    311,\n",
              "    257,\n",
              "    2416,\n",
              "    1230,\n",
              "    294,\n",
              "    257,\n",
              "    50972],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.21530433829504114,\n",
              "   'compression_ratio': 1.8339350180505416,\n",
              "   'no_speech_prob': 0.014186068437993526},\n",
              "  {'id': 41,\n",
              "   'seek': 24368,\n",
              "   'start': 256.64,\n",
              "   'end': 261.2,\n",
              "   'text': ' you know in the Indian context, but in the global context, I think there is just there should be',\n",
              "   'tokens': [51012,\n",
              "    291,\n",
              "    458,\n",
              "    294,\n",
              "    264,\n",
              "    6427,\n",
              "    4319,\n",
              "    11,\n",
              "    457,\n",
              "    294,\n",
              "    264,\n",
              "    4338,\n",
              "    4319,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    456,\n",
              "    307,\n",
              "    445,\n",
              "    456,\n",
              "    820,\n",
              "    312,\n",
              "    51240],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.21530433829504114,\n",
              "   'compression_ratio': 1.8339350180505416,\n",
              "   'no_speech_prob': 0.014186068437993526},\n",
              "  {'id': 42,\n",
              "   'seek': 24368,\n",
              "   'start': 261.2,\n",
              "   'end': 266.56,\n",
              "   'text': \" many, many more entrepreneurs who are backed to do things in India. So yeah, I'm going to come back to\",\n",
              "   'tokens': [51240,\n",
              "    867,\n",
              "    11,\n",
              "    867,\n",
              "    544,\n",
              "    12639,\n",
              "    567,\n",
              "    366,\n",
              "    20391,\n",
              "    281,\n",
              "    360,\n",
              "    721,\n",
              "    294,\n",
              "    5282,\n",
              "    13,\n",
              "    407,\n",
              "    1338,\n",
              "    11,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    808,\n",
              "    646,\n",
              "    281,\n",
              "    51508],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.21530433829504114,\n",
              "   'compression_ratio': 1.8339350180505416,\n",
              "   'no_speech_prob': 0.014186068437993526},\n",
              "  {'id': 43,\n",
              "   'seek': 24368,\n",
              "   'start': 266.56,\n",
              "   'end': 272.48,\n",
              "   'text': \" the many more entrepreneurs. I'm obviously going to ask you about Bhavesh's Krutri. So we're going to\",\n",
              "   'tokens': [51508,\n",
              "    264,\n",
              "    867,\n",
              "    544,\n",
              "    12639,\n",
              "    13,\n",
              "    286,\n",
              "    478,\n",
              "    2745,\n",
              "    516,\n",
              "    281,\n",
              "    1029,\n",
              "    291,\n",
              "    466,\n",
              "    13550,\n",
              "    706,\n",
              "    14935,\n",
              "    311,\n",
              "    6332,\n",
              "    325,\n",
              "    470,\n",
              "    13,\n",
              "    407,\n",
              "    321,\n",
              "    434,\n",
              "    516,\n",
              "    281,\n",
              "    51804],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.21530433829504114,\n",
              "   'compression_ratio': 1.8339350180505416,\n",
              "   'no_speech_prob': 0.014186068437993526},\n",
              "  {'id': 44,\n",
              "   'seek': 27248,\n",
              "   'start': 272.48,\n",
              "   'end': 278.48,\n",
              "   'text': ' come back to that question. But again, 41 million dollars. I mean, all of what you said, you know,',\n",
              "   'tokens': [50364,\n",
              "    808,\n",
              "    646,\n",
              "    281,\n",
              "    300,\n",
              "    1168,\n",
              "    13,\n",
              "    583,\n",
              "    797,\n",
              "    11,\n",
              "    18173,\n",
              "    2459,\n",
              "    3808,\n",
              "    13,\n",
              "    286,\n",
              "    914,\n",
              "    11,\n",
              "    439,\n",
              "    295,\n",
              "    437,\n",
              "    291,\n",
              "    848,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    50664],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15956013488769533,\n",
              "   'compression_ratio': 1.7007299270072993,\n",
              "   'no_speech_prob': 0.010956695303320885},\n",
              "  {'id': 45,\n",
              "   'seek': 27248,\n",
              "   'start': 278.48,\n",
              "   'end': 283.36,\n",
              "   'text': \" two million dollars, you know, that's a good amount of money for a startup which you know, which\",\n",
              "   'tokens': [50664,\n",
              "    732,\n",
              "    2459,\n",
              "    3808,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    300,\n",
              "    311,\n",
              "    257,\n",
              "    665,\n",
              "    2372,\n",
              "    295,\n",
              "    1460,\n",
              "    337,\n",
              "    257,\n",
              "    18578,\n",
              "    597,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    597,\n",
              "    50908],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15956013488769533,\n",
              "   'compression_ratio': 1.7007299270072993,\n",
              "   'no_speech_prob': 0.010956695303320885},\n",
              "  {'id': 46,\n",
              "   'seek': 27248,\n",
              "   'start': 283.36,\n",
              "   'end': 286.32,\n",
              "   'text': ' has not yet built anything. What are you going to do with all this money?',\n",
              "   'tokens': [50908,\n",
              "    575,\n",
              "    406,\n",
              "    1939,\n",
              "    3094,\n",
              "    1340,\n",
              "    13,\n",
              "    708,\n",
              "    366,\n",
              "    291,\n",
              "    516,\n",
              "    281,\n",
              "    360,\n",
              "    365,\n",
              "    439,\n",
              "    341,\n",
              "    1460,\n",
              "    30,\n",
              "    51056],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15956013488769533,\n",
              "   'compression_ratio': 1.7007299270072993,\n",
              "   'no_speech_prob': 0.010956695303320885},\n",
              "  {'id': 47,\n",
              "   'seek': 27248,\n",
              "   'start': 288.88,\n",
              "   'end': 294.24,\n",
              "   'text': \" I can have a perfect solution for the problem. I think in the last week I've got lots of calls,\",\n",
              "   'tokens': [51184,\n",
              "    286,\n",
              "    393,\n",
              "    362,\n",
              "    257,\n",
              "    2176,\n",
              "    3827,\n",
              "    337,\n",
              "    264,\n",
              "    1154,\n",
              "    13,\n",
              "    286,\n",
              "    519,\n",
              "    294,\n",
              "    264,\n",
              "    1036,\n",
              "    1243,\n",
              "    286,\n",
              "    600,\n",
              "    658,\n",
              "    3195,\n",
              "    295,\n",
              "    5498,\n",
              "    11,\n",
              "    51452],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15956013488769533,\n",
              "   'compression_ratio': 1.7007299270072993,\n",
              "   'no_speech_prob': 0.010956695303320885},\n",
              "  {'id': 48,\n",
              "   'seek': 27248,\n",
              "   'start': 294.24,\n",
              "   'end': 300.32,\n",
              "   'text': \" lots of people telling me how I can do it. But I know you first, okay, I'll be landed in the country\",\n",
              "   'tokens': [51452,\n",
              "    3195,\n",
              "    295,\n",
              "    561,\n",
              "    3585,\n",
              "    385,\n",
              "    577,\n",
              "    286,\n",
              "    393,\n",
              "    360,\n",
              "    309,\n",
              "    13,\n",
              "    583,\n",
              "    286,\n",
              "    458,\n",
              "    291,\n",
              "    700,\n",
              "    11,\n",
              "    1392,\n",
              "    11,\n",
              "    286,\n",
              "    603,\n",
              "    312,\n",
              "    15336,\n",
              "    294,\n",
              "    264,\n",
              "    1941,\n",
              "    51756],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15956013488769533,\n",
              "   'compression_ratio': 1.7007299270072993,\n",
              "   'no_speech_prob': 0.010956695303320885},\n",
              "  {'id': 49,\n",
              "   'seek': 30032,\n",
              "   'start': 300.32,\n",
              "   'end': 307.76,\n",
              "   'text': \" in the same day. I'm in the front of the queue. But honestly, I think the key thing in this is\",\n",
              "   'tokens': [50364,\n",
              "    294,\n",
              "    264,\n",
              "    912,\n",
              "    786,\n",
              "    13,\n",
              "    286,\n",
              "    478,\n",
              "    294,\n",
              "    264,\n",
              "    1868,\n",
              "    295,\n",
              "    264,\n",
              "    18639,\n",
              "    13,\n",
              "    583,\n",
              "    6095,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    264,\n",
              "    2141,\n",
              "    551,\n",
              "    294,\n",
              "    341,\n",
              "    307,\n",
              "    50736],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1412904895081812,\n",
              "   'compression_ratio': 1.7657657657657657,\n",
              "   'no_speech_prob': 0.031429655849933624},\n",
              "  {'id': 50,\n",
              "   'seek': 30032,\n",
              "   'start': 307.76,\n",
              "   'end': 313.2,\n",
              "   'text': ' to putting together an amazing team. And we actually have an amazing team, but we believe that it is',\n",
              "   'tokens': [50736,\n",
              "    281,\n",
              "    3372,\n",
              "    1214,\n",
              "    364,\n",
              "    2243,\n",
              "    1469,\n",
              "    13,\n",
              "    400,\n",
              "    321,\n",
              "    767,\n",
              "    362,\n",
              "    364,\n",
              "    2243,\n",
              "    1469,\n",
              "    11,\n",
              "    457,\n",
              "    321,\n",
              "    1697,\n",
              "    300,\n",
              "    309,\n",
              "    307,\n",
              "    51008],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1412904895081812,\n",
              "   'compression_ratio': 1.7657657657657657,\n",
              "   'no_speech_prob': 0.031429655849933624},\n",
              "  {'id': 51,\n",
              "   'seek': 30032,\n",
              "   'start': 313.2,\n",
              "   'end': 318.96,\n",
              "   'text': ' talent that will drive this kind of thing. And so it is to get key talent. And of course, the other',\n",
              "   'tokens': [51008,\n",
              "    8301,\n",
              "    300,\n",
              "    486,\n",
              "    3332,\n",
              "    341,\n",
              "    733,\n",
              "    295,\n",
              "    551,\n",
              "    13,\n",
              "    400,\n",
              "    370,\n",
              "    309,\n",
              "    307,\n",
              "    281,\n",
              "    483,\n",
              "    2141,\n",
              "    8301,\n",
              "    13,\n",
              "    400,\n",
              "    295,\n",
              "    1164,\n",
              "    11,\n",
              "    264,\n",
              "    661,\n",
              "    51296],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1412904895081812,\n",
              "   'compression_ratio': 1.7657657657657657,\n",
              "   'no_speech_prob': 0.031429655849933624},\n",
              "  {'id': 52,\n",
              "   'seek': 30032,\n",
              "   'start': 318.96,\n",
              "   'end': 325.44,\n",
              "   'text': ' thing is compute. This is extremely expensive compute wise to actually do these kinds of things.',\n",
              "   'tokens': [51296,\n",
              "    551,\n",
              "    307,\n",
              "    14722,\n",
              "    13,\n",
              "    639,\n",
              "    307,\n",
              "    4664,\n",
              "    5124,\n",
              "    14722,\n",
              "    10829,\n",
              "    281,\n",
              "    767,\n",
              "    360,\n",
              "    613,\n",
              "    3685,\n",
              "    295,\n",
              "    721,\n",
              "    13,\n",
              "    51620],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1412904895081812,\n",
              "   'compression_ratio': 1.7657657657657657,\n",
              "   'no_speech_prob': 0.031429655849933624},\n",
              "  {'id': 53,\n",
              "   'seek': 32544,\n",
              "   'start': 325.44,\n",
              "   'end': 330.24,\n",
              "   'text': \" And I think that those are the two primary things that you know, we'd use this for.\",\n",
              "   'tokens': [50364,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    729,\n",
              "    366,\n",
              "    264,\n",
              "    732,\n",
              "    6194,\n",
              "    721,\n",
              "    300,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    321,\n",
              "    1116,\n",
              "    764,\n",
              "    341,\n",
              "    337,\n",
              "    13,\n",
              "    50604],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19791620868747517,\n",
              "   'compression_ratio': 1.6088560885608856,\n",
              "   'no_speech_prob': 0.02085263654589653},\n",
              "  {'id': 54,\n",
              "   'seek': 32544,\n",
              "   'start': 332.88,\n",
              "   'end': 337.52,\n",
              "   'text': \" I'm computing in my own head as an entrepreneur. Talent, okay, you have like 2015 people.\",\n",
              "   'tokens': [50736,\n",
              "    286,\n",
              "    478,\n",
              "    15866,\n",
              "    294,\n",
              "    452,\n",
              "    1065,\n",
              "    1378,\n",
              "    382,\n",
              "    364,\n",
              "    14307,\n",
              "    13,\n",
              "    44081,\n",
              "    11,\n",
              "    1392,\n",
              "    11,\n",
              "    291,\n",
              "    362,\n",
              "    411,\n",
              "    7546,\n",
              "    561,\n",
              "    13,\n",
              "    50968],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19791620868747517,\n",
              "   'compression_ratio': 1.6088560885608856,\n",
              "   'no_speech_prob': 0.02085263654589653},\n",
              "  {'id': 55,\n",
              "   'seek': 32544,\n",
              "   'start': 337.52,\n",
              "   'end': 340.4,\n",
              "   'text': \" How much are you paying these guys? But okay, well, you won't touch on that.\",\n",
              "   'tokens': [50968,\n",
              "    1012,\n",
              "    709,\n",
              "    366,\n",
              "    291,\n",
              "    6229,\n",
              "    613,\n",
              "    1074,\n",
              "    30,\n",
              "    583,\n",
              "    1392,\n",
              "    11,\n",
              "    731,\n",
              "    11,\n",
              "    291,\n",
              "    1582,\n",
              "    380,\n",
              "    2557,\n",
              "    322,\n",
              "    300,\n",
              "    13,\n",
              "    51112],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19791620868747517,\n",
              "   'compression_ratio': 1.6088560885608856,\n",
              "   'no_speech_prob': 0.02085263654589653},\n",
              "  {'id': 56,\n",
              "   'seek': 32544,\n",
              "   'start': 341.52,\n",
              "   'end': 345.84,\n",
              "   'text': \" But let's talk about what you guys actually built. What is open Hathi? How would you explain\",\n",
              "   'tokens': [51168,\n",
              "    583,\n",
              "    718,\n",
              "    311,\n",
              "    751,\n",
              "    466,\n",
              "    437,\n",
              "    291,\n",
              "    1074,\n",
              "    767,\n",
              "    3094,\n",
              "    13,\n",
              "    708,\n",
              "    307,\n",
              "    1269,\n",
              "    389,\n",
              "    42715,\n",
              "    30,\n",
              "    1012,\n",
              "    576,\n",
              "    291,\n",
              "    2903,\n",
              "    51384],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19791620868747517,\n",
              "   'compression_ratio': 1.6088560885608856,\n",
              "   'no_speech_prob': 0.02085263654589653},\n",
              "  {'id': 57,\n",
              "   'seek': 32544,\n",
              "   'start': 345.84,\n",
              "   'end': 351.2,\n",
              "   'text': ' open Hathi to? Many people here who might not have known about it. So I think open Hathi is,',\n",
              "   'tokens': [51384,\n",
              "    1269,\n",
              "    389,\n",
              "    42715,\n",
              "    281,\n",
              "    30,\n",
              "    5126,\n",
              "    561,\n",
              "    510,\n",
              "    567,\n",
              "    1062,\n",
              "    406,\n",
              "    362,\n",
              "    2570,\n",
              "    466,\n",
              "    309,\n",
              "    13,\n",
              "    407,\n",
              "    286,\n",
              "    519,\n",
              "    1269,\n",
              "    389,\n",
              "    42715,\n",
              "    307,\n",
              "    11,\n",
              "    51652],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19791620868747517,\n",
              "   'compression_ratio': 1.6088560885608856,\n",
              "   'no_speech_prob': 0.02085263654589653},\n",
              "  {'id': 58,\n",
              "   'seek': 35120,\n",
              "   'start': 351.2,\n",
              "   'end': 357.76,\n",
              "   'text': ' so first of all, right, we come from, I personally come from the open source ecosystem and we,',\n",
              "   'tokens': [50364,\n",
              "    370,\n",
              "    700,\n",
              "    295,\n",
              "    439,\n",
              "    11,\n",
              "    558,\n",
              "    11,\n",
              "    321,\n",
              "    808,\n",
              "    490,\n",
              "    11,\n",
              "    286,\n",
              "    5665,\n",
              "    808,\n",
              "    490,\n",
              "    264,\n",
              "    1269,\n",
              "    4009,\n",
              "    11311,\n",
              "    293,\n",
              "    321,\n",
              "    11,\n",
              "    50692],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15010557271013356,\n",
              "   'compression_ratio': 1.6637931034482758,\n",
              "   'no_speech_prob': 0.02845165878534317},\n",
              "  {'id': 59,\n",
              "   'seek': 35120,\n",
              "   'start': 357.76,\n",
              "   'end': 363.68,\n",
              "   'text': ' and also from the DPI ecosystem. So we believe that for this to work, we need the ecosystem',\n",
              "   'tokens': [50692,\n",
              "    293,\n",
              "    611,\n",
              "    490,\n",
              "    264,\n",
              "    413,\n",
              "    31701,\n",
              "    11311,\n",
              "    13,\n",
              "    407,\n",
              "    321,\n",
              "    1697,\n",
              "    300,\n",
              "    337,\n",
              "    341,\n",
              "    281,\n",
              "    589,\n",
              "    11,\n",
              "    321,\n",
              "    643,\n",
              "    264,\n",
              "    11311,\n",
              "    50988],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15010557271013356,\n",
              "   'compression_ratio': 1.6637931034482758,\n",
              "   'no_speech_prob': 0.02845165878534317},\n",
              "  {'id': 60,\n",
              "   'seek': 35120,\n",
              "   'start': 364.4,\n",
              "   'end': 369.84,\n",
              "   'text': ' to be successful. And as a result of that, one of the first things we did was, hey, there are these',\n",
              "   'tokens': [51024,\n",
              "    281,\n",
              "    312,\n",
              "    4406,\n",
              "    13,\n",
              "    400,\n",
              "    382,\n",
              "    257,\n",
              "    1874,\n",
              "    295,\n",
              "    300,\n",
              "    11,\n",
              "    472,\n",
              "    295,\n",
              "    264,\n",
              "    700,\n",
              "    721,\n",
              "    321,\n",
              "    630,\n",
              "    390,\n",
              "    11,\n",
              "    4177,\n",
              "    11,\n",
              "    456,\n",
              "    366,\n",
              "    613,\n",
              "    51296],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15010557271013356,\n",
              "   'compression_ratio': 1.6637931034482758,\n",
              "   'no_speech_prob': 0.02845165878534317},\n",
              "  {'id': 61,\n",
              "   'seek': 35120,\n",
              "   'start': 369.84,\n",
              "   'end': 375.2,\n",
              "   'text': ' open source, large language models that exist, right? I mean, everybody knows about the Lama family',\n",
              "   'tokens': [51296,\n",
              "    1269,\n",
              "    4009,\n",
              "    11,\n",
              "    2416,\n",
              "    2856,\n",
              "    5245,\n",
              "    300,\n",
              "    2514,\n",
              "    11,\n",
              "    558,\n",
              "    30,\n",
              "    286,\n",
              "    914,\n",
              "    11,\n",
              "    2201,\n",
              "    3255,\n",
              "    466,\n",
              "    264,\n",
              "    441,\n",
              "    2404,\n",
              "    1605,\n",
              "    51564],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15010557271013356,\n",
              "   'compression_ratio': 1.6637931034482758,\n",
              "   'no_speech_prob': 0.02845165878534317},\n",
              "  {'id': 62,\n",
              "   'seek': 37520,\n",
              "   'start': 375.28,\n",
              "   'end': 382.64,\n",
              "   'text': ' from Meta. There are others like Mistral. There are a bunch of open source, you know, large language',\n",
              "   'tokens': [50368,\n",
              "    490,\n",
              "    6377,\n",
              "    64,\n",
              "    13,\n",
              "    821,\n",
              "    366,\n",
              "    2357,\n",
              "    411,\n",
              "    20166,\n",
              "    2155,\n",
              "    13,\n",
              "    821,\n",
              "    366,\n",
              "    257,\n",
              "    3840,\n",
              "    295,\n",
              "    1269,\n",
              "    4009,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    2416,\n",
              "    2856,\n",
              "    50736],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16295482562138483,\n",
              "   'compression_ratio': 1.790909090909091,\n",
              "   'no_speech_prob': 0.03437851741909981},\n",
              "  {'id': 63,\n",
              "   'seek': 37520,\n",
              "   'start': 382.64,\n",
              "   'end': 389.59999999999997,\n",
              "   'text': ' models. And then we said, is there any way that take an existing open source model and teach',\n",
              "   'tokens': [50736,\n",
              "    5245,\n",
              "    13,\n",
              "    400,\n",
              "    550,\n",
              "    321,\n",
              "    848,\n",
              "    11,\n",
              "    307,\n",
              "    456,\n",
              "    604,\n",
              "    636,\n",
              "    300,\n",
              "    747,\n",
              "    364,\n",
              "    6741,\n",
              "    1269,\n",
              "    4009,\n",
              "    2316,\n",
              "    293,\n",
              "    2924,\n",
              "    51084],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16295482562138483,\n",
              "   'compression_ratio': 1.790909090909091,\n",
              "   'no_speech_prob': 0.03437851741909981},\n",
              "  {'id': 64,\n",
              "   'seek': 37520,\n",
              "   'start': 389.59999999999997,\n",
              "   'end': 394.15999999999997,\n",
              "   'text': ' it language skills, right? I mean, you know, language. And that is really the, you know, what we',\n",
              "   'tokens': [51084,\n",
              "    309,\n",
              "    2856,\n",
              "    3942,\n",
              "    11,\n",
              "    558,\n",
              "    30,\n",
              "    286,\n",
              "    914,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    2856,\n",
              "    13,\n",
              "    400,\n",
              "    300,\n",
              "    307,\n",
              "    534,\n",
              "    264,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    437,\n",
              "    321,\n",
              "    51312],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16295482562138483,\n",
              "   'compression_ratio': 1.790909090909091,\n",
              "   'no_speech_prob': 0.03437851741909981},\n",
              "  {'id': 65,\n",
              "   'seek': 37520,\n",
              "   'start': 394.15999999999997,\n",
              "   'end': 400.15999999999997,\n",
              "   'text': ' decide, what we said that can we do something like that? And is this a, you know, relatively frugal way',\n",
              "   'tokens': [51312,\n",
              "    4536,\n",
              "    11,\n",
              "    437,\n",
              "    321,\n",
              "    848,\n",
              "    300,\n",
              "    393,\n",
              "    321,\n",
              "    360,\n",
              "    746,\n",
              "    411,\n",
              "    300,\n",
              "    30,\n",
              "    400,\n",
              "    307,\n",
              "    341,\n",
              "    257,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    7226,\n",
              "    431,\n",
              "    17812,\n",
              "    636,\n",
              "    51612],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16295482562138483,\n",
              "   'compression_ratio': 1.790909090909091,\n",
              "   'no_speech_prob': 0.03437851741909981},\n",
              "  {'id': 66,\n",
              "   'seek': 40016,\n",
              "   'start': 400.16,\n",
              "   'end': 408.64000000000004,\n",
              "   'text': ' of actually, you know, making models, you know, work in diverse languages? Because the truth is',\n",
              "   'tokens': [50364,\n",
              "    295,\n",
              "    767,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    1455,\n",
              "    5245,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    589,\n",
              "    294,\n",
              "    9521,\n",
              "    8650,\n",
              "    30,\n",
              "    1436,\n",
              "    264,\n",
              "    3494,\n",
              "    307,\n",
              "    50788],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11633914582272793,\n",
              "   'compression_ratio': 1.7466666666666666,\n",
              "   'no_speech_prob': 0.02772567793726921},\n",
              "  {'id': 67,\n",
              "   'seek': 40016,\n",
              "   'start': 408.64000000000004,\n",
              "   'end': 414.40000000000003,\n",
              "   'text': ' still today, I mean, if you look at the amount of data and knowledge, it is still English dominates',\n",
              "   'tokens': [50788,\n",
              "    920,\n",
              "    965,\n",
              "    11,\n",
              "    286,\n",
              "    914,\n",
              "    11,\n",
              "    498,\n",
              "    291,\n",
              "    574,\n",
              "    412,\n",
              "    264,\n",
              "    2372,\n",
              "    295,\n",
              "    1412,\n",
              "    293,\n",
              "    3601,\n",
              "    11,\n",
              "    309,\n",
              "    307,\n",
              "    920,\n",
              "    3669,\n",
              "    8859,\n",
              "    1024,\n",
              "    51076],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11633914582272793,\n",
              "   'compression_ratio': 1.7466666666666666,\n",
              "   'no_speech_prob': 0.02772567793726921},\n",
              "  {'id': 68,\n",
              "   'seek': 40016,\n",
              "   'start': 414.40000000000003,\n",
              "   'end': 419.76000000000005,\n",
              "   'text': ' these things. And I think that how do you actually take and make it understand Indian language,',\n",
              "   'tokens': [51076,\n",
              "    613,\n",
              "    721,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    577,\n",
              "    360,\n",
              "    291,\n",
              "    767,\n",
              "    747,\n",
              "    293,\n",
              "    652,\n",
              "    309,\n",
              "    1223,\n",
              "    6427,\n",
              "    2856,\n",
              "    11,\n",
              "    51344],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11633914582272793,\n",
              "   'compression_ratio': 1.7466666666666666,\n",
              "   'no_speech_prob': 0.02772567793726921},\n",
              "  {'id': 69,\n",
              "   'seek': 40016,\n",
              "   'start': 419.76000000000005,\n",
              "   'end': 425.44000000000005,\n",
              "   'text': ' understand Indian context, and all of those things in actually a, in an efficient way? And therefore,',\n",
              "   'tokens': [51344,\n",
              "    1223,\n",
              "    6427,\n",
              "    4319,\n",
              "    11,\n",
              "    293,\n",
              "    439,\n",
              "    295,\n",
              "    729,\n",
              "    721,\n",
              "    294,\n",
              "    767,\n",
              "    257,\n",
              "    11,\n",
              "    294,\n",
              "    364,\n",
              "    7148,\n",
              "    636,\n",
              "    30,\n",
              "    400,\n",
              "    4412,\n",
              "    11,\n",
              "    51628],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11633914582272793,\n",
              "   'compression_ratio': 1.7466666666666666,\n",
              "   'no_speech_prob': 0.02772567793726921},\n",
              "  {'id': 70,\n",
              "   'seek': 42544,\n",
              "   'start': 425.44,\n",
              "   'end': 431.84,\n",
              "   'text': \" this was an attempt through that. And it's an open hearty is, you know, is currently based on\",\n",
              "   'tokens': [50364,\n",
              "    341,\n",
              "    390,\n",
              "    364,\n",
              "    5217,\n",
              "    807,\n",
              "    300,\n",
              "    13,\n",
              "    400,\n",
              "    309,\n",
              "    311,\n",
              "    364,\n",
              "    1269,\n",
              "    1917,\n",
              "    88,\n",
              "    307,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    307,\n",
              "    4362,\n",
              "    2361,\n",
              "    322,\n",
              "    50684],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14025757952434262,\n",
              "   'compression_ratio': 1.7854545454545454,\n",
              "   'no_speech_prob': 0.01053453329950571},\n",
              "  {'id': 71,\n",
              "   'seek': 42544,\n",
              "   'start': 431.84,\n",
              "   'end': 436.96,\n",
              "   'text': \" the Lama 7 billion model, but we'll be releasing many more models in different languages, different\",\n",
              "   'tokens': [50684,\n",
              "    264,\n",
              "    441,\n",
              "    2404,\n",
              "    1614,\n",
              "    5218,\n",
              "    2316,\n",
              "    11,\n",
              "    457,\n",
              "    321,\n",
              "    603,\n",
              "    312,\n",
              "    16327,\n",
              "    867,\n",
              "    544,\n",
              "    5245,\n",
              "    294,\n",
              "    819,\n",
              "    8650,\n",
              "    11,\n",
              "    819,\n",
              "    50940],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14025757952434262,\n",
              "   'compression_ratio': 1.7854545454545454,\n",
              "   'no_speech_prob': 0.01053453329950571},\n",
              "  {'id': 72,\n",
              "   'seek': 42544,\n",
              "   'start': 436.96,\n",
              "   'end': 443.36,\n",
              "   'text': ' sizes, and things like that as part of this, as part of this series. And of course, you know,',\n",
              "   'tokens': [50940,\n",
              "    11602,\n",
              "    11,\n",
              "    293,\n",
              "    721,\n",
              "    411,\n",
              "    300,\n",
              "    382,\n",
              "    644,\n",
              "    295,\n",
              "    341,\n",
              "    11,\n",
              "    382,\n",
              "    644,\n",
              "    295,\n",
              "    341,\n",
              "    2638,\n",
              "    13,\n",
              "    400,\n",
              "    295,\n",
              "    1164,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    51260],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14025757952434262,\n",
              "   'compression_ratio': 1.7854545454545454,\n",
              "   'no_speech_prob': 0.01053453329950571},\n",
              "  {'id': 73,\n",
              "   'seek': 42544,\n",
              "   'start': 443.36,\n",
              "   'end': 448.72,\n",
              "   'text': \" we will be building further models on those and doing other things to actually, and we'll also have\",\n",
              "   'tokens': [51260,\n",
              "    321,\n",
              "    486,\n",
              "    312,\n",
              "    2390,\n",
              "    3052,\n",
              "    5245,\n",
              "    322,\n",
              "    729,\n",
              "    293,\n",
              "    884,\n",
              "    661,\n",
              "    721,\n",
              "    281,\n",
              "    767,\n",
              "    11,\n",
              "    293,\n",
              "    321,\n",
              "    603,\n",
              "    611,\n",
              "    362,\n",
              "    51528],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14025757952434262,\n",
              "   'compression_ratio': 1.7854545454545454,\n",
              "   'no_speech_prob': 0.01053453329950571},\n",
              "  {'id': 74,\n",
              "   'seek': 42544,\n",
              "   'start': 448.72,\n",
              "   'end': 453.04,\n",
              "   'text': \" endpoints that people can use. So therefore, it's not, it's definitely, you know, something that people\",\n",
              "   'tokens': [51528,\n",
              "    917,\n",
              "    20552,\n",
              "    300,\n",
              "    561,\n",
              "    393,\n",
              "    764,\n",
              "    13,\n",
              "    407,\n",
              "    4412,\n",
              "    11,\n",
              "    309,\n",
              "    311,\n",
              "    406,\n",
              "    11,\n",
              "    309,\n",
              "    311,\n",
              "    2138,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    746,\n",
              "    300,\n",
              "    561,\n",
              "    51744],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14025757952434262,\n",
              "   'compression_ratio': 1.7854545454545454,\n",
              "   'no_speech_prob': 0.01053453329950571},\n",
              "  {'id': 75,\n",
              "   'seek': 45304,\n",
              "   'start': 453.04,\n",
              "   'end': 460.56,\n",
              "   'text': \" can can can use two things. And that's the essence of what this open hearty is. So what does it\",\n",
              "   'tokens': [50364,\n",
              "    393,\n",
              "    393,\n",
              "    393,\n",
              "    764,\n",
              "    732,\n",
              "    721,\n",
              "    13,\n",
              "    400,\n",
              "    300,\n",
              "    311,\n",
              "    264,\n",
              "    12801,\n",
              "    295,\n",
              "    437,\n",
              "    341,\n",
              "    1269,\n",
              "    1917,\n",
              "    88,\n",
              "    307,\n",
              "    13,\n",
              "    407,\n",
              "    437,\n",
              "    775,\n",
              "    309,\n",
              "    50740],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.24163246154785156,\n",
              "   'compression_ratio': 1.6271186440677967,\n",
              "   'no_speech_prob': 0.006749432999640703},\n",
              "  {'id': 76,\n",
              "   'seek': 45304,\n",
              "   'start': 460.56,\n",
              "   'end': 464.8,\n",
              "   'text': ' mean to people in the audience here who are either doing their own startups or a business or',\n",
              "   'tokens': [50740,\n",
              "    914,\n",
              "    281,\n",
              "    561,\n",
              "    294,\n",
              "    264,\n",
              "    4034,\n",
              "    510,\n",
              "    567,\n",
              "    366,\n",
              "    2139,\n",
              "    884,\n",
              "    641,\n",
              "    1065,\n",
              "    28041,\n",
              "    420,\n",
              "    257,\n",
              "    1606,\n",
              "    420,\n",
              "    50952],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.24163246154785156,\n",
              "   'compression_ratio': 1.6271186440677967,\n",
              "   'no_speech_prob': 0.006749432999640703},\n",
              "  {'id': 77,\n",
              "   'seek': 45304,\n",
              "   'start': 466.08000000000004,\n",
              "   'end': 475.6,\n",
              "   'text': ' developers? How should they look at OpenAI? Sorry, not open. No, no, I think the way you look at it is',\n",
              "   'tokens': [51016,\n",
              "    8849,\n",
              "    30,\n",
              "    1012,\n",
              "    820,\n",
              "    436,\n",
              "    574,\n",
              "    412,\n",
              "    7238,\n",
              "    48698,\n",
              "    30,\n",
              "    4919,\n",
              "    11,\n",
              "    406,\n",
              "    1269,\n",
              "    13,\n",
              "    883,\n",
              "    11,\n",
              "    572,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    264,\n",
              "    636,\n",
              "    291,\n",
              "    574,\n",
              "    412,\n",
              "    309,\n",
              "    307,\n",
              "    51492],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.24163246154785156,\n",
              "   'compression_ratio': 1.6271186440677967,\n",
              "   'no_speech_prob': 0.006749432999640703},\n",
              "  {'id': 78,\n",
              "   'seek': 45304,\n",
              "   'start': 475.6,\n",
              "   'end': 480.48,\n",
              "   'text': \" that we are one of the important things that we are doing is we're not just building models.\",\n",
              "   'tokens': [51492,\n",
              "    300,\n",
              "    321,\n",
              "    366,\n",
              "    472,\n",
              "    295,\n",
              "    264,\n",
              "    1021,\n",
              "    721,\n",
              "    300,\n",
              "    321,\n",
              "    366,\n",
              "    884,\n",
              "    307,\n",
              "    321,\n",
              "    434,\n",
              "    406,\n",
              "    445,\n",
              "    2390,\n",
              "    5245,\n",
              "    13,\n",
              "    51736],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.24163246154785156,\n",
              "   'compression_ratio': 1.6271186440677967,\n",
              "   'no_speech_prob': 0.006749432999640703},\n",
              "  {'id': 79,\n",
              "   'seek': 48048,\n",
              "   'start': 481.12,\n",
              "   'end': 487.36,\n",
              "   'text': ' We are also going to be building a platform, a platform for developers where you can actually use',\n",
              "   'tokens': [50396,\n",
              "    492,\n",
              "    366,\n",
              "    611,\n",
              "    516,\n",
              "    281,\n",
              "    312,\n",
              "    2390,\n",
              "    257,\n",
              "    3663,\n",
              "    11,\n",
              "    257,\n",
              "    3663,\n",
              "    337,\n",
              "    8849,\n",
              "    689,\n",
              "    291,\n",
              "    393,\n",
              "    767,\n",
              "    764,\n",
              "    50708],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14022589183989026,\n",
              "   'compression_ratio': 1.7435897435897436,\n",
              "   'no_speech_prob': 0.04580613225698471},\n",
              "  {'id': 80,\n",
              "   'seek': 48048,\n",
              "   'start': 488.24,\n",
              "   'end': 492.8,\n",
              "   'text': ' a combination of various different kinds of models, some which are from us, some which are open',\n",
              "   'tokens': [50752,\n",
              "    257,\n",
              "    6562,\n",
              "    295,\n",
              "    3683,\n",
              "    819,\n",
              "    3685,\n",
              "    295,\n",
              "    5245,\n",
              "    11,\n",
              "    512,\n",
              "    597,\n",
              "    366,\n",
              "    490,\n",
              "    505,\n",
              "    11,\n",
              "    512,\n",
              "    597,\n",
              "    366,\n",
              "    1269,\n",
              "    50980],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14022589183989026,\n",
              "   'compression_ratio': 1.7435897435897436,\n",
              "   'no_speech_prob': 0.04580613225698471},\n",
              "  {'id': 81,\n",
              "   'seek': 48048,\n",
              "   'start': 492.8,\n",
              "   'end': 497.28000000000003,\n",
              "   'text': ' source, some which may not be open source, and actually to actually pull together and figure out',\n",
              "   'tokens': [50980,\n",
              "    4009,\n",
              "    11,\n",
              "    512,\n",
              "    597,\n",
              "    815,\n",
              "    406,\n",
              "    312,\n",
              "    1269,\n",
              "    4009,\n",
              "    11,\n",
              "    293,\n",
              "    767,\n",
              "    281,\n",
              "    767,\n",
              "    2235,\n",
              "    1214,\n",
              "    293,\n",
              "    2573,\n",
              "    484,\n",
              "    51204],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14022589183989026,\n",
              "   'compression_ratio': 1.7435897435897436,\n",
              "   'no_speech_prob': 0.04580613225698471},\n",
              "  {'id': 82,\n",
              "   'seek': 48048,\n",
              "   'start': 497.28000000000003,\n",
              "   'end': 504.64000000000004,\n",
              "   'text': ' how to deploy, you know, generative AI applications at scale and understand and evaluate',\n",
              "   'tokens': [51204,\n",
              "    577,\n",
              "    281,\n",
              "    7274,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    1337,\n",
              "    1166,\n",
              "    7318,\n",
              "    5821,\n",
              "    412,\n",
              "    4373,\n",
              "    293,\n",
              "    1223,\n",
              "    293,\n",
              "    13059,\n",
              "    51572],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14022589183989026,\n",
              "   'compression_ratio': 1.7435897435897436,\n",
              "   'no_speech_prob': 0.04580613225698471},\n",
              "  {'id': 83,\n",
              "   'seek': 48048,\n",
              "   'start': 504.64000000000004,\n",
              "   'end': 509.04,\n",
              "   'text': \" their performance in an efficient manner. And that's something that we're planning to do at this\",\n",
              "   'tokens': [51572,\n",
              "    641,\n",
              "    3389,\n",
              "    294,\n",
              "    364,\n",
              "    7148,\n",
              "    9060,\n",
              "    13,\n",
              "    400,\n",
              "    300,\n",
              "    311,\n",
              "    746,\n",
              "    300,\n",
              "    321,\n",
              "    434,\n",
              "    5038,\n",
              "    281,\n",
              "    360,\n",
              "    412,\n",
              "    341,\n",
              "    51792],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14022589183989026,\n",
              "   'compression_ratio': 1.7435897435897436,\n",
              "   'no_speech_prob': 0.04580613225698471},\n",
              "  {'id': 84,\n",
              "   'seek': 50904,\n",
              "   'start': 509.04,\n",
              "   'end': 513.84,\n",
              "   'text': \" and this platform is, you know, in the next couple of months, we'll be coming out there,\",\n",
              "   'tokens': [50364,\n",
              "    293,\n",
              "    341,\n",
              "    3663,\n",
              "    307,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    294,\n",
              "    264,\n",
              "    958,\n",
              "    1916,\n",
              "    295,\n",
              "    2493,\n",
              "    11,\n",
              "    321,\n",
              "    603,\n",
              "    312,\n",
              "    1348,\n",
              "    484,\n",
              "    456,\n",
              "    11,\n",
              "    50604],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1519390004021781,\n",
              "   'compression_ratio': 1.65625,\n",
              "   'no_speech_prob': 0.003252557246014476},\n",
              "  {'id': 85,\n",
              "   'seek': 50904,\n",
              "   'start': 513.84,\n",
              "   'end': 518.16,\n",
              "   'text': ' it will be available to developers. But of course, those who want to start with the open source',\n",
              "   'tokens': [50604,\n",
              "    309,\n",
              "    486,\n",
              "    312,\n",
              "    2435,\n",
              "    281,\n",
              "    8849,\n",
              "    13,\n",
              "    583,\n",
              "    295,\n",
              "    1164,\n",
              "    11,\n",
              "    729,\n",
              "    567,\n",
              "    528,\n",
              "    281,\n",
              "    722,\n",
              "    365,\n",
              "    264,\n",
              "    1269,\n",
              "    4009,\n",
              "    50820],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1519390004021781,\n",
              "   'compression_ratio': 1.65625,\n",
              "   'no_speech_prob': 0.003252557246014476},\n",
              "  {'id': 86,\n",
              "   'seek': 50904,\n",
              "   'start': 518.72,\n",
              "   'end': 521.6,\n",
              "   'text': ' things and hack with that, of course, please go ahead and do that as well.',\n",
              "   'tokens': [50848,\n",
              "    721,\n",
              "    293,\n",
              "    10339,\n",
              "    365,\n",
              "    300,\n",
              "    11,\n",
              "    295,\n",
              "    1164,\n",
              "    11,\n",
              "    1767,\n",
              "    352,\n",
              "    2286,\n",
              "    293,\n",
              "    360,\n",
              "    300,\n",
              "    382,\n",
              "    731,\n",
              "    13,\n",
              "    50992],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1519390004021781,\n",
              "   'compression_ratio': 1.65625,\n",
              "   'no_speech_prob': 0.003252557246014476},\n",
              "  {'id': 87,\n",
              "   'seek': 50904,\n",
              "   'start': 521.6,\n",
              "   'end': 528.24,\n",
              "   'text': \" That's phenomenal. But how does it compare to OpenAI itself or Google?\",\n",
              "   'tokens': [50992,\n",
              "    663,\n",
              "    311,\n",
              "    17778,\n",
              "    13,\n",
              "    583,\n",
              "    577,\n",
              "    775,\n",
              "    309,\n",
              "    6794,\n",
              "    281,\n",
              "    7238,\n",
              "    48698,\n",
              "    2564,\n",
              "    420,\n",
              "    3329,\n",
              "    30,\n",
              "    51324],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1519390004021781,\n",
              "   'compression_ratio': 1.65625,\n",
              "   'no_speech_prob': 0.003252557246014476},\n",
              "  {'id': 88,\n",
              "   'seek': 50904,\n",
              "   'start': 530.16,\n",
              "   'end': 534.72,\n",
              "   'text': ' See, at least the things that we are doing now, right? I mean, one of the things that when we',\n",
              "   'tokens': [51420,\n",
              "    3008,\n",
              "    11,\n",
              "    412,\n",
              "    1935,\n",
              "    264,\n",
              "    721,\n",
              "    300,\n",
              "    321,\n",
              "    366,\n",
              "    884,\n",
              "    586,\n",
              "    11,\n",
              "    558,\n",
              "    30,\n",
              "    286,\n",
              "    914,\n",
              "    11,\n",
              "    472,\n",
              "    295,\n",
              "    264,\n",
              "    721,\n",
              "    300,\n",
              "    562,\n",
              "    321,\n",
              "    51648],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1519390004021781,\n",
              "   'compression_ratio': 1.65625,\n",
              "   'no_speech_prob': 0.003252557246014476},\n",
              "  {'id': 89,\n",
              "   'seek': 53472,\n",
              "   'start': 534.72,\n",
              "   'end': 541.84,\n",
              "   'text': ' thought about building server, we said we want to build a full stack generative AI company and',\n",
              "   'tokens': [50364,\n",
              "    1194,\n",
              "    466,\n",
              "    2390,\n",
              "    7154,\n",
              "    11,\n",
              "    321,\n",
              "    848,\n",
              "    321,\n",
              "    528,\n",
              "    281,\n",
              "    1322,\n",
              "    257,\n",
              "    1577,\n",
              "    8630,\n",
              "    1337,\n",
              "    1166,\n",
              "    7318,\n",
              "    2237,\n",
              "    293,\n",
              "    50720],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13564390546820138,\n",
              "   'compression_ratio': 1.6946902654867257,\n",
              "   'no_speech_prob': 0.011704850010573864},\n",
              "  {'id': 90,\n",
              "   'seek': 53472,\n",
              "   'start': 541.84,\n",
              "   'end': 546.5600000000001,\n",
              "   'text': ' different people have, and our understanding of full stack is that we need to know how to train',\n",
              "   'tokens': [50720,\n",
              "    819,\n",
              "    561,\n",
              "    362,\n",
              "    11,\n",
              "    293,\n",
              "    527,\n",
              "    3701,\n",
              "    295,\n",
              "    1577,\n",
              "    8630,\n",
              "    307,\n",
              "    300,\n",
              "    321,\n",
              "    643,\n",
              "    281,\n",
              "    458,\n",
              "    577,\n",
              "    281,\n",
              "    3847,\n",
              "    50956],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13564390546820138,\n",
              "   'compression_ratio': 1.6946902654867257,\n",
              "   'no_speech_prob': 0.011704850010573864},\n",
              "  {'id': 91,\n",
              "   'seek': 53472,\n",
              "   'start': 546.5600000000001,\n",
              "   'end': 552.8000000000001,\n",
              "   'text': ' models from scratch. We need to know how to kind of figure out how to deploy models to solve',\n",
              "   'tokens': [50956,\n",
              "    5245,\n",
              "    490,\n",
              "    8459,\n",
              "    13,\n",
              "    492,\n",
              "    643,\n",
              "    281,\n",
              "    458,\n",
              "    577,\n",
              "    281,\n",
              "    733,\n",
              "    295,\n",
              "    2573,\n",
              "    484,\n",
              "    577,\n",
              "    281,\n",
              "    7274,\n",
              "    5245,\n",
              "    281,\n",
              "    5039,\n",
              "    51268],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13564390546820138,\n",
              "   'compression_ratio': 1.6946902654867257,\n",
              "   'no_speech_prob': 0.011704850010573864},\n",
              "  {'id': 92,\n",
              "   'seek': 53472,\n",
              "   'start': 552.8000000000001,\n",
              "   'end': 558.96,\n",
              "   'text': ' real world use cases. And we need to play in the ecosystem to make sure that we can actually deploy',\n",
              "   'tokens': [51268,\n",
              "    957,\n",
              "    1002,\n",
              "    764,\n",
              "    3331,\n",
              "    13,\n",
              "    400,\n",
              "    321,\n",
              "    643,\n",
              "    281,\n",
              "    862,\n",
              "    294,\n",
              "    264,\n",
              "    11311,\n",
              "    281,\n",
              "    652,\n",
              "    988,\n",
              "    300,\n",
              "    321,\n",
              "    393,\n",
              "    767,\n",
              "    7274,\n",
              "    51576],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13564390546820138,\n",
              "   'compression_ratio': 1.6946902654867257,\n",
              "   'no_speech_prob': 0.011704850010573864},\n",
              "  {'id': 93,\n",
              "   'seek': 55896,\n",
              "   'start': 559.52,\n",
              "   'end': 566.32,\n",
              "   'text': ' population scale applications. So we were thinking about all of these things. But still the models we',\n",
              "   'tokens': [50392,\n",
              "    4415,\n",
              "    4373,\n",
              "    5821,\n",
              "    13,\n",
              "    407,\n",
              "    321,\n",
              "    645,\n",
              "    1953,\n",
              "    466,\n",
              "    439,\n",
              "    295,\n",
              "    613,\n",
              "    721,\n",
              "    13,\n",
              "    583,\n",
              "    920,\n",
              "    264,\n",
              "    5245,\n",
              "    321,\n",
              "    50732],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17544406110590155,\n",
              "   'compression_ratio': 1.7092511013215859,\n",
              "   'no_speech_prob': 0.016397610306739807},\n",
              "  {'id': 94,\n",
              "   'seek': 55896,\n",
              "   'start': 566.32,\n",
              "   'end': 572.24,\n",
              "   'text': ' were talking about are fairly small models. They are fairly small models, right? The 7 to maybe',\n",
              "   'tokens': [50732,\n",
              "    645,\n",
              "    1417,\n",
              "    466,\n",
              "    366,\n",
              "    6457,\n",
              "    1359,\n",
              "    5245,\n",
              "    13,\n",
              "    814,\n",
              "    366,\n",
              "    6457,\n",
              "    1359,\n",
              "    5245,\n",
              "    11,\n",
              "    558,\n",
              "    30,\n",
              "    440,\n",
              "    1614,\n",
              "    281,\n",
              "    1310,\n",
              "    51028],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17544406110590155,\n",
              "   'compression_ratio': 1.7092511013215859,\n",
              "   'no_speech_prob': 0.016397610306739807},\n",
              "  {'id': 95,\n",
              "   'seek': 55896,\n",
              "   'start': 572.24,\n",
              "   'end': 578.0,\n",
              "   'text': \" up to 70 billion kind of range we're talking about. While these models like OpenAI and Google\",\n",
              "   'tokens': [51028,\n",
              "    493,\n",
              "    281,\n",
              "    5285,\n",
              "    5218,\n",
              "    733,\n",
              "    295,\n",
              "    3613,\n",
              "    321,\n",
              "    434,\n",
              "    1417,\n",
              "    466,\n",
              "    13,\n",
              "    3987,\n",
              "    613,\n",
              "    5245,\n",
              "    411,\n",
              "    7238,\n",
              "    48698,\n",
              "    293,\n",
              "    3329,\n",
              "    51316],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17544406110590155,\n",
              "   'compression_ratio': 1.7092511013215859,\n",
              "   'no_speech_prob': 0.016397610306739807},\n",
              "  {'id': 96,\n",
              "   'seek': 55896,\n",
              "   'start': 578.0,\n",
              "   'end': 584.32,\n",
              "   'text': ' are obviously much bigger models, right? But we want to understand the techniques and be able to',\n",
              "   'tokens': [51316,\n",
              "    366,\n",
              "    2745,\n",
              "    709,\n",
              "    3801,\n",
              "    5245,\n",
              "    11,\n",
              "    558,\n",
              "    30,\n",
              "    583,\n",
              "    321,\n",
              "    528,\n",
              "    281,\n",
              "    1223,\n",
              "    264,\n",
              "    7512,\n",
              "    293,\n",
              "    312,\n",
              "    1075,\n",
              "    281,\n",
              "    51632],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17544406110590155,\n",
              "   'compression_ratio': 1.7092511013215859,\n",
              "   'no_speech_prob': 0.016397610306739807},\n",
              "  {'id': 97,\n",
              "   'seek': 58432,\n",
              "   'start': 584.32,\n",
              "   'end': 590.8000000000001,\n",
              "   'text': ' build that muscle to do all of these things to make it available to people. Now those models are',\n",
              "   'tokens': [50364,\n",
              "    1322,\n",
              "    300,\n",
              "    8679,\n",
              "    281,\n",
              "    360,\n",
              "    439,\n",
              "    295,\n",
              "    613,\n",
              "    721,\n",
              "    281,\n",
              "    652,\n",
              "    309,\n",
              "    2435,\n",
              "    281,\n",
              "    561,\n",
              "    13,\n",
              "    823,\n",
              "    729,\n",
              "    5245,\n",
              "    366,\n",
              "    50688],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19314167896906534,\n",
              "   'compression_ratio': 1.6523605150214593,\n",
              "   'no_speech_prob': 0.015202946960926056},\n",
              "  {'id': 98,\n",
              "   'seek': 58432,\n",
              "   'start': 591.6,\n",
              "   'end': 597.0400000000001,\n",
              "   'text': ' I mean, as I said, you know, I think that there is space for all of those things. And I think as',\n",
              "   'tokens': [50728,\n",
              "    286,\n",
              "    914,\n",
              "    11,\n",
              "    382,\n",
              "    286,\n",
              "    848,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    456,\n",
              "    307,\n",
              "    1901,\n",
              "    337,\n",
              "    439,\n",
              "    295,\n",
              "    729,\n",
              "    721,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    382,\n",
              "    51000],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19314167896906534,\n",
              "   'compression_ratio': 1.6523605150214593,\n",
              "   'no_speech_prob': 0.015202946960926056},\n",
              "  {'id': 99,\n",
              "   'seek': 58432,\n",
              "   'start': 597.0400000000001,\n",
              "   'end': 603.9200000000001,\n",
              "   'text': ' even Sridhar was talking about earlier in the day, we believe that these smaller models can do',\n",
              "   'tokens': [51000,\n",
              "    754,\n",
              "    318,\n",
              "    8558,\n",
              "    5854,\n",
              "    390,\n",
              "    1417,\n",
              "    466,\n",
              "    3071,\n",
              "    294,\n",
              "    264,\n",
              "    786,\n",
              "    11,\n",
              "    321,\n",
              "    1697,\n",
              "    300,\n",
              "    613,\n",
              "    4356,\n",
              "    5245,\n",
              "    393,\n",
              "    360,\n",
              "    51344],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19314167896906534,\n",
              "   'compression_ratio': 1.6523605150214593,\n",
              "   'no_speech_prob': 0.015202946960926056},\n",
              "  {'id': 100,\n",
              "   'seek': 58432,\n",
              "   'start': 604.72,\n",
              "   'end': 611.2,\n",
              "   'text': ' very, I mean, many, many kind of domain specific tasks extremely well, probably even better than',\n",
              "   'tokens': [51384,\n",
              "    588,\n",
              "    11,\n",
              "    286,\n",
              "    914,\n",
              "    11,\n",
              "    867,\n",
              "    11,\n",
              "    867,\n",
              "    733,\n",
              "    295,\n",
              "    9274,\n",
              "    2685,\n",
              "    9608,\n",
              "    4664,\n",
              "    731,\n",
              "    11,\n",
              "    1391,\n",
              "    754,\n",
              "    1101,\n",
              "    813,\n",
              "    51708],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19314167896906534,\n",
              "   'compression_ratio': 1.6523605150214593,\n",
              "   'no_speech_prob': 0.015202946960926056},\n",
              "  {'id': 101,\n",
              "   'seek': 61120,\n",
              "   'start': 611.6,\n",
              "   'end': 616.96,\n",
              "   'text': ' the larger models. And that is really one of the key areas. And so the further value of these kinds',\n",
              "   'tokens': [50384,\n",
              "    264,\n",
              "    4833,\n",
              "    5245,\n",
              "    13,\n",
              "    400,\n",
              "    300,\n",
              "    307,\n",
              "    534,\n",
              "    472,\n",
              "    295,\n",
              "    264,\n",
              "    2141,\n",
              "    3179,\n",
              "    13,\n",
              "    400,\n",
              "    370,\n",
              "    264,\n",
              "    3052,\n",
              "    2158,\n",
              "    295,\n",
              "    613,\n",
              "    3685,\n",
              "    50652],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14460828748799986,\n",
              "   'compression_ratio': 1.7321428571428572,\n",
              "   'no_speech_prob': 0.010775940492749214},\n",
              "  {'id': 102,\n",
              "   'seek': 61120,\n",
              "   'start': 616.96,\n",
              "   'end': 623.6,\n",
              "   'text': \" of things, right? We are not aiming in these models to build any AGI, right? That's not our goal here.\",\n",
              "   'tokens': [50652,\n",
              "    295,\n",
              "    721,\n",
              "    11,\n",
              "    558,\n",
              "    30,\n",
              "    492,\n",
              "    366,\n",
              "    406,\n",
              "    20253,\n",
              "    294,\n",
              "    613,\n",
              "    5245,\n",
              "    281,\n",
              "    1322,\n",
              "    604,\n",
              "    316,\n",
              "    26252,\n",
              "    11,\n",
              "    558,\n",
              "    30,\n",
              "    663,\n",
              "    311,\n",
              "    406,\n",
              "    527,\n",
              "    3387,\n",
              "    510,\n",
              "    13,\n",
              "    50984],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14460828748799986,\n",
              "   'compression_ratio': 1.7321428571428572,\n",
              "   'no_speech_prob': 0.010775940492749214},\n",
              "  {'id': 103,\n",
              "   'seek': 61120,\n",
              "   'start': 623.6,\n",
              "   'end': 629.2,\n",
              "   'text': ' Our goal is to make things that work extremely well for domain specific use cases or',\n",
              "   'tokens': [50984,\n",
              "    2621,\n",
              "    3387,\n",
              "    307,\n",
              "    281,\n",
              "    652,\n",
              "    721,\n",
              "    300,\n",
              "    589,\n",
              "    4664,\n",
              "    731,\n",
              "    337,\n",
              "    9274,\n",
              "    2685,\n",
              "    764,\n",
              "    3331,\n",
              "    420,\n",
              "    51264],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14460828748799986,\n",
              "   'compression_ratio': 1.7321428571428572,\n",
              "   'no_speech_prob': 0.010775940492749214},\n",
              "  {'id': 104,\n",
              "   'seek': 61120,\n",
              "   'start': 629.2,\n",
              "   'end': 633.84,\n",
              "   'text': ' or increase accessibility through language and all of those kinds of things. And obviously all of',\n",
              "   'tokens': [51264,\n",
              "    420,\n",
              "    3488,\n",
              "    15002,\n",
              "    807,\n",
              "    2856,\n",
              "    293,\n",
              "    439,\n",
              "    295,\n",
              "    729,\n",
              "    3685,\n",
              "    295,\n",
              "    721,\n",
              "    13,\n",
              "    400,\n",
              "    2745,\n",
              "    439,\n",
              "    295,\n",
              "    51496],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14460828748799986,\n",
              "   'compression_ratio': 1.7321428571428572,\n",
              "   'no_speech_prob': 0.010775940492749214},\n",
              "  {'id': 105,\n",
              "   'seek': 61120,\n",
              "   'start': 633.84,\n",
              "   'end': 639.44,\n",
              "   'text': ' this unique to India. But what is unique about India? I mean, like, what is, is anything special in',\n",
              "   'tokens': [51496,\n",
              "    341,\n",
              "    3845,\n",
              "    281,\n",
              "    5282,\n",
              "    13,\n",
              "    583,\n",
              "    437,\n",
              "    307,\n",
              "    3845,\n",
              "    466,\n",
              "    5282,\n",
              "    30,\n",
              "    286,\n",
              "    914,\n",
              "    11,\n",
              "    411,\n",
              "    11,\n",
              "    437,\n",
              "    307,\n",
              "    11,\n",
              "    307,\n",
              "    1340,\n",
              "    2121,\n",
              "    294,\n",
              "    51776],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14460828748799986,\n",
              "   'compression_ratio': 1.7321428571428572,\n",
              "   'no_speech_prob': 0.010775940492749214},\n",
              "  {'id': 106,\n",
              "   'seek': 63944,\n",
              "   'start': 639.44,\n",
              "   'end': 646.5600000000001,\n",
              "   'text': ' our ecosystem that makes a small model focused with Indian languages better for more suited for our',\n",
              "   'tokens': [50364,\n",
              "    527,\n",
              "    11311,\n",
              "    300,\n",
              "    1669,\n",
              "    257,\n",
              "    1359,\n",
              "    2316,\n",
              "    5178,\n",
              "    365,\n",
              "    6427,\n",
              "    8650,\n",
              "    1101,\n",
              "    337,\n",
              "    544,\n",
              "    24736,\n",
              "    337,\n",
              "    527,\n",
              "    50720],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.135033115898211,\n",
              "   'compression_ratio': 1.6694915254237288,\n",
              "   'no_speech_prob': 0.002186726313084364},\n",
              "  {'id': 107,\n",
              "   'seek': 63944,\n",
              "   'start': 646.5600000000001,\n",
              "   'end': 653.12,\n",
              "   'text': ' problems? So I think that, I mean, there are quite a few things that are unique about India, right?',\n",
              "   'tokens': [50720,\n",
              "    2740,\n",
              "    30,\n",
              "    407,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    11,\n",
              "    286,\n",
              "    914,\n",
              "    11,\n",
              "    456,\n",
              "    366,\n",
              "    1596,\n",
              "    257,\n",
              "    1326,\n",
              "    721,\n",
              "    300,\n",
              "    366,\n",
              "    3845,\n",
              "    466,\n",
              "    5282,\n",
              "    11,\n",
              "    558,\n",
              "    30,\n",
              "    51048],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.135033115898211,\n",
              "   'compression_ratio': 1.6694915254237288,\n",
              "   'no_speech_prob': 0.002186726313084364},\n",
              "  {'id': 108,\n",
              "   'seek': 63944,\n",
              "   'start': 653.84,\n",
              "   'end': 659.44,\n",
              "   'text': ' The first thing is I think that we are a voice first nation. So therefore, I think voice has to',\n",
              "   'tokens': [51084,\n",
              "    440,\n",
              "    700,\n",
              "    551,\n",
              "    307,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    321,\n",
              "    366,\n",
              "    257,\n",
              "    3177,\n",
              "    700,\n",
              "    4790,\n",
              "    13,\n",
              "    407,\n",
              "    4412,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    3177,\n",
              "    575,\n",
              "    281,\n",
              "    51364],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.135033115898211,\n",
              "   'compression_ratio': 1.6694915254237288,\n",
              "   'no_speech_prob': 0.002186726313084364},\n",
              "  {'id': 109,\n",
              "   'seek': 63944,\n",
              "   'start': 659.44,\n",
              "   'end': 667.2,\n",
              "   'text': \" be the core to doing things. The other thing, of course, India is extremely, it's a cost-conscious\",\n",
              "   'tokens': [51364,\n",
              "    312,\n",
              "    264,\n",
              "    4965,\n",
              "    281,\n",
              "    884,\n",
              "    721,\n",
              "    13,\n",
              "    440,\n",
              "    661,\n",
              "    551,\n",
              "    11,\n",
              "    295,\n",
              "    1164,\n",
              "    11,\n",
              "    5282,\n",
              "    307,\n",
              "    4664,\n",
              "    11,\n",
              "    309,\n",
              "    311,\n",
              "    257,\n",
              "    2063,\n",
              "    12,\n",
              "    19877,\n",
              "    51752],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.135033115898211,\n",
              "   'compression_ratio': 1.6694915254237288,\n",
              "   'no_speech_prob': 0.002186726313084364},\n",
              "  {'id': 110,\n",
              "   'seek': 66720,\n",
              "   'start': 667.2,\n",
              "   'end': 673.44,\n",
              "   'text': ' country from a cost perspective. Now, I would say that there are lots of interesting use cases where',\n",
              "   'tokens': [50364,\n",
              "    1941,\n",
              "    490,\n",
              "    257,\n",
              "    2063,\n",
              "    4585,\n",
              "    13,\n",
              "    823,\n",
              "    11,\n",
              "    286,\n",
              "    576,\n",
              "    584,\n",
              "    300,\n",
              "    456,\n",
              "    366,\n",
              "    3195,\n",
              "    295,\n",
              "    1880,\n",
              "    764,\n",
              "    3331,\n",
              "    689,\n",
              "    50676],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10696743630074165,\n",
              "   'compression_ratio': 1.7097902097902098,\n",
              "   'no_speech_prob': 0.014226777479052544},\n",
              "  {'id': 111,\n",
              "   'seek': 66720,\n",
              "   'start': 673.44,\n",
              "   'end': 678.8000000000001,\n",
              "   'text': \" you can use OpenAI and the cost structure works that when we're depending on your application.\",\n",
              "   'tokens': [50676,\n",
              "    291,\n",
              "    393,\n",
              "    764,\n",
              "    7238,\n",
              "    48698,\n",
              "    293,\n",
              "    264,\n",
              "    2063,\n",
              "    3877,\n",
              "    1985,\n",
              "    300,\n",
              "    562,\n",
              "    321,\n",
              "    434,\n",
              "    5413,\n",
              "    322,\n",
              "    428,\n",
              "    3861,\n",
              "    13,\n",
              "    50944],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10696743630074165,\n",
              "   'compression_ratio': 1.7097902097902098,\n",
              "   'no_speech_prob': 0.014226777479052544},\n",
              "  {'id': 112,\n",
              "   'seek': 66720,\n",
              "   'start': 678.8000000000001,\n",
              "   'end': 684.0,\n",
              "   'text': ' But when you want to scale things to a massive level and make it work, then you have to figure out',\n",
              "   'tokens': [50944,\n",
              "    583,\n",
              "    562,\n",
              "    291,\n",
              "    528,\n",
              "    281,\n",
              "    4373,\n",
              "    721,\n",
              "    281,\n",
              "    257,\n",
              "    5994,\n",
              "    1496,\n",
              "    293,\n",
              "    652,\n",
              "    309,\n",
              "    589,\n",
              "    11,\n",
              "    550,\n",
              "    291,\n",
              "    362,\n",
              "    281,\n",
              "    2573,\n",
              "    484,\n",
              "    51204],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10696743630074165,\n",
              "   'compression_ratio': 1.7097902097902098,\n",
              "   'no_speech_prob': 0.014226777479052544},\n",
              "  {'id': 113,\n",
              "   'seek': 66720,\n",
              "   'start': 684.0,\n",
              "   'end': 689.2800000000001,\n",
              "   'text': \" how small models work. So that's something that is also specific to India. The third thing which\",\n",
              "   'tokens': [51204,\n",
              "    577,\n",
              "    1359,\n",
              "    5245,\n",
              "    589,\n",
              "    13,\n",
              "    407,\n",
              "    300,\n",
              "    311,\n",
              "    746,\n",
              "    300,\n",
              "    307,\n",
              "    611,\n",
              "    2685,\n",
              "    281,\n",
              "    5282,\n",
              "    13,\n",
              "    440,\n",
              "    2636,\n",
              "    551,\n",
              "    597,\n",
              "    51468],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10696743630074165,\n",
              "   'compression_ratio': 1.7097902097902098,\n",
              "   'no_speech_prob': 0.014226777479052544},\n",
              "  {'id': 114,\n",
              "   'seek': 66720,\n",
              "   'start': 689.2800000000001,\n",
              "   'end': 695.0400000000001,\n",
              "   'text': ' is specific to India is really the success that India has had in building all this digital public',\n",
              "   'tokens': [51468,\n",
              "    307,\n",
              "    2685,\n",
              "    281,\n",
              "    5282,\n",
              "    307,\n",
              "    534,\n",
              "    264,\n",
              "    2245,\n",
              "    300,\n",
              "    5282,\n",
              "    575,\n",
              "    632,\n",
              "    294,\n",
              "    2390,\n",
              "    439,\n",
              "    341,\n",
              "    4562,\n",
              "    1908,\n",
              "    51756],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10696743630074165,\n",
              "   'compression_ratio': 1.7097902097902098,\n",
              "   'no_speech_prob': 0.014226777479052544},\n",
              "  {'id': 115,\n",
              "   'seek': 69504,\n",
              "   'start': 695.04,\n",
              "   'end': 700.4,\n",
              "   'text': ' infrastructure. When you add the AI layer on top of it, then you can actually get dramatic,',\n",
              "   'tokens': [50364,\n",
              "    6896,\n",
              "    13,\n",
              "    1133,\n",
              "    291,\n",
              "    909,\n",
              "    264,\n",
              "    7318,\n",
              "    4583,\n",
              "    322,\n",
              "    1192,\n",
              "    295,\n",
              "    309,\n",
              "    11,\n",
              "    550,\n",
              "    291,\n",
              "    393,\n",
              "    767,\n",
              "    483,\n",
              "    12023,\n",
              "    11,\n",
              "    50632],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14748624227579357,\n",
              "   'compression_ratio': 1.5691056910569106,\n",
              "   'no_speech_prob': 0.00990657601505518},\n",
              "  {'id': 116,\n",
              "   'seek': 69504,\n",
              "   'start': 702.16,\n",
              "   'end': 707.76,\n",
              "   'text': ' you know, dramatic, I think, multiplicative combinatorial effects based on doing things like that.',\n",
              "   'tokens': [50720,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    12023,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    11,\n",
              "    17596,\n",
              "    1166,\n",
              "    2512,\n",
              "    31927,\n",
              "    831,\n",
              "    5065,\n",
              "    2361,\n",
              "    322,\n",
              "    884,\n",
              "    721,\n",
              "    411,\n",
              "    300,\n",
              "    13,\n",
              "    51000],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14748624227579357,\n",
              "   'compression_ratio': 1.5691056910569106,\n",
              "   'no_speech_prob': 0.00990657601505518},\n",
              "  {'id': 117,\n",
              "   'seek': 69504,\n",
              "   'start': 707.76,\n",
              "   'end': 712.0799999999999,\n",
              "   'text': \" That's a phenomenal point. Like, you know, it's like DPI to the power of AI almost in some ways.\",\n",
              "   'tokens': [51000,\n",
              "    663,\n",
              "    311,\n",
              "    257,\n",
              "    17778,\n",
              "    935,\n",
              "    13,\n",
              "    1743,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    309,\n",
              "    311,\n",
              "    411,\n",
              "    413,\n",
              "    31701,\n",
              "    281,\n",
              "    264,\n",
              "    1347,\n",
              "    295,\n",
              "    7318,\n",
              "    1920,\n",
              "    294,\n",
              "    512,\n",
              "    2098,\n",
              "    13,\n",
              "    51216],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14748624227579357,\n",
              "   'compression_ratio': 1.5691056910569106,\n",
              "   'no_speech_prob': 0.00990657601505518},\n",
              "  {'id': 118,\n",
              "   'seek': 69504,\n",
              "   'start': 712.0799999999999,\n",
              "   'end': 719.04,\n",
              "   'text': \" And as a part of Adar building Adar, no better person than you. So in summary, what I'm hearing is\",\n",
              "   'tokens': [51216,\n",
              "    400,\n",
              "    382,\n",
              "    257,\n",
              "    644,\n",
              "    295,\n",
              "    1999,\n",
              "    289,\n",
              "    2390,\n",
              "    1999,\n",
              "    289,\n",
              "    11,\n",
              "    572,\n",
              "    1101,\n",
              "    954,\n",
              "    813,\n",
              "    291,\n",
              "    13,\n",
              "    407,\n",
              "    294,\n",
              "    12691,\n",
              "    11,\n",
              "    437,\n",
              "    286,\n",
              "    478,\n",
              "    4763,\n",
              "    307,\n",
              "    51564],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14748624227579357,\n",
              "   'compression_ratio': 1.5691056910569106,\n",
              "   'no_speech_prob': 0.00990657601505518},\n",
              "  {'id': 119,\n",
              "   'seek': 71904,\n",
              "   'start': 719.04,\n",
              "   'end': 725.92,\n",
              "   'text': ' small models specialized with trained with Indic specific language data suited for Indian problems',\n",
              "   'tokens': [50364,\n",
              "    1359,\n",
              "    5245,\n",
              "    19813,\n",
              "    365,\n",
              "    8895,\n",
              "    365,\n",
              "    2333,\n",
              "    299,\n",
              "    2685,\n",
              "    2856,\n",
              "    1412,\n",
              "    24736,\n",
              "    337,\n",
              "    6427,\n",
              "    2740,\n",
              "    50708],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12875296934595648,\n",
              "   'compression_ratio': 1.7194244604316546,\n",
              "   'no_speech_prob': 0.07539273053407669},\n",
              "  {'id': 120,\n",
              "   'seek': 71904,\n",
              "   'start': 725.92,\n",
              "   'end': 731.4399999999999,\n",
              "   'text': \" at a compelling cost point. We'll be suited for us. We're not solving some world autonomous vehicles\",\n",
              "   'tokens': [50708,\n",
              "    412,\n",
              "    257,\n",
              "    20050,\n",
              "    2063,\n",
              "    935,\n",
              "    13,\n",
              "    492,\n",
              "    603,\n",
              "    312,\n",
              "    24736,\n",
              "    337,\n",
              "    505,\n",
              "    13,\n",
              "    492,\n",
              "    434,\n",
              "    406,\n",
              "    12606,\n",
              "    512,\n",
              "    1002,\n",
              "    23797,\n",
              "    8948,\n",
              "    50984],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12875296934595648,\n",
              "   'compression_ratio': 1.7194244604316546,\n",
              "   'no_speech_prob': 0.07539273053407669},\n",
              "  {'id': 121,\n",
              "   'seek': 71904,\n",
              "   'start': 731.4399999999999,\n",
              "   'end': 736.56,\n",
              "   'text': \" or some complex problem. We're solving some basic problems specifically focused on voice\",\n",
              "   'tokens': [50984,\n",
              "    420,\n",
              "    512,\n",
              "    3997,\n",
              "    1154,\n",
              "    13,\n",
              "    492,\n",
              "    434,\n",
              "    12606,\n",
              "    512,\n",
              "    3875,\n",
              "    2740,\n",
              "    4682,\n",
              "    5178,\n",
              "    322,\n",
              "    3177,\n",
              "    51240],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12875296934595648,\n",
              "   'compression_ratio': 1.7194244604316546,\n",
              "   'no_speech_prob': 0.07539273053407669},\n",
              "  {'id': 122,\n",
              "   'seek': 71904,\n",
              "   'start': 736.56,\n",
              "   'end': 741.28,\n",
              "   'text': ' with multiple languages. That is what you see as a future. Am I paraphrasing this correctly?',\n",
              "   'tokens': [51240,\n",
              "    365,\n",
              "    3866,\n",
              "    8650,\n",
              "    13,\n",
              "    663,\n",
              "    307,\n",
              "    437,\n",
              "    291,\n",
              "    536,\n",
              "    382,\n",
              "    257,\n",
              "    2027,\n",
              "    13,\n",
              "    2012,\n",
              "    286,\n",
              "    36992,\n",
              "    1703,\n",
              "    3349,\n",
              "    341,\n",
              "    8944,\n",
              "    30,\n",
              "    51476],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12875296934595648,\n",
              "   'compression_ratio': 1.7194244604316546,\n",
              "   'no_speech_prob': 0.07539273053407669},\n",
              "  {'id': 123,\n",
              "   'seek': 71904,\n",
              "   'start': 741.28,\n",
              "   'end': 746.7199999999999,\n",
              "   'text': ' No, yeah. So I think that certainly, I mean, voice and Indian languages are an important part of',\n",
              "   'tokens': [51476,\n",
              "    883,\n",
              "    11,\n",
              "    1338,\n",
              "    13,\n",
              "    407,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    3297,\n",
              "    11,\n",
              "    286,\n",
              "    914,\n",
              "    11,\n",
              "    3177,\n",
              "    293,\n",
              "    6427,\n",
              "    8650,\n",
              "    366,\n",
              "    364,\n",
              "    1021,\n",
              "    644,\n",
              "    295,\n",
              "    51748],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12875296934595648,\n",
              "   'compression_ratio': 1.7194244604316546,\n",
              "   'no_speech_prob': 0.07539273053407669},\n",
              "  {'id': 124,\n",
              "   'seek': 74672,\n",
              "   'start': 746.72,\n",
              "   'end': 751.84,\n",
              "   'text': ' our strategy, but we will be building, you know, custom models to solve various other kinds of',\n",
              "   'tokens': [50364,\n",
              "    527,\n",
              "    5206,\n",
              "    11,\n",
              "    457,\n",
              "    321,\n",
              "    486,\n",
              "    312,\n",
              "    2390,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    2375,\n",
              "    5245,\n",
              "    281,\n",
              "    5039,\n",
              "    3683,\n",
              "    661,\n",
              "    3685,\n",
              "    295,\n",
              "    50620],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.20480822715438715,\n",
              "   'compression_ratio': 1.6151202749140894,\n",
              "   'no_speech_prob': 0.008764914236962795},\n",
              "  {'id': 125,\n",
              "   'seek': 74672,\n",
              "   'start': 751.84,\n",
              "   'end': 757.6,\n",
              "   'text': \" problems as well, right? That's not just limited to, I think, in different domains, working in\",\n",
              "   'tokens': [50620,\n",
              "    2740,\n",
              "    382,\n",
              "    731,\n",
              "    11,\n",
              "    558,\n",
              "    30,\n",
              "    663,\n",
              "    311,\n",
              "    406,\n",
              "    445,\n",
              "    5567,\n",
              "    281,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    11,\n",
              "    294,\n",
              "    819,\n",
              "    25514,\n",
              "    11,\n",
              "    1364,\n",
              "    294,\n",
              "    50908],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.20480822715438715,\n",
              "   'compression_ratio': 1.6151202749140894,\n",
              "   'no_speech_prob': 0.008764914236962795},\n",
              "  {'id': 126,\n",
              "   'seek': 74672,\n",
              "   'start': 757.6,\n",
              "   'end': 763.0400000000001,\n",
              "   'text': ' different domains, making building things based on unique data that enterprises have and things',\n",
              "   'tokens': [50908,\n",
              "    819,\n",
              "    25514,\n",
              "    11,\n",
              "    1455,\n",
              "    2390,\n",
              "    721,\n",
              "    2361,\n",
              "    322,\n",
              "    3845,\n",
              "    1412,\n",
              "    300,\n",
              "    29034,\n",
              "    362,\n",
              "    293,\n",
              "    721,\n",
              "    51180],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.20480822715438715,\n",
              "   'compression_ratio': 1.6151202749140894,\n",
              "   'no_speech_prob': 0.008764914236962795},\n",
              "  {'id': 127,\n",
              "   'seek': 74672,\n",
              "   'start': 763.0400000000001,\n",
              "   'end': 766.96,\n",
              "   'text': \" like that. So that's something that we'll also look at. Fair enough. So coming back to the\",\n",
              "   'tokens': [51180,\n",
              "    411,\n",
              "    300,\n",
              "    13,\n",
              "    407,\n",
              "    300,\n",
              "    311,\n",
              "    746,\n",
              "    300,\n",
              "    321,\n",
              "    603,\n",
              "    611,\n",
              "    574,\n",
              "    412,\n",
              "    13,\n",
              "    12157,\n",
              "    1547,\n",
              "    13,\n",
              "    407,\n",
              "    1348,\n",
              "    646,\n",
              "    281,\n",
              "    264,\n",
              "    51376],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.20480822715438715,\n",
              "   'compression_ratio': 1.6151202749140894,\n",
              "   'no_speech_prob': 0.008764914236962795},\n",
              "  {'id': 128,\n",
              "   'seek': 74672,\n",
              "   'start': 767.84,\n",
              "   'end': 773.76,\n",
              "   'text': ' elephant in the room, no fun intended with OpenHathe, what about Bavesh Akarwal and Kruthrim?',\n",
              "   'tokens': [51420,\n",
              "    19791,\n",
              "    294,\n",
              "    264,\n",
              "    1808,\n",
              "    11,\n",
              "    572,\n",
              "    1019,\n",
              "    10226,\n",
              "    365,\n",
              "    7238,\n",
              "    39,\n",
              "    267,\n",
              "    675,\n",
              "    11,\n",
              "    437,\n",
              "    466,\n",
              "    363,\n",
              "    706,\n",
              "    14935,\n",
              "    9629,\n",
              "    289,\n",
              "    29530,\n",
              "    293,\n",
              "    6332,\n",
              "    325,\n",
              "    1703,\n",
              "    332,\n",
              "    30,\n",
              "    51716],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.20480822715438715,\n",
              "   'compression_ratio': 1.6151202749140894,\n",
              "   'no_speech_prob': 0.008764914236962795},\n",
              "  {'id': 129,\n",
              "   'seek': 77376,\n",
              "   'start': 774.4,\n",
              "   'end': 778.3199999999999,\n",
              "   'text': \" What does your take on that? No, I think it's great. I think it's wonderful, right? I mean,\",\n",
              "   'tokens': [50396,\n",
              "    708,\n",
              "    775,\n",
              "    428,\n",
              "    747,\n",
              "    322,\n",
              "    300,\n",
              "    30,\n",
              "    883,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    309,\n",
              "    311,\n",
              "    869,\n",
              "    13,\n",
              "    286,\n",
              "    519,\n",
              "    309,\n",
              "    311,\n",
              "    3715,\n",
              "    11,\n",
              "    558,\n",
              "    30,\n",
              "    286,\n",
              "    914,\n",
              "    11,\n",
              "    50592],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10444385447400681,\n",
              "   'compression_ratio': 1.6991150442477876,\n",
              "   'no_speech_prob': 0.07079662382602692},\n",
              "  {'id': 130,\n",
              "   'seek': 77376,\n",
              "   'start': 778.3199999999999,\n",
              "   'end': 785.4399999999999,\n",
              "   'text': ' the fact that the technology AI is so important that we need multiple people working on it.',\n",
              "   'tokens': [50592,\n",
              "    264,\n",
              "    1186,\n",
              "    300,\n",
              "    264,\n",
              "    2899,\n",
              "    7318,\n",
              "    307,\n",
              "    370,\n",
              "    1021,\n",
              "    300,\n",
              "    321,\n",
              "    643,\n",
              "    3866,\n",
              "    561,\n",
              "    1364,\n",
              "    322,\n",
              "    309,\n",
              "    13,\n",
              "    50948],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10444385447400681,\n",
              "   'compression_ratio': 1.6991150442477876,\n",
              "   'no_speech_prob': 0.07079662382602692},\n",
              "  {'id': 131,\n",
              "   'seek': 77376,\n",
              "   'start': 785.4399999999999,\n",
              "   'end': 790.88,\n",
              "   'text': ' The fact that there are other people thinking is actually validates that this is an important problem',\n",
              "   'tokens': [50948,\n",
              "    440,\n",
              "    1186,\n",
              "    300,\n",
              "    456,\n",
              "    366,\n",
              "    661,\n",
              "    561,\n",
              "    1953,\n",
              "    307,\n",
              "    767,\n",
              "    7363,\n",
              "    1024,\n",
              "    300,\n",
              "    341,\n",
              "    307,\n",
              "    364,\n",
              "    1021,\n",
              "    1154,\n",
              "    51220],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10444385447400681,\n",
              "   'compression_ratio': 1.6991150442477876,\n",
              "   'no_speech_prob': 0.07079662382602692},\n",
              "  {'id': 132,\n",
              "   'seek': 77376,\n",
              "   'start': 790.88,\n",
              "   'end': 798.72,\n",
              "   'text': ' to be solved. And I think that we need everybody to come together and do that. So I really welcome',\n",
              "   'tokens': [51220,\n",
              "    281,\n",
              "    312,\n",
              "    13041,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    321,\n",
              "    643,\n",
              "    2201,\n",
              "    281,\n",
              "    808,\n",
              "    1214,\n",
              "    293,\n",
              "    360,\n",
              "    300,\n",
              "    13,\n",
              "    407,\n",
              "    286,\n",
              "    534,\n",
              "    2928,\n",
              "    51612],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10444385447400681,\n",
              "   'compression_ratio': 1.6991150442477876,\n",
              "   'no_speech_prob': 0.07079662382602692},\n",
              "  {'id': 133,\n",
              "   'seek': 79872,\n",
              "   'start': 798.72,\n",
              "   'end': 803.52,\n",
              "   'text': \" that. I think it's great. And I think that there'll be different people who will have different\",\n",
              "   'tokens': [50364,\n",
              "    300,\n",
              "    13,\n",
              "    286,\n",
              "    519,\n",
              "    309,\n",
              "    311,\n",
              "    869,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    456,\n",
              "    603,\n",
              "    312,\n",
              "    819,\n",
              "    561,\n",
              "    567,\n",
              "    486,\n",
              "    362,\n",
              "    819,\n",
              "    50604],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12102890014648438,\n",
              "   'compression_ratio': 1.7137931034482758,\n",
              "   'no_speech_prob': 0.05947350710630417},\n",
              "  {'id': 134,\n",
              "   'seek': 79872,\n",
              "   'start': 803.52,\n",
              "   'end': 809.9200000000001,\n",
              "   'text': ' takes as to how to solve this kind of problem. And hopefully as a result of that, the entire ecosystem',\n",
              "   'tokens': [50604,\n",
              "    2516,\n",
              "    382,\n",
              "    281,\n",
              "    577,\n",
              "    281,\n",
              "    5039,\n",
              "    341,\n",
              "    733,\n",
              "    295,\n",
              "    1154,\n",
              "    13,\n",
              "    400,\n",
              "    4696,\n",
              "    382,\n",
              "    257,\n",
              "    1874,\n",
              "    295,\n",
              "    300,\n",
              "    11,\n",
              "    264,\n",
              "    2302,\n",
              "    11311,\n",
              "    50924],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12102890014648438,\n",
              "   'compression_ratio': 1.7137931034482758,\n",
              "   'no_speech_prob': 0.05947350710630417},\n",
              "  {'id': 135,\n",
              "   'seek': 79872,\n",
              "   'start': 809.9200000000001,\n",
              "   'end': 815.52,\n",
              "   'text': \" benefits. One more question, and then I want to talk about some of the predictions that you've\",\n",
              "   'tokens': [50924,\n",
              "    5311,\n",
              "    13,\n",
              "    1485,\n",
              "    544,\n",
              "    1168,\n",
              "    11,\n",
              "    293,\n",
              "    550,\n",
              "    286,\n",
              "    528,\n",
              "    281,\n",
              "    751,\n",
              "    466,\n",
              "    512,\n",
              "    295,\n",
              "    264,\n",
              "    21264,\n",
              "    300,\n",
              "    291,\n",
              "    600,\n",
              "    51204],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12102890014648438,\n",
              "   'compression_ratio': 1.7137931034482758,\n",
              "   'no_speech_prob': 0.05947350710630417},\n",
              "  {'id': 136,\n",
              "   'seek': 79872,\n",
              "   'start': 815.52,\n",
              "   'end': 819.6800000000001,\n",
              "   'text': ' boldly made. So Vivek, I usually ask people about what do you think the future will be, and everybody',\n",
              "   'tokens': [51204,\n",
              "    11928,\n",
              "    356,\n",
              "    1027,\n",
              "    13,\n",
              "    407,\n",
              "    44288,\n",
              "    74,\n",
              "    11,\n",
              "    286,\n",
              "    2673,\n",
              "    1029,\n",
              "    561,\n",
              "    466,\n",
              "    437,\n",
              "    360,\n",
              "    291,\n",
              "    519,\n",
              "    264,\n",
              "    2027,\n",
              "    486,\n",
              "    312,\n",
              "    11,\n",
              "    293,\n",
              "    2201,\n",
              "    51412],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12102890014648438,\n",
              "   'compression_ratio': 1.7137931034482758,\n",
              "   'no_speech_prob': 0.05947350710630417},\n",
              "  {'id': 137,\n",
              "   'seek': 79872,\n",
              "   'start': 819.6800000000001,\n",
              "   'end': 825.28,\n",
              "   'text': ' usually hedges. I ask Vivek, what do you think is going to happen by December 2024? What do you think',\n",
              "   'tokens': [51412,\n",
              "    2673,\n",
              "    33653,\n",
              "    2880,\n",
              "    13,\n",
              "    286,\n",
              "    1029,\n",
              "    44288,\n",
              "    74,\n",
              "    11,\n",
              "    437,\n",
              "    360,\n",
              "    291,\n",
              "    519,\n",
              "    307,\n",
              "    516,\n",
              "    281,\n",
              "    1051,\n",
              "    538,\n",
              "    7687,\n",
              "    45237,\n",
              "    30,\n",
              "    708,\n",
              "    360,\n",
              "    291,\n",
              "    519,\n",
              "    51692],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12102890014648438,\n",
              "   'compression_ratio': 1.7137931034482758,\n",
              "   'no_speech_prob': 0.05947350710630417},\n",
              "  {'id': 138,\n",
              "   'seek': 82528,\n",
              "   'start': 825.28,\n",
              "   'end': 830.9599999999999,\n",
              "   'text': ' sitting in this room one year later, we can expect? And you made three bold predictions. So I want',\n",
              "   'tokens': [50364,\n",
              "    3798,\n",
              "    294,\n",
              "    341,\n",
              "    1808,\n",
              "    472,\n",
              "    1064,\n",
              "    1780,\n",
              "    11,\n",
              "    321,\n",
              "    393,\n",
              "    2066,\n",
              "    30,\n",
              "    400,\n",
              "    291,\n",
              "    1027,\n",
              "    1045,\n",
              "    11928,\n",
              "    21264,\n",
              "    13,\n",
              "    407,\n",
              "    286,\n",
              "    528,\n",
              "    50648],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17538021350729055,\n",
              "   'compression_ratio': 1.7925925925925925,\n",
              "   'no_speech_prob': 0.010916564613580704},\n",
              "  {'id': 139,\n",
              "   'seek': 82528,\n",
              "   'start': 830.9599999999999,\n",
              "   'end': 835.52,\n",
              "   'text': ' to talk about that before that I have one last question. What are the top three applications that you',\n",
              "   'tokens': [50648,\n",
              "    281,\n",
              "    751,\n",
              "    466,\n",
              "    300,\n",
              "    949,\n",
              "    300,\n",
              "    286,\n",
              "    362,\n",
              "    472,\n",
              "    1036,\n",
              "    1168,\n",
              "    13,\n",
              "    708,\n",
              "    366,\n",
              "    264,\n",
              "    1192,\n",
              "    1045,\n",
              "    5821,\n",
              "    300,\n",
              "    291,\n",
              "    50876],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17538021350729055,\n",
              "   'compression_ratio': 1.7925925925925925,\n",
              "   'no_speech_prob': 0.010916564613580704},\n",
              "  {'id': 140,\n",
              "   'seek': 82528,\n",
              "   'start': 835.52,\n",
              "   'end': 841.28,\n",
              "   'text': ' think are relevant for India? You would see the talk about medical, when any quick summary,',\n",
              "   'tokens': [50876,\n",
              "    519,\n",
              "    366,\n",
              "    7340,\n",
              "    337,\n",
              "    5282,\n",
              "    30,\n",
              "    509,\n",
              "    576,\n",
              "    536,\n",
              "    264,\n",
              "    751,\n",
              "    466,\n",
              "    4625,\n",
              "    11,\n",
              "    562,\n",
              "    604,\n",
              "    1702,\n",
              "    12691,\n",
              "    11,\n",
              "    51164],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17538021350729055,\n",
              "   'compression_ratio': 1.7925925925925925,\n",
              "   'no_speech_prob': 0.010916564613580704},\n",
              "  {'id': 141,\n",
              "   'seek': 82528,\n",
              "   'start': 841.28,\n",
              "   'end': 847.36,\n",
              "   'text': ' what do you think the top three apps are for India for AI? So I mean, I think that as you said,',\n",
              "   'tokens': [51164,\n",
              "    437,\n",
              "    360,\n",
              "    291,\n",
              "    519,\n",
              "    264,\n",
              "    1192,\n",
              "    1045,\n",
              "    7733,\n",
              "    366,\n",
              "    337,\n",
              "    5282,\n",
              "    337,\n",
              "    7318,\n",
              "    30,\n",
              "    407,\n",
              "    286,\n",
              "    914,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    382,\n",
              "    291,\n",
              "    848,\n",
              "    11,\n",
              "    51468],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17538021350729055,\n",
              "   'compression_ratio': 1.7925925925925925,\n",
              "   'no_speech_prob': 0.010916564613580704},\n",
              "  {'id': 142,\n",
              "   'seek': 82528,\n",
              "   'start': 847.36,\n",
              "   'end': 855.12,\n",
              "   'text': ' things like education and medical are clearly areas where I think that things can be leveraged.',\n",
              "   'tokens': [51468,\n",
              "    721,\n",
              "    411,\n",
              "    3309,\n",
              "    293,\n",
              "    4625,\n",
              "    366,\n",
              "    4448,\n",
              "    3179,\n",
              "    689,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    721,\n",
              "    393,\n",
              "    312,\n",
              "    12451,\n",
              "    2980,\n",
              "    13,\n",
              "    51856],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17538021350729055,\n",
              "   'compression_ratio': 1.7925925925925925,\n",
              "   'no_speech_prob': 0.010916564613580704},\n",
              "  {'id': 143,\n",
              "   'seek': 85512,\n",
              "   'start': 855.12,\n",
              "   'end': 861.6,\n",
              "   'text': ' The whole idea of all these kind of the DPI aspect of it is another major application where things',\n",
              "   'tokens': [50364,\n",
              "    440,\n",
              "    1379,\n",
              "    1558,\n",
              "    295,\n",
              "    439,\n",
              "    613,\n",
              "    733,\n",
              "    295,\n",
              "    264,\n",
              "    413,\n",
              "    31701,\n",
              "    4171,\n",
              "    295,\n",
              "    309,\n",
              "    307,\n",
              "    1071,\n",
              "    2563,\n",
              "    3861,\n",
              "    689,\n",
              "    721,\n",
              "    50688],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.2034808849466258,\n",
              "   'compression_ratio': 1.6814159292035398,\n",
              "   'no_speech_prob': 0.00956549309194088},\n",
              "  {'id': 144,\n",
              "   'seek': 85512,\n",
              "   'start': 861.6,\n",
              "   'end': 866.16,\n",
              "   'text': \" can happen. And here I'm talking about country specific. And I think the whole idea which\",\n",
              "   'tokens': [50688,\n",
              "    393,\n",
              "    1051,\n",
              "    13,\n",
              "    400,\n",
              "    510,\n",
              "    286,\n",
              "    478,\n",
              "    1417,\n",
              "    466,\n",
              "    1941,\n",
              "    2685,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    264,\n",
              "    1379,\n",
              "    1558,\n",
              "    597,\n",
              "    50916],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.2034808849466258,\n",
              "   'compression_ratio': 1.6814159292035398,\n",
              "   'no_speech_prob': 0.00956549309194088},\n",
              "  {'id': 145,\n",
              "   'seek': 85512,\n",
              "   'start': 866.16,\n",
              "   'end': 872.88,\n",
              "   'text': \" we've also talked about was the concept of software. And I think that clearly we have a very large\",\n",
              "   'tokens': [50916,\n",
              "    321,\n",
              "    600,\n",
              "    611,\n",
              "    2825,\n",
              "    466,\n",
              "    390,\n",
              "    264,\n",
              "    3410,\n",
              "    295,\n",
              "    4722,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    4448,\n",
              "    321,\n",
              "    362,\n",
              "    257,\n",
              "    588,\n",
              "    2416,\n",
              "    51252],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.2034808849466258,\n",
              "   'compression_ratio': 1.6814159292035398,\n",
              "   'no_speech_prob': 0.00956549309194088},\n",
              "  {'id': 146,\n",
              "   'seek': 85512,\n",
              "   'start': 872.88,\n",
              "   'end': 877.84,\n",
              "   'text': \" software industry and how to reimagine those things in this context is also something that's\",\n",
              "   'tokens': [51252,\n",
              "    4722,\n",
              "    3518,\n",
              "    293,\n",
              "    577,\n",
              "    281,\n",
              "    33433,\n",
              "    10260,\n",
              "    729,\n",
              "    721,\n",
              "    294,\n",
              "    341,\n",
              "    4319,\n",
              "    307,\n",
              "    611,\n",
              "    746,\n",
              "    300,\n",
              "    311,\n",
              "    51500],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.2034808849466258,\n",
              "   'compression_ratio': 1.6814159292035398,\n",
              "   'no_speech_prob': 0.00956549309194088},\n",
              "  {'id': 147,\n",
              "   'seek': 87784,\n",
              "   'start': 877.84,\n",
              "   'end': 886.32,\n",
              "   'text': \" quite fair enough. Are you guys ready for Vivek Ragavan's bold predictions? Yes? No, I'm not hearing any,\",\n",
              "   'tokens': [50364,\n",
              "    1596,\n",
              "    3143,\n",
              "    1547,\n",
              "    13,\n",
              "    2014,\n",
              "    291,\n",
              "    1074,\n",
              "    1919,\n",
              "    337,\n",
              "    44288,\n",
              "    74,\n",
              "    44289,\n",
              "    21071,\n",
              "    311,\n",
              "    11928,\n",
              "    21264,\n",
              "    30,\n",
              "    1079,\n",
              "    30,\n",
              "    883,\n",
              "    11,\n",
              "    286,\n",
              "    478,\n",
              "    406,\n",
              "    4763,\n",
              "    604,\n",
              "    11,\n",
              "    50788],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.21184658281730884,\n",
              "   'compression_ratio': 1.6501650165016502,\n",
              "   'no_speech_prob': 0.09612346440553665},\n",
              "  {'id': 148,\n",
              "   'seek': 87784,\n",
              "   'start': 886.32,\n",
              "   'end': 890.8000000000001,\n",
              "   'text': \" yes, this is like a big deal. He's like one of the smartest guys that I know. He wants to make three\",\n",
              "   'tokens': [50788,\n",
              "    2086,\n",
              "    11,\n",
              "    341,\n",
              "    307,\n",
              "    411,\n",
              "    257,\n",
              "    955,\n",
              "    2028,\n",
              "    13,\n",
              "    634,\n",
              "    311,\n",
              "    411,\n",
              "    472,\n",
              "    295,\n",
              "    264,\n",
              "    41491,\n",
              "    1074,\n",
              "    300,\n",
              "    286,\n",
              "    458,\n",
              "    13,\n",
              "    634,\n",
              "    2738,\n",
              "    281,\n",
              "    652,\n",
              "    1045,\n",
              "    51012],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.21184658281730884,\n",
              "   'compression_ratio': 1.6501650165016502,\n",
              "   'no_speech_prob': 0.09612346440553665},\n",
              "  {'id': 149,\n",
              "   'seek': 87784,\n",
              "   'start': 890.8000000000001,\n",
              "   'end': 897.2800000000001,\n",
              "   'text': \" predictions. You don't want to hear it. All right. So I asked him, what do you think, you know,\",\n",
              "   'tokens': [51012,\n",
              "    21264,\n",
              "    13,\n",
              "    509,\n",
              "    500,\n",
              "    380,\n",
              "    528,\n",
              "    281,\n",
              "    1568,\n",
              "    309,\n",
              "    13,\n",
              "    1057,\n",
              "    558,\n",
              "    13,\n",
              "    407,\n",
              "    286,\n",
              "    2351,\n",
              "    796,\n",
              "    11,\n",
              "    437,\n",
              "    360,\n",
              "    291,\n",
              "    519,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    51336],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.21184658281730884,\n",
              "   'compression_ratio': 1.6501650165016502,\n",
              "   'no_speech_prob': 0.09612346440553665},\n",
              "  {'id': 150,\n",
              "   'seek': 87784,\n",
              "   'start': 897.2800000000001,\n",
              "   'end': 901.84,\n",
              "   'text': ' year later, what do you think we can expect? And he came up with three things and usually people give',\n",
              "   'tokens': [51336,\n",
              "    1064,\n",
              "    1780,\n",
              "    11,\n",
              "    437,\n",
              "    360,\n",
              "    291,\n",
              "    519,\n",
              "    321,\n",
              "    393,\n",
              "    2066,\n",
              "    30,\n",
              "    400,\n",
              "    415,\n",
              "    1361,\n",
              "    493,\n",
              "    365,\n",
              "    1045,\n",
              "    721,\n",
              "    293,\n",
              "    2673,\n",
              "    561,\n",
              "    976,\n",
              "    51564],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.21184658281730884,\n",
              "   'compression_ratio': 1.6501650165016502,\n",
              "   'no_speech_prob': 0.09612346440553665},\n",
              "  {'id': 151,\n",
              "   'seek': 87784,\n",
              "   'start': 901.84,\n",
              "   'end': 906.08,\n",
              "   'text': \" very blind answers when you ask questions like this because they don't want to be caught wrong.\",\n",
              "   'tokens': [51564,\n",
              "    588,\n",
              "    6865,\n",
              "    6338,\n",
              "    562,\n",
              "    291,\n",
              "    1029,\n",
              "    1651,\n",
              "    411,\n",
              "    341,\n",
              "    570,\n",
              "    436,\n",
              "    500,\n",
              "    380,\n",
              "    528,\n",
              "    281,\n",
              "    312,\n",
              "    5415,\n",
              "    2085,\n",
              "    13,\n",
              "    51776],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.21184658281730884,\n",
              "   'compression_ratio': 1.6501650165016502,\n",
              "   'no_speech_prob': 0.09612346440553665},\n",
              "  {'id': 152,\n",
              "   'seek': 90608,\n",
              "   'start': 906.08,\n",
              "   'end': 911.2,\n",
              "   'text': \" Not Vivek. Vivek is bold. So he basically said three things and I'm going to list out the three\",\n",
              "   'tokens': [50364,\n",
              "    1726,\n",
              "    44288,\n",
              "    74,\n",
              "    13,\n",
              "    44288,\n",
              "    74,\n",
              "    307,\n",
              "    11928,\n",
              "    13,\n",
              "    407,\n",
              "    415,\n",
              "    1936,\n",
              "    848,\n",
              "    1045,\n",
              "    721,\n",
              "    293,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    1329,\n",
              "    484,\n",
              "    264,\n",
              "    1045,\n",
              "    50620],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12806480930697534,\n",
              "   'compression_ratio': 1.7619047619047619,\n",
              "   'no_speech_prob': 0.033556435257196426},\n",
              "  {'id': 153,\n",
              "   'seek': 90608,\n",
              "   'start': 911.2,\n",
              "   'end': 916.88,\n",
              "   'text': \" things and then he's asking about it. So number one, he says, I will prefer to talk to an automated\",\n",
              "   'tokens': [50620,\n",
              "    721,\n",
              "    293,\n",
              "    550,\n",
              "    415,\n",
              "    311,\n",
              "    3365,\n",
              "    466,\n",
              "    309,\n",
              "    13,\n",
              "    407,\n",
              "    1230,\n",
              "    472,\n",
              "    11,\n",
              "    415,\n",
              "    1619,\n",
              "    11,\n",
              "    286,\n",
              "    486,\n",
              "    4382,\n",
              "    281,\n",
              "    751,\n",
              "    281,\n",
              "    364,\n",
              "    18473,\n",
              "    50904],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12806480930697534,\n",
              "   'compression_ratio': 1.7619047619047619,\n",
              "   'no_speech_prob': 0.033556435257196426},\n",
              "  {'id': 154,\n",
              "   'seek': 90608,\n",
              "   'start': 916.88,\n",
              "   'end': 922.72,\n",
              "   'text': \" customer service than a real person because they'll give me a better answer. So that is Vivek Ragavan's\",\n",
              "   'tokens': [50904,\n",
              "    5474,\n",
              "    2643,\n",
              "    813,\n",
              "    257,\n",
              "    957,\n",
              "    954,\n",
              "    570,\n",
              "    436,\n",
              "    603,\n",
              "    976,\n",
              "    385,\n",
              "    257,\n",
              "    1101,\n",
              "    1867,\n",
              "    13,\n",
              "    407,\n",
              "    300,\n",
              "    307,\n",
              "    44288,\n",
              "    74,\n",
              "    44289,\n",
              "    21071,\n",
              "    311,\n",
              "    51196],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12806480930697534,\n",
              "   'compression_ratio': 1.7619047619047619,\n",
              "   'no_speech_prob': 0.033556435257196426},\n",
              "  {'id': 155,\n",
              "   'seek': 90608,\n",
              "   'start': 922.72,\n",
              "   'end': 928.96,\n",
              "   'text': ' prediction number one. So number two is that when everybody is talking about a GPU shortage,',\n",
              "   'tokens': [51196,\n",
              "    17630,\n",
              "    1230,\n",
              "    472,\n",
              "    13,\n",
              "    407,\n",
              "    1230,\n",
              "    732,\n",
              "    307,\n",
              "    300,\n",
              "    562,\n",
              "    2201,\n",
              "    307,\n",
              "    1417,\n",
              "    466,\n",
              "    257,\n",
              "    18407,\n",
              "    24708,\n",
              "    11,\n",
              "    51508],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12806480930697534,\n",
              "   'compression_ratio': 1.7619047619047619,\n",
              "   'no_speech_prob': 0.033556435257196426},\n",
              "  {'id': 156,\n",
              "   'seek': 90608,\n",
              "   'start': 929.84,\n",
              "   'end': 933.76,\n",
              "   'text': \" Vivek predicts that there'll be a GPU glutton India. He thinks there'll be too much GPU.\",\n",
              "   'tokens': [51552,\n",
              "    44288,\n",
              "    74,\n",
              "    6069,\n",
              "    82,\n",
              "    300,\n",
              "    456,\n",
              "    603,\n",
              "    312,\n",
              "    257,\n",
              "    18407,\n",
              "    33249,\n",
              "    1756,\n",
              "    5282,\n",
              "    13,\n",
              "    634,\n",
              "    7309,\n",
              "    456,\n",
              "    603,\n",
              "    312,\n",
              "    886,\n",
              "    709,\n",
              "    18407,\n",
              "    13,\n",
              "    51748],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12806480930697534,\n",
              "   'compression_ratio': 1.7619047619047619,\n",
              "   'no_speech_prob': 0.033556435257196426},\n",
              "  {'id': 157,\n",
              "   'seek': 93376,\n",
              "   'start': 933.76,\n",
              "   'end': 940.3199999999999,\n",
              "   'text': ' Okay. So if you want a short and media stock, this is a good time. And number three, which was',\n",
              "   'tokens': [50364,\n",
              "    1033,\n",
              "    13,\n",
              "    407,\n",
              "    498,\n",
              "    291,\n",
              "    528,\n",
              "    257,\n",
              "    2099,\n",
              "    293,\n",
              "    3021,\n",
              "    4127,\n",
              "    11,\n",
              "    341,\n",
              "    307,\n",
              "    257,\n",
              "    665,\n",
              "    565,\n",
              "    13,\n",
              "    400,\n",
              "    1230,\n",
              "    1045,\n",
              "    11,\n",
              "    597,\n",
              "    390,\n",
              "    50692],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17880544857102998,\n",
              "   'compression_ratio': 1.5875,\n",
              "   'no_speech_prob': 0.006016063503921032},\n",
              "  {'id': 158,\n",
              "   'seek': 93376,\n",
              "   'start': 940.3199999999999,\n",
              "   'end': 947.36,\n",
              "   'text': ' extremely unexpected, he said some companies will suddenly die. Okay. So Vivek, these are not what I',\n",
              "   'tokens': [50692,\n",
              "    4664,\n",
              "    13106,\n",
              "    11,\n",
              "    415,\n",
              "    848,\n",
              "    512,\n",
              "    3431,\n",
              "    486,\n",
              "    5800,\n",
              "    978,\n",
              "    13,\n",
              "    1033,\n",
              "    13,\n",
              "    407,\n",
              "    44288,\n",
              "    74,\n",
              "    11,\n",
              "    613,\n",
              "    366,\n",
              "    406,\n",
              "    437,\n",
              "    286,\n",
              "    51044],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17880544857102998,\n",
              "   'compression_ratio': 1.5875,\n",
              "   'no_speech_prob': 0.006016063503921032},\n",
              "  {'id': 159,\n",
              "   'seek': 93376,\n",
              "   'start': 947.36,\n",
              "   'end': 954.72,\n",
              "   'text': ' expected. So you want to quickly talk about each of them, why you just came up with these and then',\n",
              "   'tokens': [51044,\n",
              "    5176,\n",
              "    13,\n",
              "    407,\n",
              "    291,\n",
              "    528,\n",
              "    281,\n",
              "    2661,\n",
              "    751,\n",
              "    466,\n",
              "    1184,\n",
              "    295,\n",
              "    552,\n",
              "    11,\n",
              "    983,\n",
              "    291,\n",
              "    445,\n",
              "    1361,\n",
              "    493,\n",
              "    365,\n",
              "    613,\n",
              "    293,\n",
              "    550,\n",
              "    51412],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17880544857102998,\n",
              "   'compression_ratio': 1.5875,\n",
              "   'no_speech_prob': 0.006016063503921032},\n",
              "  {'id': 160,\n",
              "   'seek': 93376,\n",
              "   'start': 954.72,\n",
              "   'end': 960.0,\n",
              "   'text': \" we'll throw the audience questions. So I don't think I quite said it the way that that\",\n",
              "   'tokens': [51412,\n",
              "    321,\n",
              "    603,\n",
              "    3507,\n",
              "    264,\n",
              "    4034,\n",
              "    1651,\n",
              "    13,\n",
              "    407,\n",
              "    286,\n",
              "    500,\n",
              "    380,\n",
              "    519,\n",
              "    286,\n",
              "    1596,\n",
              "    848,\n",
              "    309,\n",
              "    264,\n",
              "    636,\n",
              "    300,\n",
              "    300,\n",
              "    51676],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17880544857102998,\n",
              "   'compression_ratio': 1.5875,\n",
              "   'no_speech_prob': 0.006016063503921032},\n",
              "  {'id': 161,\n",
              "   'seek': 96000,\n",
              "   'start': 960.0,\n",
              "   'end': 968.24,\n",
              "   'text': \" Vala Rianne is. But it's interesting. But I think the first thing that we said is I think that\",\n",
              "   'tokens': [50364,\n",
              "    691,\n",
              "    5159,\n",
              "    497,\n",
              "    952,\n",
              "    716,\n",
              "    307,\n",
              "    13,\n",
              "    583,\n",
              "    309,\n",
              "    311,\n",
              "    1880,\n",
              "    13,\n",
              "    583,\n",
              "    286,\n",
              "    519,\n",
              "    264,\n",
              "    700,\n",
              "    551,\n",
              "    300,\n",
              "    321,\n",
              "    848,\n",
              "    307,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    50776],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.26167116359788545,\n",
              "   'compression_ratio': 1.7511520737327189,\n",
              "   'no_speech_prob': 0.0606599859893322},\n",
              "  {'id': 162,\n",
              "   'seek': 96000,\n",
              "   'start': 969.12,\n",
              "   'end': 977.12,\n",
              "   'text': \" and I don't think that this is I think there will come a time when in areas of customer service,\",\n",
              "   'tokens': [50820,\n",
              "    293,\n",
              "    286,\n",
              "    500,\n",
              "    380,\n",
              "    519,\n",
              "    300,\n",
              "    341,\n",
              "    307,\n",
              "    286,\n",
              "    519,\n",
              "    456,\n",
              "    486,\n",
              "    808,\n",
              "    257,\n",
              "    565,\n",
              "    562,\n",
              "    294,\n",
              "    3179,\n",
              "    295,\n",
              "    5474,\n",
              "    2643,\n",
              "    11,\n",
              "    51220],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.26167116359788545,\n",
              "   'compression_ratio': 1.7511520737327189,\n",
              "   'no_speech_prob': 0.0606599859893322},\n",
              "  {'id': 163,\n",
              "   'seek': 96000,\n",
              "   'start': 977.12,\n",
              "   'end': 982.96,\n",
              "   'text': ' et cetera, when you want to do something very specific, today when you call when you call',\n",
              "   'tokens': [51220,\n",
              "    1030,\n",
              "    11458,\n",
              "    11,\n",
              "    562,\n",
              "    291,\n",
              "    528,\n",
              "    281,\n",
              "    360,\n",
              "    746,\n",
              "    588,\n",
              "    2685,\n",
              "    11,\n",
              "    965,\n",
              "    562,\n",
              "    291,\n",
              "    818,\n",
              "    562,\n",
              "    291,\n",
              "    818,\n",
              "    51512],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.26167116359788545,\n",
              "   'compression_ratio': 1.7511520737327189,\n",
              "   'no_speech_prob': 0.0606599859893322},\n",
              "  {'id': 164,\n",
              "   'seek': 96000,\n",
              "   'start': 982.96,\n",
              "   'end': 988.72,\n",
              "   'text': \" some kind of a bot, you actually end up, you mostly try to disconnect the call or you're extremely\",\n",
              "   'tokens': [51512,\n",
              "    512,\n",
              "    733,\n",
              "    295,\n",
              "    257,\n",
              "    10592,\n",
              "    11,\n",
              "    291,\n",
              "    767,\n",
              "    917,\n",
              "    493,\n",
              "    11,\n",
              "    291,\n",
              "    5240,\n",
              "    853,\n",
              "    281,\n",
              "    14299,\n",
              "    264,\n",
              "    818,\n",
              "    420,\n",
              "    291,\n",
              "    434,\n",
              "    4664,\n",
              "    51800],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.26167116359788545,\n",
              "   'compression_ratio': 1.7511520737327189,\n",
              "   'no_speech_prob': 0.0606599859893322},\n",
              "  {'id': 165,\n",
              "   'seek': 98872,\n",
              "   'start': 988.88,\n",
              "   'end': 994.4,\n",
              "   'text': \" upset that you're talking to a bot. But I think that there will come a time and I'm predicting it\",\n",
              "   'tokens': [50372,\n",
              "    8340,\n",
              "    300,\n",
              "    291,\n",
              "    434,\n",
              "    1417,\n",
              "    281,\n",
              "    257,\n",
              "    10592,\n",
              "    13,\n",
              "    583,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    456,\n",
              "    486,\n",
              "    808,\n",
              "    257,\n",
              "    565,\n",
              "    293,\n",
              "    286,\n",
              "    478,\n",
              "    32884,\n",
              "    309,\n",
              "    50648],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16001122338431223,\n",
              "   'compression_ratio': 1.9109311740890689,\n",
              "   'no_speech_prob': 0.017239566892385483},\n",
              "  {'id': 166,\n",
              "   'seek': 98872,\n",
              "   'start': 994.4,\n",
              "   'end': 999.9200000000001,\n",
              "   'text': ' as sooner than later that you will actually get better responses from the bot than what the',\n",
              "   'tokens': [50648,\n",
              "    382,\n",
              "    15324,\n",
              "    813,\n",
              "    1780,\n",
              "    300,\n",
              "    291,\n",
              "    486,\n",
              "    767,\n",
              "    483,\n",
              "    1101,\n",
              "    13019,\n",
              "    490,\n",
              "    264,\n",
              "    10592,\n",
              "    813,\n",
              "    437,\n",
              "    264,\n",
              "    50924],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16001122338431223,\n",
              "   'compression_ratio': 1.9109311740890689,\n",
              "   'no_speech_prob': 0.017239566892385483},\n",
              "  {'id': 167,\n",
              "   'seek': 98872,\n",
              "   'start': 999.9200000000001,\n",
              "   'end': 1005.84,\n",
              "   'text': ' human representative that at least the average human representative that you could talk to could give.',\n",
              "   'tokens': [50924,\n",
              "    1952,\n",
              "    12424,\n",
              "    300,\n",
              "    412,\n",
              "    1935,\n",
              "    264,\n",
              "    4274,\n",
              "    1952,\n",
              "    12424,\n",
              "    300,\n",
              "    291,\n",
              "    727,\n",
              "    751,\n",
              "    281,\n",
              "    727,\n",
              "    976,\n",
              "    13,\n",
              "    51220],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16001122338431223,\n",
              "   'compression_ratio': 1.9109311740890689,\n",
              "   'no_speech_prob': 0.017239566892385483},\n",
              "  {'id': 168,\n",
              "   'seek': 98872,\n",
              "   'start': 1005.84,\n",
              "   'end': 1010.64,\n",
              "   'text': \" And I think that that's just a, I just said that that there will come a time where you know,\",\n",
              "   'tokens': [51220,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    300,\n",
              "    311,\n",
              "    445,\n",
              "    257,\n",
              "    11,\n",
              "    286,\n",
              "    445,\n",
              "    848,\n",
              "    300,\n",
              "    300,\n",
              "    456,\n",
              "    486,\n",
              "    808,\n",
              "    257,\n",
              "    565,\n",
              "    689,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    51460],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16001122338431223,\n",
              "   'compression_ratio': 1.9109311740890689,\n",
              "   'no_speech_prob': 0.017239566892385483},\n",
              "  {'id': 169,\n",
              "   'seek': 98872,\n",
              "   'start': 1010.64,\n",
              "   'end': 1015.6,\n",
              "   'text': \" it's not a human you're talking to, but it's probably more likely to solve your intent\",\n",
              "   'tokens': [51460,\n",
              "    309,\n",
              "    311,\n",
              "    406,\n",
              "    257,\n",
              "    1952,\n",
              "    291,\n",
              "    434,\n",
              "    1417,\n",
              "    281,\n",
              "    11,\n",
              "    457,\n",
              "    309,\n",
              "    311,\n",
              "    1391,\n",
              "    544,\n",
              "    3700,\n",
              "    281,\n",
              "    5039,\n",
              "    428,\n",
              "    8446,\n",
              "    51708],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16001122338431223,\n",
              "   'compression_ratio': 1.9109311740890689,\n",
              "   'no_speech_prob': 0.017239566892385483},\n",
              "  {'id': 170,\n",
              "   'seek': 101560,\n",
              "   'start': 1016.5600000000001,\n",
              "   'end': 1022.96,\n",
              "   'text': \" than the human person. That's just something that I think that could happen.\",\n",
              "   'tokens': [50412,\n",
              "    813,\n",
              "    264,\n",
              "    1952,\n",
              "    954,\n",
              "    13,\n",
              "    663,\n",
              "    311,\n",
              "    445,\n",
              "    746,\n",
              "    300,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    727,\n",
              "    1051,\n",
              "    13,\n",
              "    50732],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.25746300106956843,\n",
              "   'compression_ratio': 1.5781990521327014,\n",
              "   'no_speech_prob': 0.03129865229129791},\n",
              "  {'id': 171,\n",
              "   'seek': 101560,\n",
              "   'start': 1023.6800000000001,\n",
              "   'end': 1027.44,\n",
              "   'text': \" Definitely controversial, but we'll let it go. What about the GPU glut?\",\n",
              "   'tokens': [50768,\n",
              "    12151,\n",
              "    17323,\n",
              "    11,\n",
              "    457,\n",
              "    321,\n",
              "    603,\n",
              "    718,\n",
              "    309,\n",
              "    352,\n",
              "    13,\n",
              "    708,\n",
              "    466,\n",
              "    264,\n",
              "    18407,\n",
              "    33249,\n",
              "    30,\n",
              "    50956],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.25746300106956843,\n",
              "   'compression_ratio': 1.5781990521327014,\n",
              "   'no_speech_prob': 0.03129865229129791},\n",
              "  {'id': 172,\n",
              "   'seek': 101560,\n",
              "   'start': 1028.24,\n",
              "   'end': 1035.04,\n",
              "   'text': \" No, I don't think that. So I think that the fact that there is a tremendous shortage right now,\",\n",
              "   'tokens': [50996,\n",
              "    883,\n",
              "    11,\n",
              "    286,\n",
              "    500,\n",
              "    380,\n",
              "    519,\n",
              "    300,\n",
              "    13,\n",
              "    407,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    264,\n",
              "    1186,\n",
              "    300,\n",
              "    456,\n",
              "    307,\n",
              "    257,\n",
              "    10048,\n",
              "    24708,\n",
              "    558,\n",
              "    586,\n",
              "    11,\n",
              "    51336],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.25746300106956843,\n",
              "   'compression_ratio': 1.5781990521327014,\n",
              "   'no_speech_prob': 0.03129865229129791},\n",
              "  {'id': 173,\n",
              "   'seek': 101560,\n",
              "   'start': 1035.04,\n",
              "   'end': 1040.48,\n",
              "   'text': ' I think that shortage will ease because that is how the cycles of things go, right? When',\n",
              "   'tokens': [51336,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    24708,\n",
              "    486,\n",
              "    12708,\n",
              "    570,\n",
              "    300,\n",
              "    307,\n",
              "    577,\n",
              "    264,\n",
              "    17796,\n",
              "    295,\n",
              "    721,\n",
              "    352,\n",
              "    11,\n",
              "    558,\n",
              "    30,\n",
              "    1133,\n",
              "    51608],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.25746300106956843,\n",
              "   'compression_ratio': 1.5781990521327014,\n",
              "   'no_speech_prob': 0.03129865229129791},\n",
              "  {'id': 174,\n",
              "   'seek': 104048,\n",
              "   'start': 1041.3600000000001,\n",
              "   'end': 1046.4,\n",
              "   'text': ' I think the fact that there was such a severe shortage last year, you know, basically caused',\n",
              "   'tokens': [50408,\n",
              "    286,\n",
              "    519,\n",
              "    264,\n",
              "    1186,\n",
              "    300,\n",
              "    456,\n",
              "    390,\n",
              "    1270,\n",
              "    257,\n",
              "    8922,\n",
              "    24708,\n",
              "    1036,\n",
              "    1064,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    1936,\n",
              "    7008,\n",
              "    50660],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1778433539650657,\n",
              "   'compression_ratio': 1.6272727272727272,\n",
              "   'no_speech_prob': 0.03317919000983238},\n",
              "  {'id': 175,\n",
              "   'seek': 104048,\n",
              "   'start': 1047.04,\n",
              "   'end': 1052.72,\n",
              "   'text': ' a number of different players to ramp up in various kinds of forms. And I think that that',\n",
              "   'tokens': [50692,\n",
              "    257,\n",
              "    1230,\n",
              "    295,\n",
              "    819,\n",
              "    4150,\n",
              "    281,\n",
              "    12428,\n",
              "    493,\n",
              "    294,\n",
              "    3683,\n",
              "    3685,\n",
              "    295,\n",
              "    6422,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    300,\n",
              "    50976],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1778433539650657,\n",
              "   'compression_ratio': 1.6272727272727272,\n",
              "   'no_speech_prob': 0.03317919000983238},\n",
              "  {'id': 176,\n",
              "   'seek': 104048,\n",
              "   'start': 1052.72,\n",
              "   'end': 1058.08,\n",
              "   'text': ' will always go in a cycle. But you may find out that there are many, many more interesting',\n",
              "   'tokens': [50976,\n",
              "    486,\n",
              "    1009,\n",
              "    352,\n",
              "    294,\n",
              "    257,\n",
              "    6586,\n",
              "    13,\n",
              "    583,\n",
              "    291,\n",
              "    815,\n",
              "    915,\n",
              "    484,\n",
              "    300,\n",
              "    456,\n",
              "    366,\n",
              "    867,\n",
              "    11,\n",
              "    867,\n",
              "    544,\n",
              "    1880,\n",
              "    51244],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1778433539650657,\n",
              "   'compression_ratio': 1.6272727272727272,\n",
              "   'no_speech_prob': 0.03317919000983238},\n",
              "  {'id': 177,\n",
              "   'seek': 104048,\n",
              "   'start': 1058.08,\n",
              "   'end': 1065.92,\n",
              "   'text': ' problems that people will be able to solve. I still remember, you know, we were at a',\n",
              "   'tokens': [51244,\n",
              "    2740,\n",
              "    300,\n",
              "    561,\n",
              "    486,\n",
              "    312,\n",
              "    1075,\n",
              "    281,\n",
              "    5039,\n",
              "    13,\n",
              "    286,\n",
              "    920,\n",
              "    1604,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    321,\n",
              "    645,\n",
              "    412,\n",
              "    257,\n",
              "    51636],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1778433539650657,\n",
              "   'compression_ratio': 1.6272727272727272,\n",
              "   'no_speech_prob': 0.03317919000983238},\n",
              "  {'id': 178,\n",
              "   'seek': 106592,\n",
              "   'start': 1066.5600000000002,\n",
              "   'end': 1072.16,\n",
              "   'text': ' Genie I event in Bangalore and we were talking to people and we said, you know, how many people',\n",
              "   'tokens': [50396,\n",
              "    3632,\n",
              "    414,\n",
              "    286,\n",
              "    2280,\n",
              "    294,\n",
              "    11538,\n",
              "    304,\n",
              "    418,\n",
              "    293,\n",
              "    321,\n",
              "    645,\n",
              "    1417,\n",
              "    281,\n",
              "    561,\n",
              "    293,\n",
              "    321,\n",
              "    848,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    577,\n",
              "    867,\n",
              "    561,\n",
              "    50676],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1886990330800289,\n",
              "   'compression_ratio': 1.828793774319066,\n",
              "   'no_speech_prob': 0.07368136197328568},\n",
              "  {'id': 179,\n",
              "   'seek': 106592,\n",
              "   'start': 1072.16,\n",
              "   'end': 1077.3600000000001,\n",
              "   'text': ' have access to, you know, four, eight hundred, so this was the question that I asked. And nobody',\n",
              "   'tokens': [50676,\n",
              "    362,\n",
              "    2105,\n",
              "    281,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    1451,\n",
              "    11,\n",
              "    3180,\n",
              "    3262,\n",
              "    11,\n",
              "    370,\n",
              "    341,\n",
              "    390,\n",
              "    264,\n",
              "    1168,\n",
              "    300,\n",
              "    286,\n",
              "    2351,\n",
              "    13,\n",
              "    400,\n",
              "    5079,\n",
              "    50936],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1886990330800289,\n",
              "   'compression_ratio': 1.828793774319066,\n",
              "   'no_speech_prob': 0.07368136197328568},\n",
              "  {'id': 180,\n",
              "   'seek': 106592,\n",
              "   'start': 1077.3600000000001,\n",
              "   'end': 1081.6000000000001,\n",
              "   'text': ' in the room, and these are all extremely enthusiastic Genie I people and nobody had access.',\n",
              "   'tokens': [50936,\n",
              "    294,\n",
              "    264,\n",
              "    1808,\n",
              "    11,\n",
              "    293,\n",
              "    613,\n",
              "    366,\n",
              "    439,\n",
              "    4664,\n",
              "    28574,\n",
              "    3632,\n",
              "    414,\n",
              "    286,\n",
              "    561,\n",
              "    293,\n",
              "    5079,\n",
              "    632,\n",
              "    2105,\n",
              "    13,\n",
              "    51148],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1886990330800289,\n",
              "   'compression_ratio': 1.828793774319066,\n",
              "   'no_speech_prob': 0.07368136197328568},\n",
              "  {'id': 181,\n",
              "   'seek': 106592,\n",
              "   'start': 1081.6000000000001,\n",
              "   'end': 1085.76,\n",
              "   'text': ' And I think that thing is going to change. You will be able to get these kinds of things and',\n",
              "   'tokens': [51148,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    551,\n",
              "    307,\n",
              "    516,\n",
              "    281,\n",
              "    1319,\n",
              "    13,\n",
              "    509,\n",
              "    486,\n",
              "    312,\n",
              "    1075,\n",
              "    281,\n",
              "    483,\n",
              "    613,\n",
              "    3685,\n",
              "    295,\n",
              "    721,\n",
              "    293,\n",
              "    51356],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1886990330800289,\n",
              "   'compression_ratio': 1.828793774319066,\n",
              "   'no_speech_prob': 0.07368136197328568},\n",
              "  {'id': 182,\n",
              "   'seek': 106592,\n",
              "   'start': 1085.76,\n",
              "   'end': 1090.8000000000002,\n",
              "   'text': ' people who want to hack and do things will have access to these things in without, you know,',\n",
              "   'tokens': [51356,\n",
              "    561,\n",
              "    567,\n",
              "    528,\n",
              "    281,\n",
              "    10339,\n",
              "    293,\n",
              "    360,\n",
              "    721,\n",
              "    486,\n",
              "    362,\n",
              "    2105,\n",
              "    281,\n",
              "    613,\n",
              "    721,\n",
              "    294,\n",
              "    1553,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    51608],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1886990330800289,\n",
              "   'compression_ratio': 1.828793774319066,\n",
              "   'no_speech_prob': 0.07368136197328568},\n",
              "  {'id': 183,\n",
              "   'seek': 109080,\n",
              "   'start': 1091.44,\n",
              "   'end': 1094.8,\n",
              "   'text': ' having to write a, you know, a way to check.',\n",
              "   'tokens': [50396,\n",
              "    1419,\n",
              "    281,\n",
              "    2464,\n",
              "    257,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    257,\n",
              "    636,\n",
              "    281,\n",
              "    1520,\n",
              "    13,\n",
              "    50564],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.22473996877670288,\n",
              "   'compression_ratio': 1.6714801444043321,\n",
              "   'no_speech_prob': 0.035630445927381516},\n",
              "  {'id': 184,\n",
              "   'seek': 109080,\n",
              "   'start': 1094.8,\n",
              "   'end': 1099.52,\n",
              "   'text': ' Vivek is also a semi-conductive guy before he went into other. So I would take his predictions',\n",
              "   'tokens': [50564,\n",
              "    44288,\n",
              "    74,\n",
              "    307,\n",
              "    611,\n",
              "    257,\n",
              "    12909,\n",
              "    12,\n",
              "    38150,\n",
              "    488,\n",
              "    2146,\n",
              "    949,\n",
              "    415,\n",
              "    1437,\n",
              "    666,\n",
              "    661,\n",
              "    13,\n",
              "    407,\n",
              "    286,\n",
              "    576,\n",
              "    747,\n",
              "    702,\n",
              "    21264,\n",
              "    50800],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.22473996877670288,\n",
              "   'compression_ratio': 1.6714801444043321,\n",
              "   'no_speech_prob': 0.035630445927381516},\n",
              "  {'id': 185,\n",
              "   'seek': 109080,\n",
              "   'start': 1099.52,\n",
              "   'end': 1102.56,\n",
              "   'text': \" very seriously. So I don't know what I, I'm going to sell my immediate stock.\",\n",
              "   'tokens': [50800,\n",
              "    588,\n",
              "    6638,\n",
              "    13,\n",
              "    407,\n",
              "    286,\n",
              "    500,\n",
              "    380,\n",
              "    458,\n",
              "    437,\n",
              "    286,\n",
              "    11,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    3607,\n",
              "    452,\n",
              "    11629,\n",
              "    4127,\n",
              "    13,\n",
              "    50952],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.22473996877670288,\n",
              "   'compression_ratio': 1.6714801444043321,\n",
              "   'no_speech_prob': 0.035630445927381516},\n",
              "  {'id': 186,\n",
              "   'seek': 109080,\n",
              "   'start': 1103.76,\n",
              "   'end': 1109.2,\n",
              "   'text': \" I would not do that, but that's not what I said. I want to blame you for this. I could go that.\",\n",
              "   'tokens': [51012,\n",
              "    286,\n",
              "    576,\n",
              "    406,\n",
              "    360,\n",
              "    300,\n",
              "    11,\n",
              "    457,\n",
              "    300,\n",
              "    311,\n",
              "    406,\n",
              "    437,\n",
              "    286,\n",
              "    848,\n",
              "    13,\n",
              "    286,\n",
              "    528,\n",
              "    281,\n",
              "    10127,\n",
              "    291,\n",
              "    337,\n",
              "    341,\n",
              "    13,\n",
              "    286,\n",
              "    727,\n",
              "    352,\n",
              "    300,\n",
              "    13,\n",
              "    51284],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.22473996877670288,\n",
              "   'compression_ratio': 1.6714801444043321,\n",
              "   'no_speech_prob': 0.035630445927381516},\n",
              "  {'id': 187,\n",
              "   'seek': 109080,\n",
              "   'start': 1110.96,\n",
              "   'end': 1115.2,\n",
              "   'text': ' But the third one is pretty strange. You know, companies are born, companies die,',\n",
              "   'tokens': [51372,\n",
              "    583,\n",
              "    264,\n",
              "    2636,\n",
              "    472,\n",
              "    307,\n",
              "    1238,\n",
              "    5861,\n",
              "    13,\n",
              "    509,\n",
              "    458,\n",
              "    11,\n",
              "    3431,\n",
              "    366,\n",
              "    4232,\n",
              "    11,\n",
              "    3431,\n",
              "    978,\n",
              "    11,\n",
              "    51584],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.22473996877670288,\n",
              "   'compression_ratio': 1.6714801444043321,\n",
              "   'no_speech_prob': 0.035630445927381516},\n",
              "  {'id': 188,\n",
              "   'seek': 109080,\n",
              "   'start': 1115.2,\n",
              "   'end': 1118.48,\n",
              "   'text': ' but you said some companies will suddenly die. What does that mean?',\n",
              "   'tokens': [51584,\n",
              "    457,\n",
              "    291,\n",
              "    848,\n",
              "    512,\n",
              "    3431,\n",
              "    486,\n",
              "    5800,\n",
              "    978,\n",
              "    13,\n",
              "    708,\n",
              "    775,\n",
              "    300,\n",
              "    914,\n",
              "    30,\n",
              "    51748],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.22473996877670288,\n",
              "   'compression_ratio': 1.6714801444043321,\n",
              "   'no_speech_prob': 0.035630445927381516},\n",
              "  {'id': 189,\n",
              "   'seek': 111848,\n",
              "   'start': 1119.3600000000001,\n",
              "   'end': 1125.2,\n",
              "   'text': ' No, I think, see, I think the interesting thing is, and I think that it comes back to the',\n",
              "   'tokens': [50408,\n",
              "    883,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    11,\n",
              "    536,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    264,\n",
              "    1880,\n",
              "    551,\n",
              "    307,\n",
              "    11,\n",
              "    293,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    309,\n",
              "    1487,\n",
              "    646,\n",
              "    281,\n",
              "    264,\n",
              "    50700],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11223360756847346,\n",
              "   'compression_ratio': 1.784037558685446,\n",
              "   'no_speech_prob': 0.012969973497092724},\n",
              "  {'id': 190,\n",
              "   'seek': 111848,\n",
              "   'start': 1125.2,\n",
              "   'end': 1131.68,\n",
              "   'text': ' fundamental nature of AI. AI is a tool, right? And you have to use that. And you have to use that',\n",
              "   'tokens': [50700,\n",
              "    8088,\n",
              "    3687,\n",
              "    295,\n",
              "    7318,\n",
              "    13,\n",
              "    7318,\n",
              "    307,\n",
              "    257,\n",
              "    2290,\n",
              "    11,\n",
              "    558,\n",
              "    30,\n",
              "    400,\n",
              "    291,\n",
              "    362,\n",
              "    281,\n",
              "    764,\n",
              "    300,\n",
              "    13,\n",
              "    400,\n",
              "    291,\n",
              "    362,\n",
              "    281,\n",
              "    764,\n",
              "    300,\n",
              "    51024],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11223360756847346,\n",
              "   'compression_ratio': 1.784037558685446,\n",
              "   'no_speech_prob': 0.012969973497092724},\n",
              "  {'id': 191,\n",
              "   'seek': 111848,\n",
              "   'start': 1131.68,\n",
              "   'end': 1137.68,\n",
              "   'text': \" within your business process, right? And how AI is being used, and so, and what's going to happen\",\n",
              "   'tokens': [51024,\n",
              "    1951,\n",
              "    428,\n",
              "    1606,\n",
              "    1399,\n",
              "    11,\n",
              "    558,\n",
              "    30,\n",
              "    400,\n",
              "    577,\n",
              "    7318,\n",
              "    307,\n",
              "    885,\n",
              "    1143,\n",
              "    11,\n",
              "    293,\n",
              "    370,\n",
              "    11,\n",
              "    293,\n",
              "    437,\n",
              "    311,\n",
              "    516,\n",
              "    281,\n",
              "    1051,\n",
              "    51324],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11223360756847346,\n",
              "   'compression_ratio': 1.784037558685446,\n",
              "   'no_speech_prob': 0.012969973497092724},\n",
              "  {'id': 192,\n",
              "   'seek': 111848,\n",
              "   'start': 1137.68,\n",
              "   'end': 1143.68,\n",
              "   'text': ' is that, I mean, I think this is true with, you know, when someone said in terms of, you know,',\n",
              "   'tokens': [51324,\n",
              "    307,\n",
              "    300,\n",
              "    11,\n",
              "    286,\n",
              "    914,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    341,\n",
              "    307,\n",
              "    2074,\n",
              "    365,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    562,\n",
              "    1580,\n",
              "    848,\n",
              "    294,\n",
              "    2115,\n",
              "    295,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    51624],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11223360756847346,\n",
              "   'compression_ratio': 1.784037558685446,\n",
              "   'no_speech_prob': 0.012969973497092724},\n",
              "  {'id': 193,\n",
              "   'seek': 114368,\n",
              "   'start': 1143.68,\n",
              "   'end': 1150.16,\n",
              "   'text': ' people, they said, that the people who leverage AI will be, will, will be more effective than those',\n",
              "   'tokens': [50364,\n",
              "    561,\n",
              "    11,\n",
              "    436,\n",
              "    848,\n",
              "    11,\n",
              "    300,\n",
              "    264,\n",
              "    561,\n",
              "    567,\n",
              "    13982,\n",
              "    7318,\n",
              "    486,\n",
              "    312,\n",
              "    11,\n",
              "    486,\n",
              "    11,\n",
              "    486,\n",
              "    312,\n",
              "    544,\n",
              "    4942,\n",
              "    813,\n",
              "    729,\n",
              "    50688],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14903435808547,\n",
              "   'compression_ratio': 1.8483412322274881,\n",
              "   'no_speech_prob': 0.08944111317396164},\n",
              "  {'id': 194,\n",
              "   'seek': 114368,\n",
              "   'start': 1150.16,\n",
              "   'end': 1156.8,\n",
              "   'text': \" who don't leverage AI. And that will, too, for organizations also. Organizations that leverage AI\",\n",
              "   'tokens': [50688,\n",
              "    567,\n",
              "    500,\n",
              "    380,\n",
              "    13982,\n",
              "    7318,\n",
              "    13,\n",
              "    400,\n",
              "    300,\n",
              "    486,\n",
              "    11,\n",
              "    886,\n",
              "    11,\n",
              "    337,\n",
              "    6150,\n",
              "    611,\n",
              "    13,\n",
              "    12538,\n",
              "    14455,\n",
              "    300,\n",
              "    13982,\n",
              "    7318,\n",
              "    51020],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14903435808547,\n",
              "   'compression_ratio': 1.8483412322274881,\n",
              "   'no_speech_prob': 0.08944111317396164},\n",
              "  {'id': 195,\n",
              "   'seek': 114368,\n",
              "   'start': 1156.8,\n",
              "   'end': 1162.4,\n",
              "   'text': \" in, fundamentally in their core business processes, will be more effective than those who don't,\",\n",
              "   'tokens': [51020,\n",
              "    294,\n",
              "    11,\n",
              "    17879,\n",
              "    294,\n",
              "    641,\n",
              "    4965,\n",
              "    1606,\n",
              "    7555,\n",
              "    11,\n",
              "    486,\n",
              "    312,\n",
              "    544,\n",
              "    4942,\n",
              "    813,\n",
              "    729,\n",
              "    567,\n",
              "    500,\n",
              "    380,\n",
              "    11,\n",
              "    51300],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14903435808547,\n",
              "   'compression_ratio': 1.8483412322274881,\n",
              "   'no_speech_prob': 0.08944111317396164},\n",
              "  {'id': 196,\n",
              "   'seek': 114368,\n",
              "   'start': 1162.4,\n",
              "   'end': 1168.16,\n",
              "   'text': \" right? And I think that's the thing. And you won't know the difference until one day it becomes\",\n",
              "   'tokens': [51300,\n",
              "    558,\n",
              "    30,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    311,\n",
              "    264,\n",
              "    551,\n",
              "    13,\n",
              "    400,\n",
              "    291,\n",
              "    1582,\n",
              "    380,\n",
              "    458,\n",
              "    264,\n",
              "    2649,\n",
              "    1826,\n",
              "    472,\n",
              "    786,\n",
              "    309,\n",
              "    3643,\n",
              "    51588],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14903435808547,\n",
              "   'compression_ratio': 1.8483412322274881,\n",
              "   'no_speech_prob': 0.08944111317396164},\n",
              "  {'id': 197,\n",
              "   'seek': 116816,\n",
              "   'start': 1168.3200000000002,\n",
              "   'end': 1173.6000000000001,\n",
              "   'text': \" too obvious, and it will be too late. And I think that's the reason why everybody needs to think\",\n",
              "   'tokens': [50372,\n",
              "    886,\n",
              "    6322,\n",
              "    11,\n",
              "    293,\n",
              "    309,\n",
              "    486,\n",
              "    312,\n",
              "    886,\n",
              "    3469,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    311,\n",
              "    264,\n",
              "    1778,\n",
              "    983,\n",
              "    2201,\n",
              "    2203,\n",
              "    281,\n",
              "    519,\n",
              "    50636],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1772367456457117,\n",
              "   'compression_ratio': 1.807511737089202,\n",
              "   'no_speech_prob': 0.04559626430273056},\n",
              "  {'id': 198,\n",
              "   'seek': 116816,\n",
              "   'start': 1173.6000000000001,\n",
              "   'end': 1180.0,\n",
              "   'text': ' about what it means for your business. Because you will, everything will be fine. Everything will',\n",
              "   'tokens': [50636,\n",
              "    466,\n",
              "    437,\n",
              "    309,\n",
              "    1355,\n",
              "    337,\n",
              "    428,\n",
              "    1606,\n",
              "    13,\n",
              "    1436,\n",
              "    291,\n",
              "    486,\n",
              "    11,\n",
              "    1203,\n",
              "    486,\n",
              "    312,\n",
              "    2489,\n",
              "    13,\n",
              "    5471,\n",
              "    486,\n",
              "    50956],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1772367456457117,\n",
              "   'compression_ratio': 1.807511737089202,\n",
              "   'no_speech_prob': 0.04559626430273056},\n",
              "  {'id': 199,\n",
              "   'seek': 116816,\n",
              "   'start': 1180.0,\n",
              "   'end': 1185.52,\n",
              "   'text': ' be fine. And one day, somebody in your, either, either, either your competitor, your space,',\n",
              "   'tokens': [50956,\n",
              "    312,\n",
              "    2489,\n",
              "    13,\n",
              "    400,\n",
              "    472,\n",
              "    786,\n",
              "    11,\n",
              "    2618,\n",
              "    294,\n",
              "    428,\n",
              "    11,\n",
              "    2139,\n",
              "    11,\n",
              "    2139,\n",
              "    11,\n",
              "    2139,\n",
              "    428,\n",
              "    27266,\n",
              "    11,\n",
              "    428,\n",
              "    1901,\n",
              "    11,\n",
              "    51232],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1772367456457117,\n",
              "   'compression_ratio': 1.807511737089202,\n",
              "   'no_speech_prob': 0.04559626430273056},\n",
              "  {'id': 200,\n",
              "   'seek': 116816,\n",
              "   'start': 1185.52,\n",
              "   'end': 1191.0400000000002,\n",
              "   'text': ' or somebody brand new coming into your space will be reimagining your business process completely.',\n",
              "   'tokens': [51232,\n",
              "    420,\n",
              "    2618,\n",
              "    3360,\n",
              "    777,\n",
              "    1348,\n",
              "    666,\n",
              "    428,\n",
              "    1901,\n",
              "    486,\n",
              "    312,\n",
              "    319,\n",
              "    25228,\n",
              "    1760,\n",
              "    428,\n",
              "    1606,\n",
              "    1399,\n",
              "    2584,\n",
              "    13,\n",
              "    51508],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1772367456457117,\n",
              "   'compression_ratio': 1.807511737089202,\n",
              "   'no_speech_prob': 0.04559626430273056},\n",
              "  {'id': 201,\n",
              "   'seek': 119104,\n",
              "   'start': 1191.04,\n",
              "   'end': 1197.84,\n",
              "   'text': \" And at that stage, you will find that it's, you know, it's a very big, very tall, you know,\",\n",
              "   'tokens': [50364,\n",
              "    400,\n",
              "    412,\n",
              "    300,\n",
              "    3233,\n",
              "    11,\n",
              "    291,\n",
              "    486,\n",
              "    915,\n",
              "    300,\n",
              "    309,\n",
              "    311,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    309,\n",
              "    311,\n",
              "    257,\n",
              "    588,\n",
              "    955,\n",
              "    11,\n",
              "    588,\n",
              "    6764,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    50704],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1627689135276665,\n",
              "   'compression_ratio': 1.8365758754863812,\n",
              "   'no_speech_prob': 0.03116299770772457},\n",
              "  {'id': 202,\n",
              "   'seek': 119104,\n",
              "   'start': 1197.84,\n",
              "   'end': 1203.76,\n",
              "   'text': \" mountain to climb. And that's why I think it's important for both people and entities to think\",\n",
              "   'tokens': [50704,\n",
              "    6937,\n",
              "    281,\n",
              "    10724,\n",
              "    13,\n",
              "    400,\n",
              "    300,\n",
              "    311,\n",
              "    983,\n",
              "    286,\n",
              "    519,\n",
              "    309,\n",
              "    311,\n",
              "    1021,\n",
              "    337,\n",
              "    1293,\n",
              "    561,\n",
              "    293,\n",
              "    16667,\n",
              "    281,\n",
              "    519,\n",
              "    51000],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1627689135276665,\n",
              "   'compression_ratio': 1.8365758754863812,\n",
              "   'no_speech_prob': 0.03116299770772457},\n",
              "  {'id': 203,\n",
              "   'seek': 119104,\n",
              "   'start': 1203.76,\n",
              "   'end': 1208.8,\n",
              "   'text': ' about how they will, you know, they will upgrade themselves or they will modify their business',\n",
              "   'tokens': [51000,\n",
              "    466,\n",
              "    577,\n",
              "    436,\n",
              "    486,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    436,\n",
              "    486,\n",
              "    11484,\n",
              "    2969,\n",
              "    420,\n",
              "    436,\n",
              "    486,\n",
              "    16927,\n",
              "    641,\n",
              "    1606,\n",
              "    51252],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1627689135276665,\n",
              "   'compression_ratio': 1.8365758754863812,\n",
              "   'no_speech_prob': 0.03116299770772457},\n",
              "  {'id': 204,\n",
              "   'seek': 119104,\n",
              "   'start': 1208.8,\n",
              "   'end': 1212.96,\n",
              "   'text': \" processes to an, you know, to a certain extent. That's a very nuanced answer. And everybody here\",\n",
              "   'tokens': [51252,\n",
              "    7555,\n",
              "    281,\n",
              "    364,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    281,\n",
              "    257,\n",
              "    1629,\n",
              "    8396,\n",
              "    13,\n",
              "    663,\n",
              "    311,\n",
              "    257,\n",
              "    588,\n",
              "    45115,\n",
              "    1867,\n",
              "    13,\n",
              "    400,\n",
              "    2201,\n",
              "    510,\n",
              "    51460],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1627689135276665,\n",
              "   'compression_ratio': 1.8365758754863812,\n",
              "   'no_speech_prob': 0.03116299770772457},\n",
              "  {'id': 205,\n",
              "   'seek': 119104,\n",
              "   'start': 1212.96,\n",
              "   'end': 1217.44,\n",
              "   'text': \" who's running a business should really think about it because life will be the same. And then\",\n",
              "   'tokens': [51460,\n",
              "    567,\n",
              "    311,\n",
              "    2614,\n",
              "    257,\n",
              "    1606,\n",
              "    820,\n",
              "    534,\n",
              "    519,\n",
              "    466,\n",
              "    309,\n",
              "    570,\n",
              "    993,\n",
              "    486,\n",
              "    312,\n",
              "    264,\n",
              "    912,\n",
              "    13,\n",
              "    400,\n",
              "    550,\n",
              "    51684],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1627689135276665,\n",
              "   'compression_ratio': 1.8365758754863812,\n",
              "   'no_speech_prob': 0.03116299770772457},\n",
              "  {'id': 206,\n",
              "   'seek': 121744,\n",
              "   'start': 1217.44,\n",
              "   'end': 1222.48,\n",
              "   'text': ' suddenly, suddenly something will, you know, there will be a step change. We make a few more',\n",
              "   'tokens': [50364,\n",
              "    5800,\n",
              "    11,\n",
              "    5800,\n",
              "    746,\n",
              "    486,\n",
              "    11,\n",
              "    291,\n",
              "    458,\n",
              "    11,\n",
              "    456,\n",
              "    486,\n",
              "    312,\n",
              "    257,\n",
              "    1823,\n",
              "    1319,\n",
              "    13,\n",
              "    492,\n",
              "    652,\n",
              "    257,\n",
              "    1326,\n",
              "    544,\n",
              "    50616],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.189587170315772,\n",
              "   'compression_ratio': 1.5642201834862386,\n",
              "   'no_speech_prob': 0.011679133400321007},\n",
              "  {'id': 207,\n",
              "   'seek': 121744,\n",
              "   'start': 1222.48,\n",
              "   'end': 1226.96,\n",
              "   'text': \" questions, but I'm sure the audience has a lot of questions for you. So how are we doing on time?\",\n",
              "   'tokens': [50616,\n",
              "    1651,\n",
              "    11,\n",
              "    457,\n",
              "    286,\n",
              "    478,\n",
              "    988,\n",
              "    264,\n",
              "    4034,\n",
              "    575,\n",
              "    257,\n",
              "    688,\n",
              "    295,\n",
              "    1651,\n",
              "    337,\n",
              "    291,\n",
              "    13,\n",
              "    407,\n",
              "    577,\n",
              "    366,\n",
              "    321,\n",
              "    884,\n",
              "    322,\n",
              "    565,\n",
              "    30,\n",
              "    50840],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.189587170315772,\n",
              "   'compression_ratio': 1.5642201834862386,\n",
              "   'no_speech_prob': 0.011679133400321007},\n",
              "  {'id': 208,\n",
              "   'seek': 121744,\n",
              "   'start': 1228.0800000000002,\n",
              "   'end': 1234.96,\n",
              "   'text': ' Okay. So does, okay, a lot of questions, so love to, is there a mic that we can pass on?',\n",
              "   'tokens': [50896,\n",
              "    1033,\n",
              "    13,\n",
              "    407,\n",
              "    775,\n",
              "    11,\n",
              "    1392,\n",
              "    11,\n",
              "    257,\n",
              "    688,\n",
              "    295,\n",
              "    1651,\n",
              "    11,\n",
              "    370,\n",
              "    959,\n",
              "    281,\n",
              "    11,\n",
              "    307,\n",
              "    456,\n",
              "    257,\n",
              "    3123,\n",
              "    300,\n",
              "    321,\n",
              "    393,\n",
              "    1320,\n",
              "    322,\n",
              "    30,\n",
              "    51240],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.189587170315772,\n",
              "   'compression_ratio': 1.5642201834862386,\n",
              "   'no_speech_prob': 0.011679133400321007},\n",
              "  {'id': 209,\n",
              "   'seek': 121744,\n",
              "   'start': 1240.48,\n",
              "   'end': 1245.92,\n",
              "   'text': ' Thank you. My name is Kartik. I work for IT Service Industry.',\n",
              "   'tokens': [51516,\n",
              "    1044,\n",
              "    291,\n",
              "    13,\n",
              "    1222,\n",
              "    1315,\n",
              "    307,\n",
              "    27365,\n",
              "    1035,\n",
              "    13,\n",
              "    286,\n",
              "    589,\n",
              "    337,\n",
              "    6783,\n",
              "    9561,\n",
              "    38178,\n",
              "    13,\n",
              "    51788],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.189587170315772,\n",
              "   'compression_ratio': 1.5642201834862386,\n",
              "   'no_speech_prob': 0.011679133400321007},\n",
              "  {'id': 210,\n",
              "   'seek': 124592,\n",
              "   'start': 1246.72,\n",
              "   'end': 1254.88,\n",
              "   'text': \" So you're saying that you're working on LLM, sorry, it's a fine tuned LLM on top of LAMA.\",\n",
              "   'tokens': [50404,\n",
              "    407,\n",
              "    291,\n",
              "    434,\n",
              "    1566,\n",
              "    300,\n",
              "    291,\n",
              "    434,\n",
              "    1364,\n",
              "    322,\n",
              "    441,\n",
              "    43,\n",
              "    44,\n",
              "    11,\n",
              "    2597,\n",
              "    11,\n",
              "    309,\n",
              "    311,\n",
              "    257,\n",
              "    2489,\n",
              "    10870,\n",
              "    441,\n",
              "    43,\n",
              "    44,\n",
              "    322,\n",
              "    1192,\n",
              "    295,\n",
              "    441,\n",
              "    38136,\n",
              "    13,\n",
              "    50812],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16841363906860352,\n",
              "   'compression_ratio': 1.6260504201680672,\n",
              "   'no_speech_prob': 0.037962913513183594},\n",
              "  {'id': 211,\n",
              "   'seek': 124592,\n",
              "   'start': 1254.88,\n",
              "   'end': 1259.92,\n",
              "   'text': \" My basic question, fundamental question is we don't have a foundation model for India. Most of the\",\n",
              "   'tokens': [50812,\n",
              "    1222,\n",
              "    3875,\n",
              "    1168,\n",
              "    11,\n",
              "    8088,\n",
              "    1168,\n",
              "    307,\n",
              "    321,\n",
              "    500,\n",
              "    380,\n",
              "    362,\n",
              "    257,\n",
              "    7030,\n",
              "    2316,\n",
              "    337,\n",
              "    5282,\n",
              "    13,\n",
              "    4534,\n",
              "    295,\n",
              "    264,\n",
              "    51064],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16841363906860352,\n",
              "   'compression_ratio': 1.6260504201680672,\n",
              "   'no_speech_prob': 0.037962913513183594},\n",
              "  {'id': 212,\n",
              "   'seek': 124592,\n",
              "   'start': 1259.92,\n",
              "   'end': 1267.2,\n",
              "   'text': ' models are basically using English or those kind of things. So for example, Andrew was talking about',\n",
              "   'tokens': [51064,\n",
              "    5245,\n",
              "    366,\n",
              "    1936,\n",
              "    1228,\n",
              "    3669,\n",
              "    420,\n",
              "    729,\n",
              "    733,\n",
              "    295,\n",
              "    721,\n",
              "    13,\n",
              "    407,\n",
              "    337,\n",
              "    1365,\n",
              "    11,\n",
              "    10110,\n",
              "    390,\n",
              "    1417,\n",
              "    466,\n",
              "    51428],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16841363906860352,\n",
              "   'compression_ratio': 1.6260504201680672,\n",
              "   'no_speech_prob': 0.037962913513183594},\n",
              "  {'id': 213,\n",
              "   'seek': 124592,\n",
              "   'start': 1267.2,\n",
              "   'end': 1273.8400000000001,\n",
              "   'text': ' the tokenizers and things like that. So are you working on anything like that? Or you want to use',\n",
              "   'tokens': [51428,\n",
              "    264,\n",
              "    14862,\n",
              "    22525,\n",
              "    293,\n",
              "    721,\n",
              "    411,\n",
              "    300,\n",
              "    13,\n",
              "    407,\n",
              "    366,\n",
              "    291,\n",
              "    1364,\n",
              "    322,\n",
              "    1340,\n",
              "    411,\n",
              "    300,\n",
              "    30,\n",
              "    1610,\n",
              "    291,\n",
              "    528,\n",
              "    281,\n",
              "    764,\n",
              "    51760],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16841363906860352,\n",
              "   'compression_ratio': 1.6260504201680672,\n",
              "   'no_speech_prob': 0.037962913513183594},\n",
              "  {'id': 214,\n",
              "   'seek': 127384,\n",
              "   'start': 1273.9199999999998,\n",
              "   'end': 1279.04,\n",
              "   'text': ' mostly the existing models and run on top of it? You asked a good question. You asked the',\n",
              "   'tokens': [50368,\n",
              "    5240,\n",
              "    264,\n",
              "    6741,\n",
              "    5245,\n",
              "    293,\n",
              "    1190,\n",
              "    322,\n",
              "    1192,\n",
              "    295,\n",
              "    309,\n",
              "    30,\n",
              "    509,\n",
              "    2351,\n",
              "    257,\n",
              "    665,\n",
              "    1168,\n",
              "    13,\n",
              "    509,\n",
              "    2351,\n",
              "    264,\n",
              "    50624],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19902507910567724,\n",
              "   'compression_ratio': 1.685589519650655,\n",
              "   'no_speech_prob': 0.011015694588422775},\n",
              "  {'id': 215,\n",
              "   'seek': 127384,\n",
              "   'start': 1279.04,\n",
              "   'end': 1285.12,\n",
              "   'text': ' cherry question for himself. Well, I think the interesting thing is that if you look at, and then we',\n",
              "   'tokens': [50624,\n",
              "    20164,\n",
              "    1168,\n",
              "    337,\n",
              "    3647,\n",
              "    13,\n",
              "    1042,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    264,\n",
              "    1880,\n",
              "    551,\n",
              "    307,\n",
              "    300,\n",
              "    498,\n",
              "    291,\n",
              "    574,\n",
              "    412,\n",
              "    11,\n",
              "    293,\n",
              "    550,\n",
              "    321,\n",
              "    50928],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19902507910567724,\n",
              "   'compression_ratio': 1.685589519650655,\n",
              "   'no_speech_prob': 0.011015694588422775},\n",
              "  {'id': 216,\n",
              "   'seek': 127384,\n",
              "   'start': 1285.12,\n",
              "   'end': 1290.1599999999999,\n",
              "   'text': \" have actually a blog on this on our website, I think one of the things that we've actually built\",\n",
              "   'tokens': [50928,\n",
              "    362,\n",
              "    767,\n",
              "    257,\n",
              "    6968,\n",
              "    322,\n",
              "    341,\n",
              "    322,\n",
              "    527,\n",
              "    3144,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    472,\n",
              "    295,\n",
              "    264,\n",
              "    721,\n",
              "    300,\n",
              "    321,\n",
              "    600,\n",
              "    767,\n",
              "    3094,\n",
              "    51180],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19902507910567724,\n",
              "   'compression_ratio': 1.685589519650655,\n",
              "   'no_speech_prob': 0.011015694588422775},\n",
              "  {'id': 217,\n",
              "   'seek': 127384,\n",
              "   'start': 1291.04,\n",
              "   'end': 1297.12,\n",
              "   'text': ' a customized tokenizer, which actually fundamentally changes the cost of some of these generations',\n",
              "   'tokens': [51224,\n",
              "    257,\n",
              "    30581,\n",
              "    14862,\n",
              "    6545,\n",
              "    11,\n",
              "    597,\n",
              "    767,\n",
              "    17879,\n",
              "    2962,\n",
              "    264,\n",
              "    2063,\n",
              "    295,\n",
              "    512,\n",
              "    295,\n",
              "    613,\n",
              "    10593,\n",
              "    51528],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19902507910567724,\n",
              "   'compression_ratio': 1.685589519650655,\n",
              "   'no_speech_prob': 0.011015694588422775},\n",
              "  {'id': 218,\n",
              "   'seek': 129712,\n",
              "   'start': 1297.12,\n",
              "   'end': 1303.6799999999998,\n",
              "   'text': \" in Indian languages. And I think that we're not just fine tuning. We're actually, we are leveraging\",\n",
              "   'tokens': [50364,\n",
              "    294,\n",
              "    6427,\n",
              "    8650,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    321,\n",
              "    434,\n",
              "    406,\n",
              "    445,\n",
              "    2489,\n",
              "    15164,\n",
              "    13,\n",
              "    492,\n",
              "    434,\n",
              "    767,\n",
              "    11,\n",
              "    321,\n",
              "    366,\n",
              "    32666,\n",
              "    50692],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.2080859112483199,\n",
              "   'compression_ratio': 1.6567796610169492,\n",
              "   'no_speech_prob': 0.0923042818903923},\n",
              "  {'id': 219,\n",
              "   'seek': 129712,\n",
              "   'start': 1303.6799999999998,\n",
              "   'end': 1308.9599999999998,\n",
              "   'text': \" the existing retaining, but we are doing what's known as continual free training, but having said\",\n",
              "   'tokens': [50692,\n",
              "    264,\n",
              "    6741,\n",
              "    34936,\n",
              "    11,\n",
              "    457,\n",
              "    321,\n",
              "    366,\n",
              "    884,\n",
              "    437,\n",
              "    311,\n",
              "    2570,\n",
              "    382,\n",
              "    1421,\n",
              "    901,\n",
              "    1737,\n",
              "    3097,\n",
              "    11,\n",
              "    457,\n",
              "    1419,\n",
              "    848,\n",
              "    50956],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.2080859112483199,\n",
              "   'compression_ratio': 1.6567796610169492,\n",
              "   'no_speech_prob': 0.0923042818903923},\n",
              "  {'id': 220,\n",
              "   'seek': 129712,\n",
              "   'start': 1308.9599999999998,\n",
              "   'end': 1315.84,\n",
              "   'text': ' that, I think that when we have to figure out where is the data to train an extremely large model',\n",
              "   'tokens': [50956,\n",
              "    300,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    562,\n",
              "    321,\n",
              "    362,\n",
              "    281,\n",
              "    2573,\n",
              "    484,\n",
              "    689,\n",
              "    307,\n",
              "    264,\n",
              "    1412,\n",
              "    281,\n",
              "    3847,\n",
              "    364,\n",
              "    4664,\n",
              "    2416,\n",
              "    2316,\n",
              "    51300],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.2080859112483199,\n",
              "   'compression_ratio': 1.6567796610169492,\n",
              "   'no_speech_prob': 0.0923042818903923},\n",
              "  {'id': 221,\n",
              "   'seek': 129712,\n",
              "   'start': 1315.84,\n",
              "   'end': 1321.1999999999998,\n",
              "   'text': ' from scratch and some of those things are things which will happen over time. But I think that,',\n",
              "   'tokens': [51300,\n",
              "    490,\n",
              "    8459,\n",
              "    293,\n",
              "    512,\n",
              "    295,\n",
              "    729,\n",
              "    721,\n",
              "    366,\n",
              "    721,\n",
              "    597,\n",
              "    486,\n",
              "    1051,\n",
              "    670,\n",
              "    565,\n",
              "    13,\n",
              "    583,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    11,\n",
              "    51568],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.2080859112483199,\n",
              "   'compression_ratio': 1.6567796610169492,\n",
              "   'no_speech_prob': 0.0923042818903923},\n",
              "  {'id': 222,\n",
              "   'seek': 132120,\n",
              "   'start': 1321.44,\n",
              "   'end': 1328.16,\n",
              "   'text': ' I think that yes, I think that we will be doing various kinds of things, but the interesting thing is',\n",
              "   'tokens': [50376,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    2086,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    321,\n",
              "    486,\n",
              "    312,\n",
              "    884,\n",
              "    3683,\n",
              "    3685,\n",
              "    295,\n",
              "    721,\n",
              "    11,\n",
              "    457,\n",
              "    264,\n",
              "    1880,\n",
              "    551,\n",
              "    307,\n",
              "    50712],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.26216991364009795,\n",
              "   'compression_ratio': 1.6375838926174497,\n",
              "   'no_speech_prob': 0.019846448674798012},\n",
              "  {'id': 223,\n",
              "   'seek': 132120,\n",
              "   'start': 1328.16,\n",
              "   'end': 1334.24,\n",
              "   'text': ' that if I want to change the accessibility problem with an existing open source model, how do I do that?',\n",
              "   'tokens': [50712,\n",
              "    300,\n",
              "    498,\n",
              "    286,\n",
              "    528,\n",
              "    281,\n",
              "    1319,\n",
              "    264,\n",
              "    15002,\n",
              "    1154,\n",
              "    365,\n",
              "    364,\n",
              "    6741,\n",
              "    1269,\n",
              "    4009,\n",
              "    2316,\n",
              "    11,\n",
              "    577,\n",
              "    360,\n",
              "    286,\n",
              "    360,\n",
              "    300,\n",
              "    30,\n",
              "    51016],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.26216991364009795,\n",
              "   'compression_ratio': 1.6375838926174497,\n",
              "   'no_speech_prob': 0.019846448674798012},\n",
              "  {'id': 224,\n",
              "   'seek': 132120,\n",
              "   'start': 1334.24,\n",
              "   'end': 1339.28,\n",
              "   'text': \" And that's the problem that we have that we think we have solved and is going to be the heart of\",\n",
              "   'tokens': [51016,\n",
              "    400,\n",
              "    300,\n",
              "    311,\n",
              "    264,\n",
              "    1154,\n",
              "    300,\n",
              "    321,\n",
              "    362,\n",
              "    300,\n",
              "    321,\n",
              "    519,\n",
              "    321,\n",
              "    362,\n",
              "    13041,\n",
              "    293,\n",
              "    307,\n",
              "    516,\n",
              "    281,\n",
              "    312,\n",
              "    264,\n",
              "    1917,\n",
              "    295,\n",
              "    51268],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.26216991364009795,\n",
              "   'compression_ratio': 1.6375838926174497,\n",
              "   'no_speech_prob': 0.019846448674798012},\n",
              "  {'id': 225,\n",
              "   'seek': 132120,\n",
              "   'start': 1339.28,\n",
              "   'end': 1343.44,\n",
              "   'text': \" this open RTC series. It's extremely well explained in the blog even I could understand it so.\",\n",
              "   'tokens': [51268,\n",
              "    341,\n",
              "    1269,\n",
              "    497,\n",
              "    18238,\n",
              "    2638,\n",
              "    13,\n",
              "    467,\n",
              "    311,\n",
              "    4664,\n",
              "    731,\n",
              "    8825,\n",
              "    294,\n",
              "    264,\n",
              "    6968,\n",
              "    754,\n",
              "    286,\n",
              "    727,\n",
              "    1223,\n",
              "    309,\n",
              "    370,\n",
              "    13,\n",
              "    51476],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.26216991364009795,\n",
              "   'compression_ratio': 1.6375838926174497,\n",
              "   'no_speech_prob': 0.019846448674798012},\n",
              "  {'id': 226,\n",
              "   'seek': 132120,\n",
              "   'start': 1345.1200000000001,\n",
              "   'end': 1351.04,\n",
              "   'text': \" Hi, I'm Prishant. I work for a FITEC company. My question is like, unlike China, we never\",\n",
              "   'tokens': [51560,\n",
              "    2421,\n",
              "    11,\n",
              "    286,\n",
              "    478,\n",
              "    2114,\n",
              "    742,\n",
              "    394,\n",
              "    13,\n",
              "    286,\n",
              "    589,\n",
              "    337,\n",
              "    257,\n",
              "    479,\n",
              "    3927,\n",
              "    8140,\n",
              "    2237,\n",
              "    13,\n",
              "    1222,\n",
              "    1168,\n",
              "    307,\n",
              "    411,\n",
              "    11,\n",
              "    8343,\n",
              "    3533,\n",
              "    11,\n",
              "    321,\n",
              "    1128,\n",
              "    51856],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.26216991364009795,\n",
              "   'compression_ratio': 1.6375838926174497,\n",
              "   'no_speech_prob': 0.019846448674798012},\n",
              "  {'id': 227,\n",
              "   'seek': 135104,\n",
              "   'start': 1351.04,\n",
              "   'end': 1358.56,\n",
              "   'text': ' had a consumer facing application coming out from India and in Web 1, Web 2, crypto and all.',\n",
              "   'tokens': [50364,\n",
              "    632,\n",
              "    257,\n",
              "    9711,\n",
              "    7170,\n",
              "    3861,\n",
              "    1348,\n",
              "    484,\n",
              "    490,\n",
              "    5282,\n",
              "    293,\n",
              "    294,\n",
              "    9573,\n",
              "    502,\n",
              "    11,\n",
              "    9573,\n",
              "    568,\n",
              "    11,\n",
              "    17240,\n",
              "    293,\n",
              "    439,\n",
              "    13,\n",
              "    50740],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.24598231785733934,\n",
              "   'compression_ratio': 1.4946808510638299,\n",
              "   'no_speech_prob': 0.019374528899788857},\n",
              "  {'id': 228,\n",
              "   'seek': 135104,\n",
              "   'start': 1359.36,\n",
              "   'end': 1368.48,\n",
              "   'text': ' Why do you think it will be different this time in like AI? Because will the BPI and other things',\n",
              "   'tokens': [50780,\n",
              "    1545,\n",
              "    360,\n",
              "    291,\n",
              "    519,\n",
              "    309,\n",
              "    486,\n",
              "    312,\n",
              "    819,\n",
              "    341,\n",
              "    565,\n",
              "    294,\n",
              "    411,\n",
              "    7318,\n",
              "    30,\n",
              "    1436,\n",
              "    486,\n",
              "    264,\n",
              "    363,\n",
              "    31701,\n",
              "    293,\n",
              "    661,\n",
              "    721,\n",
              "    51236],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.24598231785733934,\n",
              "   'compression_ratio': 1.4946808510638299,\n",
              "   'no_speech_prob': 0.019374528899788857},\n",
              "  {'id': 229,\n",
              "   'seek': 135104,\n",
              "   'start': 1368.48,\n",
              "   'end': 1374.96,\n",
              "   'text': ' will solve the same purpose, but the great five world did in China or do you think like in',\n",
              "   'tokens': [51236,\n",
              "    486,\n",
              "    5039,\n",
              "    264,\n",
              "    912,\n",
              "    4334,\n",
              "    11,\n",
              "    457,\n",
              "    264,\n",
              "    869,\n",
              "    1732,\n",
              "    1002,\n",
              "    630,\n",
              "    294,\n",
              "    3533,\n",
              "    420,\n",
              "    360,\n",
              "    291,\n",
              "    519,\n",
              "    411,\n",
              "    294,\n",
              "    51560],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.24598231785733934,\n",
              "   'compression_ratio': 1.4946808510638299,\n",
              "   'no_speech_prob': 0.019374528899788857},\n",
              "  {'id': 230,\n",
              "   'seek': 137496,\n",
              "   'start': 1375.52,\n",
              "   'end': 1383.28,\n",
              "   'text': ' because AI is a strategic sector, no outside country can work in NASA projects maybe',\n",
              "   'tokens': [50392,\n",
              "    570,\n",
              "    7318,\n",
              "    307,\n",
              "    257,\n",
              "    10924,\n",
              "    6977,\n",
              "    11,\n",
              "    572,\n",
              "    2380,\n",
              "    1941,\n",
              "    393,\n",
              "    589,\n",
              "    294,\n",
              "    12077,\n",
              "    4455,\n",
              "    1310,\n",
              "    50780],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.2800588607788086,\n",
              "   'compression_ratio': 1.488888888888889,\n",
              "   'no_speech_prob': 0.0471775084733963},\n",
              "  {'id': 231,\n",
              "   'seek': 137496,\n",
              "   'start': 1383.28,\n",
              "   'end': 1388.56,\n",
              "   'text': ' or government content will go to them. What is at least the mode here for an Indian company?',\n",
              "   'tokens': [50780,\n",
              "    420,\n",
              "    2463,\n",
              "    2701,\n",
              "    486,\n",
              "    352,\n",
              "    281,\n",
              "    552,\n",
              "    13,\n",
              "    708,\n",
              "    307,\n",
              "    412,\n",
              "    1935,\n",
              "    264,\n",
              "    4391,\n",
              "    510,\n",
              "    337,\n",
              "    364,\n",
              "    6427,\n",
              "    2237,\n",
              "    30,\n",
              "    51044],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.2800588607788086,\n",
              "   'compression_ratio': 1.488888888888889,\n",
              "   'no_speech_prob': 0.0471775084733963},\n",
              "  {'id': 232,\n",
              "   'seek': 137496,\n",
              "   'start': 1391.1200000000001,\n",
              "   'end': 1398.24,\n",
              "   'text': \" So I don't I think I think the the question is I don't know the answer to these questions,\",\n",
              "   'tokens': [51172,\n",
              "    407,\n",
              "    286,\n",
              "    500,\n",
              "    380,\n",
              "    286,\n",
              "    519,\n",
              "    286,\n",
              "    519,\n",
              "    264,\n",
              "    264,\n",
              "    1168,\n",
              "    307,\n",
              "    286,\n",
              "    500,\n",
              "    380,\n",
              "    458,\n",
              "    264,\n",
              "    1867,\n",
              "    281,\n",
              "    613,\n",
              "    1651,\n",
              "    11,\n",
              "    51528],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.2800588607788086,\n",
              "   'compression_ratio': 1.488888888888889,\n",
              "   'no_speech_prob': 0.0471775084733963},\n",
              "  {'id': 233,\n",
              "   'seek': 139824,\n",
              "   'start': 1398.24,\n",
              "   'end': 1405.1200000000001,\n",
              "   'text': \" right? I mean, I think that it's difficult to predict, but I do believe, and as I'm repeating,\",\n",
              "   'tokens': [50364,\n",
              "    558,\n",
              "    30,\n",
              "    286,\n",
              "    914,\n",
              "    11,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    309,\n",
              "    311,\n",
              "    2252,\n",
              "    281,\n",
              "    6069,\n",
              "    11,\n",
              "    457,\n",
              "    286,\n",
              "    360,\n",
              "    1697,\n",
              "    11,\n",
              "    293,\n",
              "    382,\n",
              "    286,\n",
              "    478,\n",
              "    18617,\n",
              "    11,\n",
              "    50708],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18543222427368164,\n",
              "   'compression_ratio': 1.6069868995633187,\n",
              "   'no_speech_prob': 0.07256335765123367},\n",
              "  {'id': 234,\n",
              "   'seek': 139824,\n",
              "   'start': 1405.1200000000001,\n",
              "   'end': 1410.64,\n",
              "   'text': ' that the combinatorial effect of being using Genai at a large scale in addition,',\n",
              "   'tokens': [50708,\n",
              "    300,\n",
              "    264,\n",
              "    2512,\n",
              "    31927,\n",
              "    831,\n",
              "    1802,\n",
              "    295,\n",
              "    885,\n",
              "    1228,\n",
              "    3632,\n",
              "    1301,\n",
              "    412,\n",
              "    257,\n",
              "    2416,\n",
              "    4373,\n",
              "    294,\n",
              "    4500,\n",
              "    11,\n",
              "    50984],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18543222427368164,\n",
              "   'compression_ratio': 1.6069868995633187,\n",
              "   'no_speech_prob': 0.07256335765123367},\n",
              "  {'id': 235,\n",
              "   'seek': 139824,\n",
              "   'start': 1410.64,\n",
              "   'end': 1416.08,\n",
              "   'text': \" along with the DPI work that we've done in India, will have people. And I think that in the end,\",\n",
              "   'tokens': [50984,\n",
              "    2051,\n",
              "    365,\n",
              "    264,\n",
              "    413,\n",
              "    31701,\n",
              "    589,\n",
              "    300,\n",
              "    321,\n",
              "    600,\n",
              "    1096,\n",
              "    294,\n",
              "    5282,\n",
              "    11,\n",
              "    486,\n",
              "    362,\n",
              "    561,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    294,\n",
              "    264,\n",
              "    917,\n",
              "    11,\n",
              "    51256],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18543222427368164,\n",
              "   'compression_ratio': 1.6069868995633187,\n",
              "   'no_speech_prob': 0.07256335765123367},\n",
              "  {'id': 236,\n",
              "   'seek': 139824,\n",
              "   'start': 1416.8,\n",
              "   'end': 1422.8,\n",
              "   'text': ' it is the intent is that people need to be able to use it and they will vote by things that are',\n",
              "   'tokens': [51292,\n",
              "    309,\n",
              "    307,\n",
              "    264,\n",
              "    8446,\n",
              "    307,\n",
              "    300,\n",
              "    561,\n",
              "    643,\n",
              "    281,\n",
              "    312,\n",
              "    1075,\n",
              "    281,\n",
              "    764,\n",
              "    309,\n",
              "    293,\n",
              "    436,\n",
              "    486,\n",
              "    4740,\n",
              "    538,\n",
              "    721,\n",
              "    300,\n",
              "    366,\n",
              "    51592],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18543222427368164,\n",
              "   'compression_ratio': 1.6069868995633187,\n",
              "   'no_speech_prob': 0.07256335765123367},\n",
              "  {'id': 237,\n",
              "   'seek': 142280,\n",
              "   'start': 1422.8,\n",
              "   'end': 1429.84,\n",
              "   'text': \" useful for them. And if that doesn't happen, you're right. I think that we have to figure out\",\n",
              "   'tokens': [50364,\n",
              "    4420,\n",
              "    337,\n",
              "    552,\n",
              "    13,\n",
              "    400,\n",
              "    498,\n",
              "    300,\n",
              "    1177,\n",
              "    380,\n",
              "    1051,\n",
              "    11,\n",
              "    291,\n",
              "    434,\n",
              "    558,\n",
              "    13,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    321,\n",
              "    362,\n",
              "    281,\n",
              "    2573,\n",
              "    484,\n",
              "    50716],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.24348915947808158,\n",
              "   'compression_ratio': 1.5925925925925926,\n",
              "   'no_speech_prob': 0.07091043889522552},\n",
              "  {'id': 238,\n",
              "   'seek': 142280,\n",
              "   'start': 1429.84,\n",
              "   'end': 1436.32,\n",
              "   'text': ' what is the mechanism of delivery of apps, right? In Bhojhaut, where do Indians consume content?',\n",
              "   'tokens': [50716,\n",
              "    437,\n",
              "    307,\n",
              "    264,\n",
              "    7513,\n",
              "    295,\n",
              "    8982,\n",
              "    295,\n",
              "    7733,\n",
              "    11,\n",
              "    558,\n",
              "    30,\n",
              "    682,\n",
              "    363,\n",
              "    1289,\n",
              "    73,\n",
              "    71,\n",
              "    1375,\n",
              "    11,\n",
              "    689,\n",
              "    360,\n",
              "    23838,\n",
              "    14732,\n",
              "    2701,\n",
              "    30,\n",
              "    51040],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.24348915947808158,\n",
              "   'compression_ratio': 1.5925925925925926,\n",
              "   'no_speech_prob': 0.07091043889522552},\n",
              "  {'id': 239,\n",
              "   'seek': 142280,\n",
              "   'start': 1436.32,\n",
              "   'end': 1442.56,\n",
              "   'text': \" That's a question. I'm so sorry, but we are out of time. Vivek will be outside. So he would be able\",\n",
              "   'tokens': [51040,\n",
              "    663,\n",
              "    311,\n",
              "    257,\n",
              "    1168,\n",
              "    13,\n",
              "    286,\n",
              "    478,\n",
              "    370,\n",
              "    2597,\n",
              "    11,\n",
              "    457,\n",
              "    321,\n",
              "    366,\n",
              "    484,\n",
              "    295,\n",
              "    565,\n",
              "    13,\n",
              "    44288,\n",
              "    74,\n",
              "    486,\n",
              "    312,\n",
              "    2380,\n",
              "    13,\n",
              "    407,\n",
              "    415,\n",
              "    576,\n",
              "    312,\n",
              "    1075,\n",
              "    51352],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.24348915947808158,\n",
              "   'compression_ratio': 1.5925925925925926,\n",
              "   'no_speech_prob': 0.07091043889522552},\n",
              "  {'id': 240,\n",
              "   'seek': 142280,\n",
              "   'start': 1442.56,\n",
              "   'end': 1447.68,\n",
              "   'text': ' to answer the question. We have time for one last question. Can I just take one last? Thank you.',\n",
              "   'tokens': [51352,\n",
              "    281,\n",
              "    1867,\n",
              "    264,\n",
              "    1168,\n",
              "    13,\n",
              "    492,\n",
              "    362,\n",
              "    565,\n",
              "    337,\n",
              "    472,\n",
              "    1036,\n",
              "    1168,\n",
              "    13,\n",
              "    1664,\n",
              "    286,\n",
              "    445,\n",
              "    747,\n",
              "    472,\n",
              "    1036,\n",
              "    30,\n",
              "    1044,\n",
              "    291,\n",
              "    13,\n",
              "    51608],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.24348915947808158,\n",
              "   'compression_ratio': 1.5925925925925926,\n",
              "   'no_speech_prob': 0.07091043889522552},\n",
              "  {'id': 241,\n",
              "   'seek': 144768,\n",
              "   'start': 1447.68,\n",
              "   'end': 1453.04,\n",
              "   'text': \" I'm Manish Kudhari. I'm from ISBR Business School. Good that I got a chance to ask you this question.\",\n",
              "   'tokens': [50364,\n",
              "    286,\n",
              "    478,\n",
              "    2458,\n",
              "    742,\n",
              "    591,\n",
              "    532,\n",
              "    71,\n",
              "    3504,\n",
              "    13,\n",
              "    286,\n",
              "    478,\n",
              "    490,\n",
              "    6205,\n",
              "    11609,\n",
              "    10715,\n",
              "    5070,\n",
              "    13,\n",
              "    2205,\n",
              "    300,\n",
              "    286,\n",
              "    658,\n",
              "    257,\n",
              "    2931,\n",
              "    281,\n",
              "    1029,\n",
              "    291,\n",
              "    341,\n",
              "    1168,\n",
              "    13,\n",
              "    50632],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1659119229356782,\n",
              "   'compression_ratio': 1.6610169491525424,\n",
              "   'no_speech_prob': 0.42492106556892395},\n",
              "  {'id': 242,\n",
              "   'seek': 144768,\n",
              "   'start': 1453.04,\n",
              "   'end': 1458.72,\n",
              "   'text': ' During lunchtime, there were a few of our educationists whom we were talking about, and there was one',\n",
              "   'tokens': [50632,\n",
              "    6842,\n",
              "    6349,\n",
              "    3766,\n",
              "    11,\n",
              "    456,\n",
              "    645,\n",
              "    257,\n",
              "    1326,\n",
              "    295,\n",
              "    527,\n",
              "    3309,\n",
              "    1751,\n",
              "    7101,\n",
              "    321,\n",
              "    645,\n",
              "    1417,\n",
              "    466,\n",
              "    11,\n",
              "    293,\n",
              "    456,\n",
              "    390,\n",
              "    472,\n",
              "    50916],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1659119229356782,\n",
              "   'compression_ratio': 1.6610169491525424,\n",
              "   'no_speech_prob': 0.42492106556892395},\n",
              "  {'id': 243,\n",
              "   'seek': 144768,\n",
              "   'start': 1458.72,\n",
              "   'end': 1463.8400000000001,\n",
              "   'text': ' from school, and we are from the MBA institutions. We were thinking of these present generations,',\n",
              "   'tokens': [50916,\n",
              "    490,\n",
              "    1395,\n",
              "    11,\n",
              "    293,\n",
              "    321,\n",
              "    366,\n",
              "    490,\n",
              "    264,\n",
              "    26674,\n",
              "    8142,\n",
              "    13,\n",
              "    492,\n",
              "    645,\n",
              "    1953,\n",
              "    295,\n",
              "    613,\n",
              "    1974,\n",
              "    10593,\n",
              "    11,\n",
              "    51172],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1659119229356782,\n",
              "   'compression_ratio': 1.6610169491525424,\n",
              "   'no_speech_prob': 0.42492106556892395},\n",
              "  {'id': 244,\n",
              "   'seek': 144768,\n",
              "   'start': 1463.8400000000001,\n",
              "   'end': 1469.44,\n",
              "   'text': ' how do we get them into what you are doing? There is one thing that they have been regularly,',\n",
              "   'tokens': [51172,\n",
              "    577,\n",
              "    360,\n",
              "    321,\n",
              "    483,\n",
              "    552,\n",
              "    666,\n",
              "    437,\n",
              "    291,\n",
              "    366,\n",
              "    884,\n",
              "    30,\n",
              "    821,\n",
              "    307,\n",
              "    472,\n",
              "    551,\n",
              "    300,\n",
              "    436,\n",
              "    362,\n",
              "    668,\n",
              "    11672,\n",
              "    11,\n",
              "    51452],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1659119229356782,\n",
              "   'compression_ratio': 1.6610169491525424,\n",
              "   'no_speech_prob': 0.42492106556892395},\n",
              "  {'id': 245,\n",
              "   'seek': 144768,\n",
              "   'start': 1469.44,\n",
              "   'end': 1473.52,\n",
              "   'text': \" that the concentrations that they're working on, but artificial intelligence, and getting into\",\n",
              "   'tokens': [51452,\n",
              "    300,\n",
              "    264,\n",
              "    33512,\n",
              "    300,\n",
              "    436,\n",
              "    434,\n",
              "    1364,\n",
              "    322,\n",
              "    11,\n",
              "    457,\n",
              "    11677,\n",
              "    7599,\n",
              "    11,\n",
              "    293,\n",
              "    1242,\n",
              "    666,\n",
              "    51656],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1659119229356782,\n",
              "   'compression_ratio': 1.6610169491525424,\n",
              "   'no_speech_prob': 0.42492106556892395},\n",
              "  {'id': 246,\n",
              "   'seek': 147352,\n",
              "   'start': 1474.08,\n",
              "   'end': 1478.08,\n",
              "   'text': ' this, getting them into their academics, and making them a part of it is very important,',\n",
              "   'tokens': [50392,\n",
              "    341,\n",
              "    11,\n",
              "    1242,\n",
              "    552,\n",
              "    666,\n",
              "    641,\n",
              "    25695,\n",
              "    11,\n",
              "    293,\n",
              "    1455,\n",
              "    552,\n",
              "    257,\n",
              "    644,\n",
              "    295,\n",
              "    309,\n",
              "    307,\n",
              "    588,\n",
              "    1021,\n",
              "    11,\n",
              "    50592],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13372033497072616,\n",
              "   'compression_ratio': 1.7786561264822134,\n",
              "   'no_speech_prob': 0.11212604492902756},\n",
              "  {'id': 247,\n",
              "   'seek': 147352,\n",
              "   'start': 1478.08,\n",
              "   'end': 1483.6,\n",
              "   'text': ' including the trainers who train them, making them future-ready into what you are doing is amazing.',\n",
              "   'tokens': [50592,\n",
              "    3009,\n",
              "    264,\n",
              "    35393,\n",
              "    567,\n",
              "    3847,\n",
              "    552,\n",
              "    11,\n",
              "    1455,\n",
              "    552,\n",
              "    2027,\n",
              "    12,\n",
              "    1201,\n",
              "    666,\n",
              "    437,\n",
              "    291,\n",
              "    366,\n",
              "    884,\n",
              "    307,\n",
              "    2243,\n",
              "    13,\n",
              "    50868],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13372033497072616,\n",
              "   'compression_ratio': 1.7786561264822134,\n",
              "   'no_speech_prob': 0.11212604492902756},\n",
              "  {'id': 248,\n",
              "   'seek': 147352,\n",
              "   'start': 1483.6,\n",
              "   'end': 1489.6,\n",
              "   'text': ' And the speed that which is growing, it is calling for a lot of training that needs to be done.',\n",
              "   'tokens': [50868,\n",
              "    400,\n",
              "    264,\n",
              "    3073,\n",
              "    300,\n",
              "    597,\n",
              "    307,\n",
              "    4194,\n",
              "    11,\n",
              "    309,\n",
              "    307,\n",
              "    5141,\n",
              "    337,\n",
              "    257,\n",
              "    688,\n",
              "    295,\n",
              "    3097,\n",
              "    300,\n",
              "    2203,\n",
              "    281,\n",
              "    312,\n",
              "    1096,\n",
              "    13,\n",
              "    51168],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13372033497072616,\n",
              "   'compression_ratio': 1.7786561264822134,\n",
              "   'no_speech_prob': 0.11212604492902756},\n",
              "  {'id': 249,\n",
              "   'seek': 147352,\n",
              "   'start': 1490.16,\n",
              "   'end': 1494.8799999999999,\n",
              "   'text': ' Can you from your angle through some light on how we could make them future-ready,',\n",
              "   'tokens': [51196,\n",
              "    1664,\n",
              "    291,\n",
              "    490,\n",
              "    428,\n",
              "    5802,\n",
              "    807,\n",
              "    512,\n",
              "    1442,\n",
              "    322,\n",
              "    577,\n",
              "    321,\n",
              "    727,\n",
              "    652,\n",
              "    552,\n",
              "    2027,\n",
              "    12,\n",
              "    1201,\n",
              "    11,\n",
              "    51432],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13372033497072616,\n",
              "   'compression_ratio': 1.7786561264822134,\n",
              "   'no_speech_prob': 0.11212604492902756},\n",
              "  {'id': 250,\n",
              "   'seek': 147352,\n",
              "   'start': 1494.8799999999999,\n",
              "   'end': 1500.48,\n",
              "   'text': ' how these people who are management graduates and from schools who are coming out,',\n",
              "   'tokens': [51432,\n",
              "    577,\n",
              "    613,\n",
              "    561,\n",
              "    567,\n",
              "    366,\n",
              "    4592,\n",
              "    13577,\n",
              "    293,\n",
              "    490,\n",
              "    4656,\n",
              "    567,\n",
              "    366,\n",
              "    1348,\n",
              "    484,\n",
              "    11,\n",
              "    51712],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13372033497072616,\n",
              "   'compression_ratio': 1.7786561264822134,\n",
              "   'no_speech_prob': 0.11212604492902756},\n",
              "  {'id': 251,\n",
              "   'seek': 150048,\n",
              "   'start': 1500.56,\n",
              "   'end': 1503.52,\n",
              "   'text': ' how do we get into this part of technology that you spoke about?',\n",
              "   'tokens': [50368,\n",
              "    577,\n",
              "    360,\n",
              "    321,\n",
              "    483,\n",
              "    666,\n",
              "    341,\n",
              "    644,\n",
              "    295,\n",
              "    2899,\n",
              "    300,\n",
              "    291,\n",
              "    7179,\n",
              "    466,\n",
              "    30,\n",
              "    50516],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1693053197379064,\n",
              "   'compression_ratio': 1.7291666666666667,\n",
              "   'no_speech_prob': 0.051503781229257584},\n",
              "  {'id': 252,\n",
              "   'seek': 150048,\n",
              "   'start': 1504.96,\n",
              "   'end': 1511.04,\n",
              "   'text': ' So this is really a challenge, because I think everyone will need to understand at some level',\n",
              "   'tokens': [50588,\n",
              "    407,\n",
              "    341,\n",
              "    307,\n",
              "    534,\n",
              "    257,\n",
              "    3430,\n",
              "    11,\n",
              "    570,\n",
              "    286,\n",
              "    519,\n",
              "    1518,\n",
              "    486,\n",
              "    643,\n",
              "    281,\n",
              "    1223,\n",
              "    412,\n",
              "    512,\n",
              "    1496,\n",
              "    50892],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1693053197379064,\n",
              "   'compression_ratio': 1.7291666666666667,\n",
              "   'no_speech_prob': 0.051503781229257584},\n",
              "  {'id': 253,\n",
              "   'seek': 150048,\n",
              "   'start': 1511.6,\n",
              "   'end': 1516.72,\n",
              "   'text': ' what this technology does, and I think that we have to rethink how we get everyone',\n",
              "   'tokens': [50920,\n",
              "    437,\n",
              "    341,\n",
              "    2899,\n",
              "    775,\n",
              "    11,\n",
              "    293,\n",
              "    286,\n",
              "    519,\n",
              "    300,\n",
              "    321,\n",
              "    362,\n",
              "    281,\n",
              "    34595,\n",
              "    577,\n",
              "    321,\n",
              "    483,\n",
              "    1518,\n",
              "    51176],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1693053197379064,\n",
              "   'compression_ratio': 1.7291666666666667,\n",
              "   'no_speech_prob': 0.051503781229257584},\n",
              "  {'id': 254,\n",
              "   'seek': 150048,\n",
              "   'start': 1517.44,\n",
              "   'end': 1521.84,\n",
              "   'text': ' into this, and that this kind of education has to be at many different levels.',\n",
              "   'tokens': [51212,\n",
              "    666,\n",
              "    341,\n",
              "    11,\n",
              "    293,\n",
              "    300,\n",
              "    341,\n",
              "    733,\n",
              "    295,\n",
              "    3309,\n",
              "    575,\n",
              "    281,\n",
              "    312,\n",
              "    412,\n",
              "    867,\n",
              "    819,\n",
              "    4358,\n",
              "    13,\n",
              "    51432],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1693053197379064,\n",
              "   'compression_ratio': 1.7291666666666667,\n",
              "   'no_speech_prob': 0.051503781229257584},\n",
              "  {'id': 255,\n",
              "   'seek': 150048,\n",
              "   'start': 1521.84,\n",
              "   'end': 1529.04,\n",
              "   'text': \" There are from a core set of having people who are extremely good at some, and there you don't\",\n",
              "   'tokens': [51432,\n",
              "    821,\n",
              "    366,\n",
              "    490,\n",
              "    257,\n",
              "    4965,\n",
              "    992,\n",
              "    295,\n",
              "    1419,\n",
              "    561,\n",
              "    567,\n",
              "    366,\n",
              "    4664,\n",
              "    665,\n",
              "    412,\n",
              "    512,\n",
              "    11,\n",
              "    293,\n",
              "    456,\n",
              "    291,\n",
              "    500,\n",
              "    380,\n",
              "    51792],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1693053197379064,\n",
              "   'compression_ratio': 1.7291666666666667,\n",
              "   'no_speech_prob': 0.051503781229257584},\n",
              "  {'id': 256,\n",
              "   'seek': 152904,\n",
              "   'start': 1529.04,\n",
              "   'end': 1533.76,\n",
              "   'text': ' need as many, but then there are basically vast numbers of people who can actually leverage these',\n",
              "   'tokens': [50364,\n",
              "    643,\n",
              "    382,\n",
              "    867,\n",
              "    11,\n",
              "    457,\n",
              "    550,\n",
              "    456,\n",
              "    366,\n",
              "    1936,\n",
              "    8369,\n",
              "    3547,\n",
              "    295,\n",
              "    561,\n",
              "    567,\n",
              "    393,\n",
              "    767,\n",
              "    13982,\n",
              "    613,\n",
              "    50600],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11120223999023438,\n",
              "   'compression_ratio': 1.6943231441048034,\n",
              "   'no_speech_prob': 0.012571604922413826},\n",
              "  {'id': 257,\n",
              "   'seek': 152904,\n",
              "   'start': 1533.76,\n",
              "   'end': 1538.6399999999999,\n",
              "   'text': \" tools. By the way, the most important thing about, and maybe that's part of what makes an LLM\",\n",
              "   'tokens': [50600,\n",
              "    3873,\n",
              "    13,\n",
              "    3146,\n",
              "    264,\n",
              "    636,\n",
              "    11,\n",
              "    264,\n",
              "    881,\n",
              "    1021,\n",
              "    551,\n",
              "    466,\n",
              "    11,\n",
              "    293,\n",
              "    1310,\n",
              "    300,\n",
              "    311,\n",
              "    644,\n",
              "    295,\n",
              "    437,\n",
              "    1669,\n",
              "    364,\n",
              "    441,\n",
              "    43,\n",
              "    44,\n",
              "    50844],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11120223999023438,\n",
              "   'compression_ratio': 1.6943231441048034,\n",
              "   'no_speech_prob': 0.012571604922413826},\n",
              "  {'id': 258,\n",
              "   'seek': 152904,\n",
              "   'start': 1538.6399999999999,\n",
              "   'end': 1546.32,\n",
              "   'text': ' interesting, is that how you use it, your mileage varies by that, and to understand how to actually',\n",
              "   'tokens': [50844,\n",
              "    1880,\n",
              "    11,\n",
              "    307,\n",
              "    300,\n",
              "    577,\n",
              "    291,\n",
              "    764,\n",
              "    309,\n",
              "    11,\n",
              "    428,\n",
              "    43121,\n",
              "    21716,\n",
              "    538,\n",
              "    300,\n",
              "    11,\n",
              "    293,\n",
              "    281,\n",
              "    1223,\n",
              "    577,\n",
              "    281,\n",
              "    767,\n",
              "    51228],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11120223999023438,\n",
              "   'compression_ratio': 1.6943231441048034,\n",
              "   'no_speech_prob': 0.012571604922413826},\n",
              "  {'id': 259,\n",
              "   'seek': 152904,\n",
              "   'start': 1546.32,\n",
              "   'end': 1552.3999999999999,\n",
              "   'text': ' leverage this in an interesting way is something that we have to widely teach many, many people,',\n",
              "   'tokens': [51228,\n",
              "    13982,\n",
              "    341,\n",
              "    294,\n",
              "    364,\n",
              "    1880,\n",
              "    636,\n",
              "    307,\n",
              "    746,\n",
              "    300,\n",
              "    321,\n",
              "    362,\n",
              "    281,\n",
              "    13371,\n",
              "    2924,\n",
              "    867,\n",
              "    11,\n",
              "    867,\n",
              "    561,\n",
              "    11,\n",
              "    51532],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11120223999023438,\n",
              "   'compression_ratio': 1.6943231441048034,\n",
              "   'no_speech_prob': 0.012571604922413826},\n",
              "  {'id': 260,\n",
              "   'seek': 155240,\n",
              "   'start': 1552.4,\n",
              "   'end': 1559.2,\n",
              "   'text': ' and because asking the things in the right way and having the right kind of applications',\n",
              "   'tokens': [50364,\n",
              "    293,\n",
              "    570,\n",
              "    3365,\n",
              "    264,\n",
              "    721,\n",
              "    294,\n",
              "    264,\n",
              "    558,\n",
              "    636,\n",
              "    293,\n",
              "    1419,\n",
              "    264,\n",
              "    558,\n",
              "    733,\n",
              "    295,\n",
              "    5821,\n",
              "    50704],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.30316563870044466,\n",
              "   'compression_ratio': 1.6308411214953271,\n",
              "   'no_speech_prob': 0.08316906541585922},\n",
              "  {'id': 261,\n",
              "   'seek': 155240,\n",
              "   'start': 1559.2,\n",
              "   'end': 1564.8000000000002,\n",
              "   'text': ' will make a huge difference to how people can leverage these tools. Thank you. Thank you very much.',\n",
              "   'tokens': [50704,\n",
              "    486,\n",
              "    652,\n",
              "    257,\n",
              "    2603,\n",
              "    2649,\n",
              "    281,\n",
              "    577,\n",
              "    561,\n",
              "    393,\n",
              "    13982,\n",
              "    613,\n",
              "    3873,\n",
              "    13,\n",
              "    1044,\n",
              "    291,\n",
              "    13,\n",
              "    1044,\n",
              "    291,\n",
              "    588,\n",
              "    709,\n",
              "    13,\n",
              "    50984],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.30316563870044466,\n",
              "   'compression_ratio': 1.6308411214953271,\n",
              "   'no_speech_prob': 0.08316906541585922},\n",
              "  {'id': 262,\n",
              "   'seek': 155240,\n",
              "   'start': 1564.8000000000002,\n",
              "   'end': 1569.2,\n",
              "   'text': \" We'll make a very good luck to Sarvam and good luck to India. I think it's going to be a lot\",\n",
              "   'tokens': [50984,\n",
              "    492,\n",
              "    603,\n",
              "    652,\n",
              "    257,\n",
              "    588,\n",
              "    665,\n",
              "    3668,\n",
              "    281,\n",
              "    6894,\n",
              "    85,\n",
              "    335,\n",
              "    293,\n",
              "    665,\n",
              "    3668,\n",
              "    281,\n",
              "    5282,\n",
              "    13,\n",
              "    286,\n",
              "    519,\n",
              "    309,\n",
              "    311,\n",
              "    516,\n",
              "    281,\n",
              "    312,\n",
              "    257,\n",
              "    688,\n",
              "    51204],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.30316563870044466,\n",
              "   'compression_ratio': 1.6308411214953271,\n",
              "   'no_speech_prob': 0.08316906541585922},\n",
              "  {'id': 263,\n",
              "   'seek': 155240,\n",
              "   'start': 1569.2,\n",
              "   'end': 1574.96,\n",
              "   'text': ' right on the show, guys. Thanks, Bala. Thank you, Mr. Raghavan, and',\n",
              "   'tokens': [51204,\n",
              "    558,\n",
              "    322,\n",
              "    264,\n",
              "    855,\n",
              "    11,\n",
              "    1074,\n",
              "    13,\n",
              "    2561,\n",
              "    11,\n",
              "    363,\n",
              "    5159,\n",
              "    13,\n",
              "    1044,\n",
              "    291,\n",
              "    11,\n",
              "    2221,\n",
              "    13,\n",
              "    497,\n",
              "    14842,\n",
              "    21071,\n",
              "    11,\n",
              "    293,\n",
              "    51492],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.30316563870044466,\n",
              "   'compression_ratio': 1.6308411214953271,\n",
              "   'no_speech_prob': 0.08316906541585922}],\n",
              " 'language': 'en'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "# Initialize the SentenceTransformer model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def truncate_to_last_punctuation(sentence):\n",
        "    is_match = re.search(r'([.!?])[^.!?]*$', sentence)\n",
        "    return sentence[:is_match.end()] if is_match else sentence\n",
        "\n",
        "# Extract and truncate all sentences\n",
        "segments = result['segments']\n",
        "sentences = [truncate_to_last_punctuation(segment['text']) for segment in segments]\n",
        "start_times = [segment['start'] for segment in segments]\n",
        "end_times = [segment['end'] for segment in segments]\n",
        "\n",
        "\n",
        "# Encode all sentences\n",
        "embeddings = embedding_model.encode(sentences)\n",
        "\n",
        "# Compute the cosine similarities\n",
        "cosine_similarities = cosine_similarity(embeddings)\n",
        "\n",
        "# Print the cosine similarities matrix\n",
        "print(cosine_similarities)\n"
      ],
      "metadata": {
        "id": "gv4wSAdRE1aS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e33ca7c-84d9-4893-cb01-028d5329c41b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.0000004   0.1429085   0.2574929  ...  0.13034494  0.40642732\n",
            "   0.6199169 ]\n",
            " [ 0.1429085   1.0000002   0.32472086 ...  0.02078965  0.14182517\n",
            "   0.12865487]\n",
            " [ 0.2574929   0.32472086  1.0000001  ... -0.01411895  0.14436342\n",
            "   0.15252638]\n",
            " ...\n",
            " [ 0.13034494  0.02078965 -0.01411895 ...  1.0000002   0.07002784\n",
            "   0.05722925]\n",
            " [ 0.40642732  0.14182517  0.14436342 ...  0.07002784  0.99999994\n",
            "   0.24416229]\n",
            " [ 0.6199169   0.12865487  0.15252638 ...  0.05722925  0.24416229\n",
            "   1.0000002 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1u7JJshzRWk",
        "outputId": "fe090bae-5977-4402-850d-c4bef56d26be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "264"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarities[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qra-o_y-2sGQ",
        "outputId": "62707972-c8fd-40f9-bb1a-f3ff6cb5f181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.00000036e+00,  1.42908499e-01,  2.57492900e-01,  7.70775825e-02,\n",
              "        9.81340110e-02,  2.52681315e-01, -1.13153039e-02,  5.73605001e-02,\n",
              "        2.48922095e-01,  2.39508361e-01,  2.04844713e-01,  1.44979000e-01,\n",
              "        2.31736481e-01,  3.08326721e-01,  4.08598721e-01,  2.22971588e-01,\n",
              "        3.05503249e-01,  9.81618762e-02,  2.50901490e-01,  2.08901435e-01,\n",
              "        2.95054257e-01,  1.75275832e-01,  9.49166268e-02,  6.88187685e-03,\n",
              "       -1.27508482e-02,  2.08675817e-01,  3.56323346e-02,  1.05732180e-01,\n",
              "        1.18965819e-01,  1.10564187e-01,  8.80994871e-02,  1.99401855e-01,\n",
              "        8.61672238e-02,  1.46211445e-01,  7.81481341e-02,  1.11926407e-01,\n",
              "        7.25968480e-02,  9.28634405e-02,  2.88047671e-01,  3.33515741e-02,\n",
              "        5.83096668e-02,  1.65270507e-01,  3.51877153e-01,  2.11163282e-01,\n",
              "        2.91476622e-02,  5.38781937e-03,  7.93977678e-02,  7.88376927e-02,\n",
              "        1.46378815e-01,  1.17527097e-01,  8.32714513e-02, -2.23345645e-02,\n",
              "        9.21840668e-02,  2.80010458e-02,  1.41780317e-01,  5.99370301e-02,\n",
              "        1.16969101e-01,  1.57386452e-01,  9.23506469e-02,  7.49449879e-02,\n",
              "        2.16754138e-01, -1.36684617e-02,  2.19594035e-02,  4.86064516e-03,\n",
              "        4.57405709e-02,  9.96799991e-02,  5.80701008e-02, -7.55293816e-02,\n",
              "        2.83746123e-01,  2.21890986e-01,  1.08137324e-01, -5.26981801e-03,\n",
              "        6.11227676e-02,  7.66071454e-02,  3.57107706e-02,  3.86177935e-03,\n",
              "        1.26113549e-01,  4.84925918e-02,  7.55406320e-02,  1.14018843e-01,\n",
              "        2.48196442e-03,  2.72572022e-02,  1.14401646e-01,  4.06402647e-02,\n",
              "        1.54252350e-01,  1.07814193e-01,  1.47911400e-01,  4.70708385e-02,\n",
              "        1.24495722e-01,  1.59718916e-01,  8.79534930e-02, -3.24347503e-02,\n",
              "        1.93941556e-02, -3.50610390e-02, -3.65587994e-02,  1.09713431e-03,\n",
              "       -2.75496766e-02,  1.05983272e-01,  1.45574547e-02,  1.78093255e-01,\n",
              "        9.21645537e-02, -5.44048138e-02,  1.58436209e-01,  7.84710944e-02,\n",
              "        4.30010036e-02,  3.03466916e-01,  1.01219520e-01,  2.54174322e-01,\n",
              "        5.16810864e-02,  3.29749495e-01, -4.43015546e-02,  3.48985270e-02,\n",
              "        2.09490582e-02,  1.81352168e-01,  2.28741139e-01,  2.27922052e-02,\n",
              "        3.03578302e-02,  8.89270529e-02,  1.89483613e-01,  9.77691859e-02,\n",
              "        1.09147176e-01,  2.34004296e-02,  5.18065877e-02,  1.35688156e-01,\n",
              "        4.78127152e-02,  4.82019708e-02,  3.24172340e-02,  2.60852128e-02,\n",
              "        2.18726099e-01,  1.12261400e-01,  1.32225007e-01, -1.56820323e-02,\n",
              "        2.31890082e-01,  4.42937687e-02,  3.72005440e-02,  7.80780390e-02,\n",
              "        1.67900220e-01,  4.27434742e-02,  1.26952633e-01,  1.43352687e-01,\n",
              "        2.17666388e-01,  1.94652662e-01,  5.69122285e-03,  2.53214743e-02,\n",
              "        2.88904570e-02,  5.49513549e-02,  1.17044775e-02,  2.47333169e-01,\n",
              "        4.85705845e-02,  6.14033192e-02,  2.51088403e-02,  4.83661108e-02,\n",
              "        4.62949127e-02,  1.25984043e-01,  2.58598596e-01, -1.19318515e-02,\n",
              "        1.34483695e-01,  7.07895905e-02,  2.10159898e-01,  1.11245580e-01,\n",
              "        9.01119784e-02,  3.16314399e-03,  9.82497185e-02,  8.20595622e-02,\n",
              "        3.71070392e-03,  6.11463711e-02,  1.17248371e-01,  1.54859692e-01,\n",
              "        7.60157108e-02,  1.25992224e-01,  5.25687449e-03,  4.13115099e-02,\n",
              "       -1.11622792e-02,  2.87864134e-02,  2.65907496e-03, -2.93878205e-02,\n",
              "        1.36196390e-01,  7.73635209e-02,  2.49714047e-01,  1.05490036e-01,\n",
              "        1.26066685e-01,  1.37663662e-01,  2.40535289e-03,  5.16305864e-02,\n",
              "        5.72284982e-02,  9.54697281e-02,  1.25152245e-01,  9.14365500e-02,\n",
              "        8.68573338e-02,  5.62446937e-02,  1.05630286e-01,  7.30691254e-02,\n",
              "       -1.08771976e-02, -3.11033968e-02, -1.45341801e-02,  1.95128378e-02,\n",
              "       -1.03116967e-04,  1.36785340e-02,  1.96066856e-01,  1.63706318e-01,\n",
              "        1.51314706e-01,  4.28999364e-02,  1.51162837e-02,  2.41353177e-02,\n",
              "        4.12705019e-02,  3.77220707e-03,  1.73894376e-01,  1.02770932e-01,\n",
              "        2.03231890e-02,  3.82752061e-01,  1.69604316e-01,  2.29310632e-01,\n",
              "       -1.01920679e-01,  9.25661623e-02, -3.88453156e-02,  3.09535898e-02,\n",
              "        1.16372071e-01, -1.19678564e-02,  1.44251078e-01,  1.00698307e-01,\n",
              "       -8.07685181e-02,  6.66964427e-02,  1.01812348e-01,  7.65503570e-03,\n",
              "        4.58429083e-02,  1.43280536e-01,  1.71940669e-01,  1.89763486e-01,\n",
              "        6.70785084e-02, -1.28637441e-02,  6.33297116e-02,  1.98510423e-01,\n",
              "        8.92771594e-03, -6.90306276e-02,  8.48807916e-02,  2.70320594e-01,\n",
              "       -9.84490290e-03, -1.48353791e-02,  6.15701377e-02,  7.47128874e-02,\n",
              "        1.60695285e-01,  2.72207350e-01,  1.07300073e-01,  1.78396016e-01,\n",
              "        1.93763658e-01,  1.24422126e-02,  1.93419278e-01,  1.62945583e-01,\n",
              "        5.24536744e-02,  1.65005386e-01,  1.90986350e-01,  1.44299537e-01,\n",
              "        1.27879411e-01,  1.03181049e-01,  1.39827788e-01,  8.75776634e-02,\n",
              "        1.81257073e-02,  6.80539012e-02, -1.02924751e-02,  1.23603895e-01,\n",
              "        8.21510255e-02,  1.30344942e-01,  4.06427324e-01,  6.19916916e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.35  # Adjusted threshold\n",
        "window_size = 5  # Sliding window size\n",
        "\n",
        "# Function to determine if sentences should be grouped based on the sliding window\n",
        "def should_group(window):\n",
        "    return any(cosine_similarities[i][i+1] >= threshold for i in range(window[0], window[1] - 1))\n",
        "\n",
        "# Group sentences based on the threshold and sliding window\n",
        "groups = []\n",
        "current_group = [0]  # Store indices of the sentences in the current group\n",
        "\n",
        "for i in range(1, len(sentences)):\n",
        "    window_start = max(0, i - window_size + 1)\n",
        "    window_end = min(i + 1, len(sentences))\n",
        "    if should_group((window_start, window_end)):\n",
        "        current_group.append(i)\n",
        "    else:\n",
        "        if len(current_group) >= 8:\n",
        "            groups.append(current_group)\n",
        "        current_group = [i]\n",
        "\n",
        "# Append the last group if it has at least 8 sentences\n",
        "if len(current_group) >= 8:\n",
        "    groups.append(current_group)\n",
        "\n",
        "# Create the final dictionary of groups\n",
        "final_groups = []\n",
        "for idx, group in enumerate(groups):\n",
        "    chunk_id = idx + 1\n",
        "    group_sentences = [sentences[i] for i in group]\n",
        "    group_text = ' '.join(group_sentences)\n",
        "    group_start_time = start_times[group[0]]\n",
        "    group_end_time = end_times[group[-1]]\n",
        "    final_groups.append({\n",
        "        'chunk_id': chunk_id,\n",
        "        'text': group_text,\n",
        "        'start_time': group_start_time,\n",
        "        'end_time': group_end_time\n",
        "    })\n",
        "\n",
        "# Print the final groups\n",
        "for group in final_groups:\n",
        "    print(f\"Chunk ID: {group['chunk_id']}\")\n",
        "    print(f\"Text: {group['text']}\")\n",
        "    print(f\"Start Time: {group['start_time']}\")\n",
        "    print(f\"End Time: {group['end_time']}\")\n",
        "    print()  # Print a new line for better readability"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNERHZEM5ff3",
        "outputId": "596d359c-8a73-4d89-ef3b-a62b0a5ca0fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: 1\n",
            "Text:  Vivek and I moved back to India from both in the valley on the same day actually. And you've been  in India for the last 16 years. And what most people don't know is your journey at Adhar.  He spent 13 years selflessly at Adhar. Nobody would have heard of him. But he was a pioneering  technology visionary behind Adhar which we all take for granted today. So please give it out.  Honestly when people, when I think of selfless service, truly selfless service, I always think of  Vivek. And since then he also was at AI for Bharat which we're going to touch on where he met  Pratiyusha's other co-founder. Pratiyusha had a PhD from ETH at Zurich. He was in the IBM research.  He was at Microsoft Research playing a key role and a faculty at IIT Madras and at AI for Bharat.  So that's a little brief introduction about them. These guys are modest, modest engineers.  So they don't do their own hon. So forgive me for tooting their hon in this case. But let's jump  right in about the money. Funding. 41 million bucks man. That's a lot of money. Every entrepreneur  here is saying what the hell did these guys do? What did the investors see to write such a big check?  No, I think it's a trend of what's going on in India. I think that for the very first time,  I think the investors have looked at, let's try and build something deep-tech out of the country.  And let's try to figure out how to build something as a foundational technology out of the country.  And that's really what's really exciting. And I think that about, as Bala was mentioning for  the past 15 years, I've been working in both digital public infrastructure and kind of  non-profit kind of things. But when this whole thing of generative AI came about,\n",
            "Start Time: 72.24\n",
            "End Time: 193.6\n",
            "\n",
            "Chunk ID: 2\n",
            "Text:  many, many more entrepreneurs who are backed to do things in India. So yeah, I'm going to come back to  the many more entrepreneurs. I'm obviously going to ask you about Bhavesh's Krutri. So we're going to  come back to that question. But again, 41 million dollars. I mean, all of what you said, you know,  two million dollars, you know, that's a good amount of money for a startup which you know, which  has not yet built anything. What are you going to do with all this money?  I can have a perfect solution for the problem. I think in the last week I've got lots of calls,  lots of people telling me how I can do it. But I know you first, okay, I'll be landed in the country  in the same day. I'm in the front of the queue. But honestly, I think the key thing in this is\n",
            "Start Time: 261.2\n",
            "End Time: 307.76\n",
            "\n",
            "Chunk ID: 3\n",
            "Text:  But let's talk about what you guys actually built. What is open Hathi? How would you explain  open Hathi to? Many people here who might not have known about it. So I think open Hathi is,  so first of all, right, we come from, I personally come from the open source ecosystem and we,  and also from the DPI ecosystem. So we believe that for this to work, we need the ecosystem  to be successful. And as a result of that, one of the first things we did was, hey, there are these  open source, large language models that exist, right? I mean, everybody knows about the Lama family  from Meta. There are others like Mistral. There are a bunch of open source, you know, large language  models. And then we said, is there any way that take an existing open source model and teach  it language skills, right? I mean, you know, language. And that is really the, you know, what we  decide, what we said that can we do something like that? And is this a, you know, relatively frugal way  of actually, you know, making models, you know, work in diverse languages? Because the truth is  still today, I mean, if you look at the amount of data and knowledge, it is still English dominates  these things. And I think that how do you actually take and make it understand Indian language,  understand Indian context, and all of those things in actually a, in an efficient way? And therefore,  this was an attempt through that. And it's an open hearty is, you know, is currently based on  the Lama 7 billion model, but we'll be releasing many more models in different languages, different  sizes, and things like that as part of this, as part of this series. And of course, you know,\n",
            "Start Time: 341.52\n",
            "End Time: 443.36\n",
            "\n",
            "Chunk ID: 4\n",
            "Text:  endpoints that people can use. So therefore, it's not, it's definitely, you know, something that people  can can can use two things. And that's the essence of what this open hearty is. So what does it  mean to people in the audience here who are either doing their own startups or a business or  developers? How should they look at OpenAI? Sorry, not open. No, no, I think the way you look at it is  that we are one of the important things that we are doing is we're not just building models.  We are also going to be building a platform, a platform for developers where you can actually use  a combination of various different kinds of models, some which are from us, some which are open  source, some which may not be open source, and actually to actually pull together and figure out  how to deploy, you know, generative AI applications at scale and understand and evaluate\n",
            "Start Time: 448.72\n",
            "End Time: 504.64000000000004\n",
            "\n",
            "Chunk ID: 5\n",
            "Text:  thought about building server, we said we want to build a full stack generative AI company and  different people have, and our understanding of full stack is that we need to know how to train  models from scratch. We need to know how to kind of figure out how to deploy models to solve  real world use cases. And we need to play in the ecosystem to make sure that we can actually deploy  population scale applications. So we were thinking about all of these things. But still the models we  were talking about are fairly small models. They are fairly small models, right? The 7 to maybe  up to 70 billion kind of range we're talking about. While these models like OpenAI and Google  are obviously much bigger models, right? But we want to understand the techniques and be able to  build that muscle to do all of these things to make it available to people. Now those models are  I mean, as I said, you know, I think that there is space for all of those things. And I think as  even Sridhar was talking about earlier in the day, we believe that these smaller models can do  very, I mean, many, many kind of domain specific tasks extremely well, probably even better than\n",
            "Start Time: 534.72\n",
            "End Time: 611.2\n",
            "\n",
            "Chunk ID: 6\n",
            "Text:  the larger models. And that is really one of the key areas. And so the further value of these kinds  of things, right? We are not aiming in these models to build any AGI, right? That's not our goal here.  Our goal is to make things that work extremely well for domain specific use cases or  or increase accessibility through language and all of those kinds of things. And obviously all of  this unique to India. But what is unique about India? I mean, like, what is, is anything special in  our ecosystem that makes a small model focused with Indian languages better for more suited for our  problems? So I think that, I mean, there are quite a few things that are unique about India, right?  The first thing is I think that we are a voice first nation. So therefore, I think voice has to  be the core to doing things. The other thing, of course, India is extremely, it's a cost-conscious  country from a cost perspective. Now, I would say that there are lots of interesting use cases where\n",
            "Start Time: 611.6\n",
            "End Time: 673.44\n",
            "\n",
            "Chunk ID: 7\n",
            "Text:  benefits. One more question, and then I want to talk about some of the predictions that you've  boldly made. So Vivek, I usually ask people about what do you think the future will be, and everybody  usually hedges. I ask Vivek, what do you think is going to happen by December 2024? What do you think  sitting in this room one year later, we can expect? And you made three bold predictions. So I want  to talk about that before that I have one last question. What are the top three applications that you  think are relevant for India? You would see the talk about medical, when any quick summary,  what do you think the top three apps are for India for AI? So I mean, I think that as you said,  things like education and medical are clearly areas where I think that things can be leveraged.  The whole idea of all these kind of the DPI aspect of it is another major application where things  can happen. And here I'm talking about country specific. And I think the whole idea which\n",
            "Start Time: 809.9200000000001\n",
            "End Time: 866.16\n",
            "\n",
            "Chunk ID: 8\n",
            "Text:  we've also talked about was the concept of software. And I think that clearly we have a very large  software industry and how to reimagine those things in this context is also something that's  quite fair enough. Are you guys ready for Vivek Ragavan's bold predictions? Yes? No, I'm not hearing any,  yes, this is like a big deal. He's like one of the smartest guys that I know. He wants to make three  predictions. You don't want to hear it. All right. So I asked him, what do you think, you know,  year later, what do you think we can expect? And he came up with three things and usually people give  very blind answers when you ask questions like this because they don't want to be caught wrong.  Not Vivek. Vivek is bold. So he basically said three things and I'm going to list out the three  things and then he's asking about it. So number one, he says, I will prefer to talk to an automated\n",
            "Start Time: 866.16\n",
            "End Time: 916.88\n",
            "\n",
            "Chunk ID: 9\n",
            "Text:  et cetera, when you want to do something very specific, today when you call when you call  some kind of a bot, you actually end up, you mostly try to disconnect the call or you're extremely  upset that you're talking to a bot. But I think that there will come a time and I'm predicting it  as sooner than later that you will actually get better responses from the bot than what the  human representative that at least the average human representative that you could talk to could give.  And I think that that's just a, I just said that that there will come a time where you know,  it's not a human you're talking to, but it's probably more likely to solve your intent  than the human person. That's just something that I think that could happen.  Definitely controversial, but we'll let it go. What about the GPU glut?  No, I don't think that. So I think that the fact that there is a tremendous shortage right now,  I think that shortage will ease because that is how the cycles of things go, right? When  I think the fact that there was such a severe shortage last year, you know, basically caused  a number of different players to ramp up in various kinds of forms. And I think that that  will always go in a cycle. But you may find out that there are many, many more interesting  problems that people will be able to solve. I still remember, you know, we were at a\n",
            "Start Time: 977.12\n",
            "End Time: 1065.92\n",
            "\n",
            "Chunk ID: 10\n",
            "Text:  But the third one is pretty strange. You know, companies are born, companies die,  but you said some companies will suddenly die. What does that mean?  No, I think, see, I think the interesting thing is, and I think that it comes back to the  fundamental nature of AI. AI is a tool, right? And you have to use that. And you have to use that  within your business process, right? And how AI is being used, and so, and what's going to happen  is that, I mean, I think this is true with, you know, when someone said in terms of, you know,  people, they said, that the people who leverage AI will be, will, will be more effective than those  who don't leverage AI. And that will, too, for organizations also. Organizations that leverage AI  in, fundamentally in their core business processes, will be more effective than those who don't,  right? And I think that's the thing. And you won't know the difference until one day it becomes  too obvious, and it will be too late. And I think that's the reason why everybody needs to think  about what it means for your business. Because you will, everything will be fine. Everything will  be fine. And one day, somebody in your, either, either, either your competitor, your space,  or somebody brand new coming into your space will be reimagining your business process completely.  And at that stage, you will find that it's, you know, it's a very big, very tall, you know,  mountain to climb. And that's why I think it's important for both people and entities to think  about how they will, you know, they will upgrade themselves or they will modify their business\n",
            "Start Time: 1110.96\n",
            "End Time: 1208.8\n",
            "\n",
            "Chunk ID: 11\n",
            "Text:  During lunchtime, there were a few of our educationists whom we were talking about, and there was one  from school, and we are from the MBA institutions. We were thinking of these present generations,  how do we get them into what you are doing? There is one thing that they have been regularly,  that the concentrations that they're working on, but artificial intelligence, and getting into  this, getting them into their academics, and making them a part of it is very important,  including the trainers who train them, making them future-ready into what you are doing is amazing.  And the speed that which is growing, it is calling for a lot of training that needs to be done.  Can you from your angle through some light on how we could make them future-ready,  how these people who are management graduates and from schools who are coming out,  how do we get into this part of technology that you spoke about?\n",
            "Start Time: 1453.04\n",
            "End Time: 1503.52\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Assuming `result` is provided with the following structure:\n",
        "# result = {'segments': [{'text': 'sentence1', 'start': start_time1, 'end': end_time1}, ... ]}\n",
        "\n",
        "# Initialize the SentenceTransformer model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Function to truncate sentences to the last punctuation\n",
        "def truncate_to_last_punctuation(sentence):\n",
        "    match = re.search(r'([.!?])[^.!?]*$', sentence)\n",
        "    return sentence[:match.end()] if match else sentence\n",
        "\n",
        "# Extract and truncate all sentences along with their start and end times\n",
        "segments = result['segments']\n",
        "sentences = [truncate_to_last_punctuation(segment['text']) for segment in segments]\n",
        "start_times = [segment['start'] for segment in segments]\n",
        "end_times = [segment['end'] for segment in segments]\n",
        "\n",
        "# Encode all sentences\n",
        "embeddings = embedding_model.encode(sentences)\n",
        "\n",
        "# Compute the cosine similarities\n",
        "cosine_similarities = cosine_similarity(embeddings)\n",
        "\n",
        "# Define the hyperparameters\n",
        "threshold = 0.20  # Adjusted threshold\n",
        "window_size = 3  # Sliding window size\n",
        "min_duration = 5  # Minimum duration for a group in seconds\n",
        "max_duration = 15  # Maximum duration for a group in seconds\n",
        "\n",
        "# Function to determine if sentences should be grouped based on the sliding window\n",
        "def should_group(window):\n",
        "    return any(cosine_similarities[i][i+1] >= threshold for i in range(window[0], window[1] - 1))\n",
        "\n",
        "# Group sentences based on the threshold, sliding window, and duration limits\n",
        "groups = []\n",
        "current_group = [0]  # Store indices of the sentences in the current group\n",
        "\n",
        "for i in range(1, len(sentences)):\n",
        "    window_start = max(0, i - window_size + 1)\n",
        "    window_end = min(i + 1, len(sentences))\n",
        "    group_start_time = start_times[current_group[0]]\n",
        "    group_end_time = end_times[current_group[-1]]\n",
        "\n",
        "    if should_group((window_start, window_end)) and (group_end_time - group_start_time <= max_duration):\n",
        "        current_group.append(i)\n",
        "    else:\n",
        "        if min_duration <= (group_end_time - group_start_time) <= max_duration:\n",
        "            groups.append(current_group)\n",
        "        current_group = [i]\n",
        "\n",
        "# Append the last group if it meets the duration criteria\n",
        "group_start_time = start_times[current_group[0]]\n",
        "group_end_time = end_times[current_group[-1]]\n",
        "if min_duration <= (group_end_time - group_start_time) <= max_duration:\n",
        "    groups.append(current_group)\n",
        "\n",
        "# Create the final dictionary of groups\n",
        "final_groups = []\n",
        "for idx, group in enumerate(groups):\n",
        "    chunk_id = idx + 1\n",
        "    group_sentences = [sentences[i] for i in group]\n",
        "    group_text = ' '.join(group_sentences)\n",
        "    group_start_time = start_times[group[0]]\n",
        "    group_end_time = end_times[group[-1]]\n",
        "    final_groups.append({\n",
        "        'chunk_id': chunk_id,\n",
        "        'text': group_text,\n",
        "        'start_time': group_start_time,\n",
        "        'end_time': group_end_time\n",
        "    })\n",
        "\n",
        "# Print the final groups\n",
        "for group in final_groups:\n",
        "    print(f\"Chunk ID: {group['chunk_id']}\")\n",
        "    print(f\"Text: {group['text']}\")\n",
        "    print(f\"Start Time: {group['start_time']}\")\n",
        "    print(f\"End Time: {group['end_time']}\")\n",
        "    print()  # Print a new line for better readability\n"
      ],
      "metadata": {
        "id": "kbpTAiZ-6Oji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58eb3df6-ed82-4a09-cead-738656cfd79d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: 1\n",
            "Text:  dear friend and he is very, very modest. One of the most modest guys that I know. But his personal\n",
            "Start Time: 59.199999999999996\n",
            "End Time: 65.36\n",
            "\n",
            "Chunk ID: 2\n",
            "Text:  here is saying what the hell did these guys do? What did the investors see to write such a big check?  No, I think it's a trend of what's going on in India. I think that for the very first time,\n",
            "Start Time: 146.48\n",
            "End Time: 159.84\n",
            "\n",
            "Chunk ID: 3\n",
            "Text:  the reason of what we want to do at Server May I is we want to basically make generative AI  available and accessible to the people in the country. And that's the intent. And when we said\n",
            "Start Time: 220.64000000000001\n",
            "End Time: 232.0\n",
            "\n",
            "Chunk ID: 4\n",
            "Text:  has not yet built anything. What are you going to do with all this money?  I can have a perfect solution for the problem. I think in the last week I've got lots of calls,\n",
            "Start Time: 283.36\n",
            "End Time: 294.24\n",
            "\n",
            "Chunk ID: 5\n",
            "Text:  lots of people telling me how I can do it. But I know you first, okay, I'll be landed in the country\n",
            "Start Time: 294.24\n",
            "End Time: 300.32\n",
            "\n",
            "Chunk ID: 6\n",
            "Text:  in the same day. I'm in the front of the queue. But honestly, I think the key thing in this is\n",
            "Start Time: 300.32\n",
            "End Time: 307.76\n",
            "\n",
            "Chunk ID: 7\n",
            "Text:  And I think that those are the two primary things that you know, we'd use this for.  I'm computing in my own head as an entrepreneur. Talent, okay, you have like 2015 people.\n",
            "Start Time: 325.44\n",
            "End Time: 337.52\n",
            "\n",
            "Chunk ID: 8\n",
            "Text:  decide, what we said that can we do something like that? And is this a, you know, relatively frugal way\n",
            "Start Time: 394.15999999999997\n",
            "End Time: 400.15999999999997\n",
            "\n",
            "Chunk ID: 9\n",
            "Text:  understand Indian context, and all of those things in actually a, in an efficient way? And therefore,  this was an attempt through that. And it's an open hearty is, you know, is currently based on\n",
            "Start Time: 419.76000000000005\n",
            "End Time: 431.84\n",
            "\n",
            "Chunk ID: 10\n",
            "Text:  the Lama 7 billion model, but we'll be releasing many more models in different languages, different\n",
            "Start Time: 431.84\n",
            "End Time: 436.96\n",
            "\n",
            "Chunk ID: 11\n",
            "Text:  source, some which may not be open source, and actually to actually pull together and figure out  how to deploy, you know, generative AI applications at scale and understand and evaluate\n",
            "Start Time: 492.8\n",
            "End Time: 504.64000000000004\n",
            "\n",
            "Chunk ID: 12\n",
            "Text:  That's phenomenal. But how does it compare to OpenAI itself or Google?\n",
            "Start Time: 521.6\n",
            "End Time: 528.24\n",
            "\n",
            "Chunk ID: 13\n",
            "Text:  I mean, as I said, you know, I think that there is space for all of those things. And I think as\n",
            "Start Time: 591.6\n",
            "End Time: 597.0400000000001\n",
            "\n",
            "Chunk ID: 14\n",
            "Text:  The first thing is I think that we are a voice first nation. So therefore, I think voice has to\n",
            "Start Time: 653.84\n",
            "End Time: 659.44\n",
            "\n",
            "Chunk ID: 15\n",
            "Text:  And as a part of Adar building Adar, no better person than you. So in summary, what I'm hearing is\n",
            "Start Time: 712.0799999999999\n",
            "End Time: 719.04\n",
            "\n",
            "Chunk ID: 16\n",
            "Text:  small models specialized with trained with Indic specific language data suited for Indian problems\n",
            "Start Time: 719.04\n",
            "End Time: 725.92\n",
            "\n",
            "Chunk ID: 17\n",
            "Text:  different domains, making building things based on unique data that enterprises have and things  like that. So that's something that we'll also look at. Fair enough. So coming back to the\n",
            "Start Time: 757.6\n",
            "End Time: 766.96\n",
            "\n",
            "Chunk ID: 18\n",
            "Text:  elephant in the room, no fun intended with OpenHathe, what about Bavesh Akarwal and Kruthrim?\n",
            "Start Time: 767.84\n",
            "End Time: 773.76\n",
            "\n",
            "Chunk ID: 19\n",
            "Text:  that. I think it's great. And I think that there'll be different people who will have different  takes as to how to solve this kind of problem. And hopefully as a result of that, the entire ecosystem\n",
            "Start Time: 798.72\n",
            "End Time: 809.9200000000001\n",
            "\n",
            "Chunk ID: 20\n",
            "Text:  quite fair enough. Are you guys ready for Vivek Ragavan's bold predictions? Yes? No, I'm not hearing any,\n",
            "Start Time: 877.84\n",
            "End Time: 886.32\n",
            "\n",
            "Chunk ID: 21\n",
            "Text:  Not Vivek. Vivek is bold. So he basically said three things and I'm going to list out the three\n",
            "Start Time: 906.08\n",
            "End Time: 911.2\n",
            "\n",
            "Chunk ID: 22\n",
            "Text:  I think the fact that there was such a severe shortage last year, you know, basically caused  a number of different players to ramp up in various kinds of forms. And I think that that\n",
            "Start Time: 1041.3600000000001\n",
            "End Time: 1052.72\n",
            "\n",
            "Chunk ID: 23\n",
            "Text:  will always go in a cycle. But you may find out that there are many, many more interesting\n",
            "Start Time: 1052.72\n",
            "End Time: 1058.08\n",
            "\n",
            "Chunk ID: 24\n",
            "Text:  problems that people will be able to solve. I still remember, you know, we were at a\n",
            "Start Time: 1058.08\n",
            "End Time: 1065.92\n",
            "\n",
            "Chunk ID: 25\n",
            "Text:  I would not do that, but that's not what I said. I want to blame you for this. I could go that.\n",
            "Start Time: 1103.76\n",
            "End Time: 1109.2\n",
            "\n",
            "Chunk ID: 26\n",
            "Text:  But the third one is pretty strange. You know, companies are born, companies die,  but you said some companies will suddenly die. What does that mean?  No, I think, see, I think the interesting thing is, and I think that it comes back to the\n",
            "Start Time: 1110.96\n",
            "End Time: 1125.2\n",
            "\n",
            "Chunk ID: 27\n",
            "Text:  Thank you. My name is Kartik. I work for IT Service Industry.\n",
            "Start Time: 1240.48\n",
            "End Time: 1245.92\n",
            "\n",
            "Chunk ID: 28\n",
            "Text:  So you're saying that you're working on LLM, sorry, it's a fine tuned LLM on top of LAMA.\n",
            "Start Time: 1246.72\n",
            "End Time: 1254.88\n",
            "\n",
            "Chunk ID: 29\n",
            "Text:  mostly the existing models and run on top of it? You asked a good question. You asked the\n",
            "Start Time: 1273.9199999999998\n",
            "End Time: 1279.04\n",
            "\n",
            "Chunk ID: 30\n",
            "Text:  And that's the problem that we have that we think we have solved and is going to be the heart of\n",
            "Start Time: 1334.24\n",
            "End Time: 1339.28\n",
            "\n",
            "Chunk ID: 31\n",
            "Text:  what is the mechanism of delivery of apps, right? In Bhojhaut, where do Indians consume content?\n",
            "Start Time: 1429.84\n",
            "End Time: 1436.32\n",
            "\n",
            "Chunk ID: 32\n",
            "Text:  We'll make a very good luck to Sarvam and good luck to India. I think it's going to be a lot  right on the show, guys. Thanks, Bala. Thank you, Mr. Raghavan, and\n",
            "Start Time: 1564.8000000000002\n",
            "End Time: 1574.96\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U stable-ts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDVaXMnEsEvf",
        "outputId": "e7d20b8e-b1ba-4c12-9484-e4c04d88b6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-ts\n",
            "  Downloading stable-ts-2.17.2.tar.gz (147 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/148.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m143.4/148.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.0/148.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stable-ts) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from stable-ts) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from stable-ts) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-ts) (4.66.4)\n",
            "Requirement already satisfied: openai-whisper==20231117 in /usr/local/lib/python3.10/dist-packages (from stable-ts) (20231117)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117->stable-ts) (0.58.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117->stable-ts) (10.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117->stable-ts) (0.7.0)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117->stable-ts) (2.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->stable-ts) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->stable-ts) (2.1.5)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117->stable-ts) (0.41.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->stable-ts) (1.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117->stable-ts) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117->stable-ts) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117->stable-ts) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117->stable-ts) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117->stable-ts) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117->stable-ts) (2024.2.2)\n",
            "Building wheels for collected packages: stable-ts\n",
            "  Building wheel for stable-ts (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stable-ts: filename=stable_ts-2.17.2-py3-none-any.whl size=130674 sha256=23819f16bb28e397ea73c4e6423c10c3edb939e56dfcd84b4513fb650502e0ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/c5/df/86d8e8ee5aea7d5d104f9c314e4da25d731a701da0717a1c55\n",
            "Successfully built stable-ts\n",
            "Installing collected packages: stable-ts\n",
            "Successfully installed stable-ts-2.17.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stable_whisper\n",
        "model = stable_whisper.load_model('base')\n",
        "result = model.transcribe('audio.mp3')\n",
        "# result.to_srt_vtt('audio_2.srt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJC3HjRpTQ5y",
        "outputId": "37ec39b3-8839-45c7-9c60-15bc8a67ce5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_whisper/whisper_compatibility.py:242: UserWarning: The installed version of Whisper might be incompatible.\n",
            "The detected version appears to be installed from the repository which can have compatibility issues due to multiple commits sharing the same version number. It is recommended to install version 20231117 from PyPI.\n",
            "To prevent errors and performance issues, reinstall correct version with: \"pip install --upgrade --no-deps --force-reinstall openai-whisper==20231117\". Or use transcribe_minimal().\n",
            "  warnings.warn(compatibility_warning)\n",
            "Transcribe:   0%|          | 0/1575.18 [00:00<?, ?sec/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: english\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Transcribe: 100%|█████████▉| 1575.15/1575.18 [01:28<00:00, 17.85sec/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for segment in result[:2]:\n",
        "    for word in segment:\n",
        "        print(word.word, word.word == ' Mr.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7UbIRLXTWWW",
        "outputId": "e7973925-c0e8-4ef5-b131-300550009d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Congratulations False\n",
            " to False\n",
            " you False\n",
            " Mr. True\n",
            " Raghavan False\n",
            " for False\n",
            " that. False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming result is a list of segments, where each segment is a list of word objects with attributes 'word', 'start', and 'end'\n",
        "\n",
        "# List of abbreviations that should not signal the end of a sentence\n",
        "abbreviations = [\" Dr.\", \" Mr.\", \" Mrs.\", \" Ms.\", \" Prof.\"]\n",
        "\n",
        "# List to hold sentences_struct with their start and end times\n",
        "sentences_struct = []\n",
        "\n",
        "# Initialize variables for current sentence, start time, and end time\n",
        "current_sentence = \"\"\n",
        "sentence_start_time = None\n",
        "sentence_end_time = None\n",
        "\n",
        "# Iterate through segments\n",
        "for segment in result:\n",
        "    for word in segment:\n",
        "        if current_sentence == \"\":\n",
        "            # If starting a new sentence, set the start time\n",
        "            sentence_start_time = word.start\n",
        "\n",
        "        # Add the word to the current sentence\n",
        "        current_sentence += (word.word + \" \")\n",
        "\n",
        "        # Update the end time\n",
        "        sentence_end_time = word.end\n",
        "\n",
        "        # Check if the word ends with a punctuation mark indicating the end of a sentence\n",
        "        if (word.word.endswith(('.', '?')) and word.word not in abbreviations):\n",
        "            # Finalize the current sentence\n",
        "            sentences_struct.append({\n",
        "                'sentence': current_sentence.strip(),\n",
        "                'start': sentence_start_time,\n",
        "                'end': sentence_end_time\n",
        "            })\n",
        "            # Reset the current sentence\n",
        "            current_sentence = \"\"\n",
        "            sentence_start_time = None\n",
        "            sentence_end_time = None\n",
        "\n",
        "# Handle any remaining sentence\n",
        "if current_sentence:\n",
        "    sentences_struct.append({\n",
        "        'sentence': current_sentence.strip(),\n",
        "        'start': sentence_start_time,\n",
        "        'end': sentence_end_time\n",
        "    })\n",
        "\n",
        "# Print the sentences_struct\n",
        "for sentence_info in sentences_struct:\n",
        "    if(sentence_info['end'] - sentence_info['start'] > 15):\n",
        "        print(sentence_info['sentence'], sentence_info['start'], sentence_info['end'])\n",
        "        print(sentence_info['end'] - sentence_info['start'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_2Skg8-UXBy",
        "outputId": "6a689ae9-5452-42e5-cfc7-ff17b89bf83b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And  I  think  that  about,  you  know,  as  Bala  was  mentioning  for  the  past  15  years,  I've  been  kind  of  working  in  kind  of,  you  know,  both  digital  public  infrastructure  and  kind  of  nonprofit  kind  of  things. 173.5 190.52\n",
            "17.02000000000001\n",
            "And  I  think,  you  know,  I  think  as  even  Sridhar  was  talking  about  earlier  in  the  day,  we  believe  that  these  smaller  models  can  do  very,  I  mean,  many,  many  kind  of  domain  specific  tasks,  extremely  well,  probably  even  better  than  the,  than  the  larger  models. 596.54 612.34\n",
            "15.800000000000068\n",
            "And  what's  going  to  happen  is  that,  I  think  this  is  true  when  someone  said  in  terms  of  people,  they  said  that  the  people  who  leverage  AI  will  be  more  effective  than  those  who  don't  leverage  AI. 1135.99 1151.36\n",
            "15.36999999999989\n",
            "Well,  I  think  the  interesting  thing  is  that  if  you  look  at  the  token  and  we  have  actually  a  blog  on  this  on  our  website,  I  think  one  of  the  things  that  we've  actually  built  a  customized  tokenizer,  which  actually  fundamentally  changes  the  cost  of  some  of  these  generations  in  Indian  languages. 1281.34 1297.8\n",
            "16.460000000000036\n",
            "We  are  actually,  we  are  leveraging  the  existing  retaining,  but  we  are  doing  what's  known  as  continual  free  training,  but  having  said  that,  you  know,  I  think  that  once  we  have  to  figure  out  where  is  the  data  to  train  an  extremely  large  model  from  scratch  and  some  of  those  things  are  things  which  will  happen  over  time. 1301.62 1320.16\n",
            "18.54000000000019\n",
            "There  is  one  thing  that  they  have  been  regularly,  that  the  concentrations  that  they  are  working  on,  but  artificial  intelligence  and  getting  into  this,  getting  them  into  their  academics  and  making  them  a  part  of  it  is  very  important,  including  the  trainers  who  train  them,  making  them  future  ready  into  what  you  are  doing  is  amazing. 1467.7 1483.6\n",
            "15.899999999999864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Assuming `result` is provided with the following structure:\n",
        "# result = {'segments': [{'text': 'sentence1', 'start': start_time1, 'end': end_time1}, ... ]}\n",
        "\n",
        "# Initialize the SentenceTransformer model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Function to truncate sentences to the last punctuation\n",
        "# def truncate_to_last_punctuation(sentence):\n",
        "#     match = re.search(r'([.!?])[^.!?]*$', sentence)\n",
        "#     return sentence[:match.end()] if match else sentence\n",
        "\n",
        "# # Extract and truncate all sentences along with their start and end times\n",
        "# segments = result['segments']\n",
        "sentences = [sentence_info['sentence'] for sentence_info in sentences_struct]\n",
        "start_times = [sentence_info['start'] for sentence_info in sentences_struct]\n",
        "end_times = [sentence_info['end'] for sentence_info in sentences_struct]\n",
        "\n",
        "# Encode all sentences\n",
        "embeddings = embedding_model.encode(sentences)\n",
        "\n",
        "# Compute the cosine similarities\n",
        "cosine_similarities = cosine_similarity(embeddings)\n",
        "\n",
        "# Define the hyperparameters\n",
        "threshold = 0.20  # Adjusted threshold\n",
        "window_size = 3  # Sliding window size\n",
        "min_duration = 5  # Minimum duration for a group in seconds\n",
        "max_duration = 15  # Maximum duration for a group in seconds\n",
        "\n",
        "# Function to determine if sentences should be grouped based on the sliding window\n",
        "def should_group(window):\n",
        "    return any(cosine_similarities[i][i+1] >= threshold for i in range(window[0], window[1] - 1))\n",
        "\n",
        "# Group sentences based on the threshold, sliding window, and duration limits\n",
        "groups = []\n",
        "current_group = [0]  # Store indices of the sentences in the current group\n",
        "\n",
        "for i in range(1, len(sentences)):\n",
        "    window_start = max(0, i - window_size + 1)\n",
        "    window_end = min(i + 1, len(sentences))\n",
        "    group_start_time = start_times[current_group[0]]\n",
        "    group_end_time = end_times[current_group[-1]]\n",
        "\n",
        "    if should_group((window_start, window_end)) and (group_end_time - group_start_time <= max_duration):\n",
        "        current_group.append(i)\n",
        "    else:\n",
        "        if min_duration <= (group_end_time - group_start_time) <= max_duration:\n",
        "            groups.append(current_group)\n",
        "        current_group = [i]\n",
        "\n",
        "# Append the last group if it meets the duration criteria\n",
        "group_start_time = start_times[current_group[0]]\n",
        "group_end_time = end_times[current_group[-1]]\n",
        "if min_duration <= (group_end_time - group_start_time) <= max_duration:\n",
        "    groups.append(current_group)\n",
        "\n",
        "# Create the final dictionary of groups\n",
        "final_groups = []\n",
        "for idx, group in enumerate(groups):\n",
        "    chunk_id = idx + 1\n",
        "    group_sentences = [sentences[i] for i in group]\n",
        "    group_text = ' '.join(group_sentences)\n",
        "    group_start_time = start_times[group[0]]\n",
        "    group_end_time = end_times[group[-1]]\n",
        "    final_groups.append({\n",
        "        'chunk_id': chunk_id,\n",
        "        'text': group_text,\n",
        "        'start_time': group_start_time,\n",
        "        'end_time': group_end_time\n",
        "    })\n",
        "\n",
        "# Print the final groups\n",
        "for group in final_groups:\n",
        "    print(f\"Chunk ID: {group['chunk_id']}\")\n",
        "    print(f\"Text: {group['text']}\")\n",
        "    print(f\"Start Time: {group['start_time']}\")\n",
        "    print(f\"End Time: {group['end_time']}\")\n",
        "    print()  # Print a new line for better readability\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkfkHtJeiXB_",
        "outputId": "4d5631c4-d936-4a7f-f560-07612b26d2b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: 1\n",
            "Text: Thank  you  so  much  for  joining  us. Over  to  you. Hi  everybody. How  are  you? Okay  I  am  not  hearing  this  at  all.\n",
            "Start Time: 2.46\n",
            "End Time: 13.62\n",
            "\n",
            "Chunk ID: 2\n",
            "Text: Okay. But  let's  get  started. I  want  to  introduce  Vivek  and  Pratius.\n",
            "Start Time: 38.36\n",
            "End Time: 43.66\n",
            "\n",
            "Chunk ID: 3\n",
            "Text: Honestly,  when  I  think  of  selfless  service,  truly  selfless  service,  I  always  think  of  Vivek.\n",
            "Start Time: 101.46\n",
            "End Time: 107.0\n",
            "\n",
            "Chunk ID: 4\n",
            "Text: And  at  AI  for  Bharat. So  that's  a  little  brief  introduction  about  them. These  guys  are  modest,  modest  engineers. So  they  don't  toot  their  own  horn. So  forgive  me  for  tooting  their  horn  in  this  case. But  let's  jump  right  in.\n",
            "Start Time: 125.3\n",
            "End Time: 138.82\n",
            "\n",
            "Chunk ID: 5\n",
            "Text: About  the  money. Funding. Forty -one  million  bucks  man. That's  a  lot  of  money. Right.\n",
            "Start Time: 140.68\n",
            "End Time: 145.68\n",
            "\n",
            "Chunk ID: 6\n",
            "Text: Every  entrepreneur  here  is  saying  what  the  hell  did  these  guys  do? What  did  the  investors  see  to  write  such  a  big  check? No,  I  think  it's  a  trend  of  what's  going  on  in  India.\n",
            "Start Time: 145.84\n",
            "End Time: 158.06\n",
            "\n",
            "Chunk ID: 7\n",
            "Text: I  think  that  for  the  very  first  time  I  think  the  investors  have  looked  at,  you  know,  let's  try  and  build  something  deep  deck  out  of  the  country. And  let's  try  to  figure  out  how  to  build  something  as  a  foundational  technology  out  of  the  country. And  that's  really  what's  really  exciting.\n",
            "Start Time: 158.24\n",
            "End Time: 173.12\n",
            "\n",
            "Chunk ID: 8\n",
            "Text: I  think  in  the  last  week  I've  got  lots  of  calls,  lots  of  people  telling  me  how  I'm  going  to  do  it. I  can  do  it. But  I  know  you  first,  okay,  I'll  be  landed  in  the  country  the  same  day.\n",
            "Start Time: 292.08\n",
            "End Time: 300.9\n",
            "\n",
            "Chunk ID: 9\n",
            "Text: How  should  they  look  at  open  AI? Oh,  sorry,  not  open. No,  yeah,  I  don't  know.\n",
            "Start Time: 467.72\n",
            "End Time: 473.38\n",
            "\n",
            "Chunk ID: 10\n",
            "Text: And  that  is  really  one  of  the  key  areas. And,  and  so  the  further  value  of  these  kinds  of  things,  right? We're  not  aiming  in  these  most  set  of  models  to,  to  build  any  AGI,  right? That's  not  our  goal  here.\n",
            "Start Time: 612.78\n",
            "End Time: 623.34\n",
            "\n",
            "Chunk ID: 11\n",
            "Text: Our  goal  is  to  make  things  that  work  extremely  well  for  our  domain  specific  use  cases  or,  or  increase  accessibility  through  language  and,  and  all  of  those  kinds  of  things.\n",
            "Start Time: 623.84\n",
            "End Time: 633.18\n",
            "\n",
            "Chunk ID: 12\n",
            "Text: We're  not  solving  some  world  autonomous  vehicles  or  some  complex  problem. We're  solving  some  basic  problems  specifically  focused  with  on  voice  with  multiple  languages. That  is  what  you  see  as  a  future.\n",
            "Start Time: 728.98\n",
            "End Time: 739.34\n",
            "\n",
            "Chunk ID: 13\n",
            "Text: Am  I  paraphrasing  this  correctly? No,  yeah. So  I  think  that  certainly,  I  mean,  voice  and  Indian  languages  are  a,  are  an  important  part  of  what  we're  doing.\n",
            "Start Time: 739.58\n",
            "End Time: 746.86\n",
            "\n",
            "Chunk ID: 14\n",
            "Text: We're  doing  a  lot  of  our  strategy,  but  we  will  be  building,  you  know,  custom  models  to  solve  various  other  kinds  of  problems  as  well.\n",
            "Start Time: 746.88\n",
            "End Time: 752.8\n",
            "\n",
            "Chunk ID: 15\n",
            "Text: That's  not  just  limited  to,  I  think,  in  different  domains,  working  in  different  domains,  making  building  things  based  on  unique  data  that  enterprises  have  and  things  like  that.\n",
            "Start Time: 753.34\n",
            "End Time: 763.48\n",
            "\n",
            "Chunk ID: 16\n",
            "Text: And  hopefully  as  a  result  of  that,  the  entire  ecosystem  benefits. One  more  question.\n",
            "Start Time: 806.92\n",
            "End Time: 812.62\n",
            "\n",
            "Chunk ID: 17\n",
            "Text: And  then  I  want  to  talk  about  some  of  the  predictions  that  you've  boldly  made. So  Vivek,  I  usually  ask  people  about  what  do  you  think  the  future  will  be? And  everybody  usually  hedges.\n",
            "Start Time: 812.9\n",
            "End Time: 820.44\n",
            "\n",
            "Chunk ID: 18\n",
            "Text: I  asked  Vivek,  what  do  you  think  is  going  to  happen  by  December  2024? What  do  you  think  sitting  in  this  room  one  year  later,  we  can  expect. And  you  made  three  bold  predictions.\n",
            "Start Time: 821.16\n",
            "End Time: 829.72\n",
            "\n",
            "Chunk ID: 19\n",
            "Text: You  don't  want  to  hear  it. All  right. So  I  asked  him,  what  do  you  think?\n",
            "Start Time: 891.42\n",
            "End Time: 896.6\n",
            "\n",
            "Chunk ID: 20\n",
            "Text: And  he  came  up  with  three  things  and  usually  people  give  very  blind  answers  when  you  ask  questions  like  this  because  they  don't  want  to  be  caught  wrong.\n",
            "Start Time: 899.82\n",
            "End Time: 905.68\n",
            "\n",
            "Chunk ID: 21\n",
            "Text: So  you  want  to  quickly  talk  about  each  of  them. Why  you  just  came  up  with  these  and  then  we'll  throw  the  open  audience  questions. So  I  don't  think  I  quite  said  it  the  way  that  that  Vala  Rhyana  marketing  idea.\n",
            "Start Time: 949.84\n",
            "End Time: 961.36\n",
            "\n",
            "Chunk ID: 22\n",
            "Text: But  I  think  the  first  thing  that  we  said  is  I  think  that  and  I  don't  think  that  this  is  I  think  there  will  come  a  time  when  you  know  in  in  areas  of  customer  service,  etc.\n",
            "Start Time: 964.08\n",
            "End Time: 977.5\n",
            "\n",
            "Chunk ID: 23\n",
            "Text: You  know  basically  caused  a  number  of  different  players  to  to  ramp  up  in  various  kinds  of  forms.\n",
            "Start Time: 1045.28\n",
            "End Time: 1051.02\n",
            "\n",
            "Chunk ID: 24\n",
            "Text: I  want  to  blame  you  for  this. I  could  go  that. But  the  third  one  is  pretty  strange.\n",
            "Start Time: 1107.34\n",
            "End Time: 1112.4\n",
            "\n",
            "Chunk ID: 25\n",
            "Text: Companies  are  born,  companies  die. But  you  said  some  companies  will  suddenly  die. What  does  that  mean?\n",
            "Start Time: 1113.18\n",
            "End Time: 1118.5\n",
            "\n",
            "Chunk ID: 26\n",
            "Text: And  that  will  stood  for  organizations  also. Organizations  that  leverage  AI,  fundamentally  in  their  core  business  processes,  will  be  more  effective  than  those  who  don't. And  I  think  that's  the  thing.\n",
            "Start Time: 1151.88\n",
            "End Time: 1163.92\n",
            "\n",
            "Chunk ID: 27\n",
            "Text: You  won't  know  the  difference  until  one  day  it  becomes  too  obvious,  and  it  will  be  too  late.\n",
            "Start Time: 1164.88\n",
            "End Time: 1170.14\n",
            "\n",
            "Chunk ID: 28\n",
            "Text: And  I  think  that's  the  reason  why  everybody  needs  to  think  about  what  it  means  for  your  business.\n",
            "Start Time: 1170.54\n",
            "End Time: 1176.2\n",
            "\n",
            "Chunk ID: 29\n",
            "Text: Because  everything  will  be  fine. Everything  will  be  fine. And  one  day  somebody  in  your,  either  your  competitor  in  your  space  or  somebody  brand  new  coming  into  your  space  will  be  reimagining  your  business  process  completely.\n",
            "Start Time: 1177.02\n",
            "End Time: 1190.7\n",
            "\n",
            "Chunk ID: 30\n",
            "Text: Okay. So  does  okay,  a  lot  of  questions. So  love  to,  is  there  a  mic  that  we  can  pass  along?\n",
            "Start Time: 1228.54\n",
            "End Time: 1235.04\n",
            "\n",
            "Chunk ID: 31\n",
            "Text: So  you're  saying  that  you're  working  on  LLM,  sorry,  it's  a  fine  tuned  LLM  on  top  of  Lama.\n",
            "Start Time: 1246.96\n",
            "End Time: 1253.86\n",
            "\n",
            "Chunk ID: 32\n",
            "Text: But  I  think  that,  I  think  that,  yes,  I  think  that  we  will  be  doing  various  kinds  of  things. But  the  interesting  thing  is  that  if  I  want  to  change  the  accessibility  problem  with  an  existing  open  source  model,  how  do  I  do  that?\n",
            "Start Time: 1320.68\n",
            "End Time: 1334.34\n",
            "\n",
            "Chunk ID: 33\n",
            "Text: And  that's  the  problem  that  we  have  that  we  think  we  have  solved  and  is  going  to  be  the  heart  of  this  open -heart  TV  series.\n",
            "Start Time: 1334.5\n",
            "End Time: 1340.7\n",
            "\n",
            "Chunk ID: 34\n",
            "Text: My  question  is  like,  unlike  China,  we  never  had  a  consumer  facing  application  coming  out  from  India  and  in  Web  1,  Web  2,  crypto  and  all.\n",
            "Start Time: 1348.46\n",
            "End Time: 1358.7\n",
            "\n",
            "Chunk ID: 35\n",
            "Text: So  I  think  the  question  is,  I  don't  know  the  answer  to  these  questions,  right?\n",
            "Start Time: 1391.32\n",
            "End Time: 1398.56\n",
            "\n",
            "Chunk ID: 36\n",
            "Text: So  he  would  be  able  to  answer  the  question. We  have  time  for  one  last  question. Can  I  just  take  one  last? Thank  you. Thank  you. I'm  Manish  Kuthari.\n",
            "Start Time: 1441.22\n",
            "End Time: 1448.92\n",
            "\n",
            "Chunk ID: 37\n",
            "Text: I'm  from  ISBR  Business  School. Good  that  I  got  a  chance  to  ask  you  this  question. During  lunchtime,  there  were  a  few  of  our  educationists  whom  we  were  talking  about. And  there  was  one  from  school. There  were  some  of  them  from  the  MBA  institutions. We  were  thinking  of  these  present  generations.\n",
            "Start Time: 1449.1\n",
            "End Time: 1463.78\n",
            "\n",
            "Chunk ID: 38\n",
            "Text: Thank  you. Thank  you  very  much. We  will  make  a  very  good  luck  to  Sarvam  and  good  luck  to  India. I  think  it's  going  to  be  a  lot  right  when  we  show  this. Thanks,  Bala. Thank  you,  Mr.  Raghavan.\n",
            "Start Time: 1563.18\n",
            "End Time: 1574.72\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube pydub --quiet\n",
        "!pip install git+https://github.com/m-bain/whisperx.git --quiet\n",
        "!pip install git+https://github.com/openai/whisper.git --quiet\n",
        "!pip install -U sentence-transformers\n",
        "!pip install -U stable-ts\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import re\n",
        "from pytube import YouTube\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "import subprocess\n",
        "import stable_whisper\n",
        "\n",
        "def download_youtube_audio(youtube_url, output_path='/content'):\n",
        "    try:\n",
        "        # Download YouTube video\n",
        "        yt = YouTube(youtube_url)\n",
        "        video = yt.streams.filter(only_audio=True).first()\n",
        "        video.download(output_path=output_path, filename='video.mp4')\n",
        "\n",
        "        # Convert video to audio\n",
        "        video_path = os.path.join(output_path, 'video.mp4')\n",
        "        audio_path = os.path.join(output_path, 'audio.mp3')\n",
        "\n",
        "        audio = AudioSegment.from_file(video_path)\n",
        "        audio.export(audio_path, format='mp3')\n",
        "\n",
        "        print(f'Audio extracted and saved to {audio_path}')\n",
        "        return audio_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage\n",
        "youtube_url = 'https://www.youtube.com/watch?v=Sby1uJ_NFIY'\n",
        "audio_file_path = download_youtube_audio(youtube_url)\n",
        "\n",
        "\n",
        "model = stable_whisper.load_model('base')\n",
        "result = model.transcribe(audio_file_path)\n",
        "\n",
        "# Assuming result is a list of segments, where each segment is a list of word objects with attributes 'word', 'start', and 'end'\n",
        "\n",
        "# List of abbreviations that should not signal the end of a sentence\n",
        "abbreviations = [\" Dr.\", \" Mr.\", \" Mrs.\", \" Ms.\", \" Prof.\"]\n",
        "\n",
        "# List to hold sentences_struct with their start and end times\n",
        "sentences_struct = []\n",
        "\n",
        "# Initialize variables for current sentence, start time, and end time\n",
        "current_sentence = \"\"\n",
        "sentence_start_time = None\n",
        "sentence_end_time = None\n",
        "\n",
        "# Iterate through segments\n",
        "for segment in result:\n",
        "    for word in segment:\n",
        "        if current_sentence == \"\":\n",
        "            # If starting a new sentence, set the start time\n",
        "            sentence_start_time = word.start\n",
        "\n",
        "        # Add the word to the current sentence\n",
        "        current_sentence += (word.word + \" \")\n",
        "\n",
        "        # Update the end time\n",
        "        sentence_end_time = word.end\n",
        "\n",
        "        # Check if the word ends with a punctuation mark indicating the end of a sentence\n",
        "        if (word.word.endswith(('.', '?')) and word.word not in abbreviations):\n",
        "            # Finalize the current sentence\n",
        "            sentences_struct.append({\n",
        "                'sentence': current_sentence.strip(),\n",
        "                'start': sentence_start_time,\n",
        "                'end': sentence_end_time\n",
        "            })\n",
        "            # Reset the current sentence\n",
        "            current_sentence = \"\"\n",
        "            sentence_start_time = None\n",
        "            sentence_end_time = None\n",
        "\n",
        "# Handle any remaining sentence\n",
        "if current_sentence:\n",
        "    sentences_struct.append({\n",
        "        'sentence': current_sentence.strip(),\n",
        "        'start': sentence_start_time,\n",
        "        'end': sentence_end_time\n",
        "    })\n",
        "\n",
        "# Print the sentences_struct\n",
        "for sentence_info in sentences_struct:\n",
        "    if(sentence_info['end'] - sentence_info['start'] > 15):\n",
        "        print(sentence_info['sentence'], sentence_info['start'], sentence_info['end'])\n",
        "        print(sentence_info['end'] - sentence_info['start'])\n",
        "\n",
        "\n",
        "# Initialize the SentenceTransformer model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "sentences = [sentence_info['sentence'] for sentence_info in sentences_struct]\n",
        "start_times = [sentence_info['start'] for sentence_info in sentences_struct]\n",
        "end_times = [sentence_info['end'] for sentence_info in sentences_struct]\n",
        "\n",
        "# Encode all sentences\n",
        "embeddings = embedding_model.encode(sentences)\n",
        "\n",
        "# Compute the cosine similarities\n",
        "cosine_similarities = cosine_similarity(embeddings)\n",
        "\n",
        "# Define the hyperparameters\n",
        "threshold = 0.20  # Adjusted threshold\n",
        "window_size = 3  # Sliding window size\n",
        "min_duration = 5  # Minimum duration for a group in seconds\n",
        "max_duration = 15  # Maximum duration for a group in seconds\n",
        "\n",
        "# Function to determine if sentences should be grouped based on the sliding window\n",
        "def should_group(window):\n",
        "    return any(cosine_similarities[i][i+1] >= threshold for i in range(window[0], window[1] - 1))\n",
        "\n",
        "# Group sentences based on the threshold, sliding window, and duration limits\n",
        "groups = []\n",
        "current_group = [0]  # Store indices of the sentences in the current group\n",
        "\n",
        "for i in range(1, len(sentences)):\n",
        "    window_start = max(0, i - window_size + 1)\n",
        "    window_end = min(i + 1, len(sentences))\n",
        "    group_start_time = start_times[current_group[0]]\n",
        "    group_end_time = end_times[current_group[-1]]\n",
        "\n",
        "    if should_group((window_start, window_end)) and (group_end_time - group_start_time <= max_duration):\n",
        "        current_group.append(i)\n",
        "    else:\n",
        "        if min_duration <= (group_end_time - group_start_time) <= max_duration:\n",
        "            groups.append(current_group)\n",
        "        current_group = [i]\n",
        "\n",
        "# Append the last group if it meets the duration criteria\n",
        "group_start_time = start_times[current_group[0]]\n",
        "group_end_time = end_times[current_group[-1]]\n",
        "if min_duration <= (group_end_time - group_start_time) <= max_duration:\n",
        "    groups.append(current_group)\n",
        "\n",
        "# Create the final dictionary of groups\n",
        "final_groups = []\n",
        "for idx, group in enumerate(groups):\n",
        "    chunk_id = idx + 1\n",
        "    group_sentences = [sentences[i] for i in group]\n",
        "    group_text = ' '.join(group_sentences)\n",
        "    group_start_time = start_times[group[0]]\n",
        "    group_end_time = end_times[group[-1]]\n",
        "    final_groups.append({\n",
        "        'chunk_id': chunk_id,\n",
        "        'text': group_text,\n",
        "        'start_time': group_start_time,\n",
        "        'end_time': group_end_time\n",
        "    })\n",
        "\n",
        "# Print the final groups\n",
        "for group in final_groups:\n",
        "    print(f\"Chunk ID: {group['chunk_id']}\")\n",
        "    print(f\"Text: {group['text']}\")\n",
        "    print(f\"Start Time: {group['start_time']}\")\n",
        "    print(f\"End Time: {group['end_time']}\")\n",
        "    print()  # Print a new line for better readability\n"
      ],
      "metadata": {
        "id": "Pt1jtSeNqk8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oSV5eIk4tGPR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}